<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-26T00:00:00Z">2025-06-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximal Matching Matters: Preventing Representation Collapse for Robust
  <span class="highlight-title">Cross</span>-Modal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hani Alomari, Anushka Sivakumar, Andrew Zhang, Chris Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 63rd Annual Meeting of the Association for
  Computational Linguistics (ACL 2025 Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skLEP: A Slovak General Language Understanding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Cypher A<span class="highlight-title">cross</span> Languages: Evaluating Foundational Models Beyond
  English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makbule Gulcin Ozsoy, William Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">LLM</span>-Assisted Query Understanding for Live Retrieval-Augmented
  Generation <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Xiaoxi Li, Yuyao Zhang, Mengjie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time and personalized product <span class="highlight-title">recommendation</span>s for large e-commerce
  platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Tolloso, Davide Bacciu, Shahab Mokarizadeh, Marco Varesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology to provide real-time and personalized product
recommendations for large e-commerce platforms, specifically focusing on
fashion retail. Our approach aims to achieve accurate and scalable
recommendations with minimal response times, ensuring user satisfaction,
leveraging Graph Neural Networks and parsimonious learning methodologies.
Extensive experimentation with datasets from one of the largest e-commerce
platforms demonstrates the effectiveness of our approach in forecasting
purchase sequences and handling multi-interaction scenarios, achieving
efficient personalized recommendations under real-world constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the International
  Conference on Artificial Neural Networks (ICANN) 2025. The final
  authenticated version will be available for purchase through the publisher's
  website. The conference proceedings will be published by Springer in the
  Lecture Notes in Computer Science (LNCS) series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Encoders Can Rival Large Decoders in Detecting Groundedness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Automatic Term Extraction with <span class="highlight-title">Large Language Model</span>s via
  Syntactic Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchan Chun, Minhyuk Kim, Dongjun Kim, Chanjun Park, Heuiseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time
  Stretching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillem Cortès-Sebastià, Benjamin Martin, Emilio Molina, Xavier Serra, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)
system designed specifically around spectral peaks. This novel system is
designed to leverage the sparse spectral coordinates typically computed by
traditional peak-based AFP methods. PeakNetFP performs hierarchical point
feature extraction techniques similar to the computer vision model PointNet++,
and is trained using contrastive learning like in the state-of-the-art deep
learning AFP, NeuralFP. This combination allows PeakNetFP to outperform
conventional AFP systems and achieves comparable performance to NeuralFP when
handling challenging time-stretched audio data. In extensive evaluation,
PeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging
from 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:
compared to NeuralFP, it has 100 times fewer parameters and uses 11 times
smaller input data. These features make PeakNetFP a lightweight and efficient
solution for AFP tasks where time stretching is involved. Overall, this system
represents a promising direction for future AFP technologies, as it
successfully merges the lightweight nature of peak-based AFP with the
adaptability and pattern recognition capabilities of neural network-based
approaches, paving the way for more scalable and efficient solutions in the
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISMIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-supervised Scalable Unified Framework for E-commerce Query
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyuan Yuan, Chong Zhang, Zheng Fang, Ming Pang, Xue Jiang, Changping Peng, Zhangang Lin, Ching Law
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecCoT: Enhancing <span class="highlight-title">Recommendation</span> via Chain-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Jiangxia Cao, Haipeng Li, Yuqi Mao, Shuchao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, users always interact with items in multiple
aspects, such as through implicit binary feedback (e.g., clicks, dislikes, long
views) and explicit feedback (e.g., comments, reviews). Modern recommendation
systems (RecSys) learn user-item collaborative signals from these implicit
feedback signals as a large-scale binary data-streaming, subsequently
recommending other highly similar items based on users' personalized historical
interactions. However, from this collaborative-connection perspective, the
RecSys does not focus on the actual content of the items themselves but instead
prioritizes higher-probability signals of behavioral co-occurrence among items.
Consequently, under this binary learning paradigm, the RecSys struggles to
understand why a user likes or dislikes certain items. To alleviate it, some
works attempt to utilize the content-based reviews to capture the semantic
knowledge to enhance recommender models. However, most of these methods focus
on predicting the ratings of reviews, but do not provide a human-understandable
explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Response Quality Assessment for Retrieval-Augmented Generation via
  Conditional Conformal Factuality <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research on Retrieval-Augmented Generation (RAG) primarily focuses
on improving overall question-answering accuracy, often overlooking the quality
of sub-claims within generated responses. Recent methods that attempt to
improve RAG trustworthiness, such as through auto-evaluation metrics, lack
probabilistic guarantees or require ground truth answers. To address these
limitations, we propose Conformal-RAG, a novel framework inspired by recent
applications of conformal prediction (CP) on large language models (LLMs).
Conformal-RAG leverages CP and internal information from the RAG mechanism to
offer statistical guarantees on response quality. It ensures group-conditional
coverage spanning multiple sub-domains without requiring manual labelling of
conformal sets, making it suitable for complex RAG applications. Compared to
existing RAG auto-evaluation methods, Conformal-RAG offers statistical
guarantees on the quality of refined sub-claims, ensuring response reliability
without the need for ground truth answers. Additionally, our experiments
demonstrate that by leveraging information from the RAG system, Conformal-RAG
retains up to 60\% more high-quality sub-claims from the response compared to
direct applications of CP to LLMs, while maintaining the same reliability
guarantee.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025 short paper, 5 pages, Code is available at
  https://github.com/n4feng/ResponseQualityAssessment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EraRAG: Efficient and Incremental Retrieval Augmented Generation for
  Growing Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metadata Enrichment of Long Text Documents using <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manika Lamba, You Peng, Sophie Nikolov, Glen Layne-Worthey, J. Stephen Downie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this project, we semantically enriched and enhanced the metadata of long
text documents, theses and dissertations, retrieved from the HathiTrust Digital
Library in English published from 1920 to 2020 through a combination of manual
efforts and large language models. This dataset provides a valuable resource
for advancing research in areas such as computational social science, digital
humanities, and information science. Our paper shows that enriching metadata
using LLMs is particularly beneficial for digital repositories by introducing
additional metadata access points that may not have originally been foreseen to
accommodate various content types. This approach is particularly effective for
repositories that have significant missing data in their existing metadata
fields, enhancing search results and improving the accessibility of the digital
repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GATSY: Graph Attention Network for Music Artist Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Giuseppe Di Francesco, Giuliano Giampietro, Indro Spinelli, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The artist similarity quest has become a crucial subject in social and
scientific contexts, driven by the desire to enhance music discovery according
to user preferences. Modern research solutions facilitate music discovery
according to user tastes. However, defining similarity among artists remains
challenging due to its inherently subjective nature, which can impact
recommendation accuracy. This paper introduces GATSY, a novel recommendation
system built upon graph attention networks and driven by a clusterized
embedding of artists. The proposed framework leverages the graph topology of
the input data to achieve outstanding performance results without relying
heavily on hand-crafted features. This flexibility allows us to include
fictitious artists within a music dataset, facilitating connections between
previously unlinked artists and enabling diverse recommendations from various
and heterogeneous sources. Experimental results prove the effectiveness of the
proposed method with respect to state-of-the-art solutions while maintaining
flexibility. The code to reproduce these experiments is available at
https://github.com/difra100/GATSY-Music_Artist_Similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-Ready version, Accepted at IJCNN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Web Search towards Agentic Deep Research: Incentivizing Search with
  Reasoning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Adaptive Memory-Based Optimization for Enhanced
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitao Qin, Yucong Luo, Yihang Lu, Zhibo Chu, Xianwei Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge
from external knowledge bases into models, has emerged as a promising approach
to enhancing response accuracy while mitigating factual errors and
hallucinations. This method has been widely applied in tasks such as Question
Answering (QA). However, existing RAG methods struggle with open-domain QA
tasks because they perform independent retrieval operations and directly
incorporate the retrieved information into generation without maintaining a
summarizing memory or using adaptive retrieval strategies, leading to noise
from redundant information and insufficient information integration. To address
these challenges, we propose Adaptive memory-based optimization for enhanced
RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory
Updater, an Adaptive Information Collector, and a Multi-granular Content
Filter, working together within an iterative memory updating paradigm.
Specifically, Amber integrates and optimizes the language model's memory
through a multi-agent collaborative approach, ensuring comprehensive knowledge
integration from previous retrieval steps. It dynamically adjusts retrieval
queries and decides when to stop retrieval based on the accumulated knowledge,
enhancing retrieval efficiency and effectiveness. Additionally, it reduces
noise by filtering irrelevant content at multiple levels, retaining essential
information to improve overall model performance. We conduct extensive
experiments on several open-domain QA datasets, and the results demonstrate the
superiority and effectiveness of our method and its components. The source code
is available \footnote{https://anonymous.4open.science/r/Amber-B203/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages. arXiv admin note: text overlap with arXiv:2410.08821 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Rank for Multiple Retrieval-Augmented Models through
  Iterative Utility Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the design of a unified search engine to serve
multiple retrieval-augmented generation (RAG) agents, each with a distinct
task, backbone large language model (LLM), and RAG strategy. We introduce an
iterative approach where the search engine generates retrieval results for the
RAG agents and gathers feedback on the quality of the retrieved documents
during an offline phase. This feedback is then used to iteratively optimize the
search engine using an expectation-maximization algorithm, with the goal of
maximizing each agent's utility function. Additionally, we adapt this to an
online setting, allowing the search engine to refine its behavior based on
real-time individual agents feedback to better serve the results for each of
them. Experiments on datasets from the Knowledge-Intensive Language Tasks
(KILT) benchmark demonstrates that our approach significantly on average
outperforms baselines across 18 RAG models. We demonstrate that our method
effectively ``personalizes'' the retrieval for each RAG agent based on the
collected feedback. Finally, we provide a comprehensive ablation study to
explore various aspects of our method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Condensed Representation of RDF and its Application on Graph Versioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jey Puget Gil, Emmanuel Coquery, John Samuel, Gilles Gesquiere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localized RETE for Incremental Graph Queries with Nested Graph
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Barkowsky, Holger Giese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing size of graph-based modeling artifacts in model-driven
engineering calls for techniques that enable efficient execution of graph
queries. Incremental approaches based on the RETE algorithm provide an adequate
solution in many scenarios, but are generally designed to search for query
results over the entire graph. However, in certain situations, a user may only
be interested in query results for a subgraph, for instance when a developer is
working on a large model of which only a part is loaded into their workspace.
In this case, the global execution semantics can result in significant
computational overhead.
  To mitigate the outlined shortcoming, in this article we propose an extension
of the RETE approach that enables local, yet fully incremental execution of
graph queries, while still guaranteeing completeness of results with respect to
the relevant subgraph.
  We empirically evaluate the presented approach via experiments inspired by a
scenario from software development and with queries and data from an
independent social network benchmark. The experimental results indicate that
the proposed technique can significantly improve performance regarding memory
consumption and execution time in favorable cases, but may incur a noticeable
overhead in unfavorable cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2405.01145</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-Body Conditioned Egocentric Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://dannytran123.github.io/PEVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and
  Model Selection at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaona Zhou, Constantin Brif, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HalluSegBench: Counterfactual Visual Reasoning for Segmentation
  Hallucination Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhuo Li, Adheesh Juvekar, Xingyou Liu, Muntasir Wahed, Kiet A. Nguyen, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://plan-lab.github.io/hallusegbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldVLA: Towards Autoregressive Action World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present WorldVLA, an autoregressive action world model that unifies action
and image understanding and generation. Our WorldVLA intergrates
Vision-Language-Action (VLA) model and world model in one single framework. The
world model predicts future images by leveraging both action and image
understanding, with the purpose of learning the underlying physics of the
environment to improve action generation. Meanwhile, the action model generates
the subsequent actions based on image observations, aiding in visual
understanding and in turn helps visual generation of the world model. We
demonstrate that WorldVLA outperforms standalone action and world models,
highlighting the mutual enhancement between the world model and the action
model. In addition, we find that the performance of the action model
deteriorates when generating sequences of actions in an autoregressive manner.
This phenomenon can be attributed to the model's limited generalization
capability for action prediction, leading to the propagation of errors from
earlier actions to subsequent ones. To address this issue, we propose an
attention mask strategy that selectively masks prior actions during the
generation of the current action, which shows significant performance
improvement in the action chunk generation task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/alibaba-damo-academy/WorldVLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PsyLite Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "What's Up, Doc?": Analyzing How Users Seek Health Information in
  Large-Scale Conversational AI Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Paruchuri, Maryam Aziz, Rohit Vartak, Ayman Ali, Best Uchehara, Xin Liu, Ishan Chatterjee, Monica Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K
  dataset release</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Potemkin Understanding in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skLEP: A Slovak General Language Understanding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process mining-driven modeling and simulation to enhance fault diagnosis
  in cyber-physical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Vitale, Nicola Dall'Ora, Sebastiano Gaiardelli, Enrico Fraccaroli, Nicola Mazzocca, Franco Fummi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ad-Hoc Human-AI Coordination Challenge <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin Dizdarević, Ravi Hammond, Tobias Gessler, Anisoara Calinescu, Jonathan Cook, Matteo Gallici, Andrei Lupu, Jakob Nicolaus Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TITAN: Query-Token based <span class="highlight-title">Domain</span> Adaptive Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tajamul Ashraf, Janibul Bashir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on the source-free domain adaptive object detection (SF-DAOD)
problem when source data is unavailable during adaptation and the model must
adapt to an unlabeled target domain. The majority of approaches for the problem
employ a self-supervised approach using a student-teacher (ST) framework where
pseudo-labels are generated via a source-pretrained model for further
fine-tuning. We observe that the performance of a student model often degrades
drastically, due to the collapse of the teacher model, primarily caused by high
noise in pseudo-labels, resulting from domain bias, discrepancies, and a
significant domain shift across domains. To obtain reliable pseudo-labels, we
propose a Target-based Iterative Query-Token Adversarial Network (TITAN), which
separates the target images into two subsets: those similar to the source
(easy) and those dissimilar (hard). We propose a strategy to estimate variance
to partition the target domain. This approach leverages the insight that higher
detection variances correspond to higher recall and greater similarity to the
source domain. Also, we incorporate query-token-based adversarial modules into
a student-teacher baseline framework to reduce the domain gaps between two
feature representations. Experiments conducted on four natural imaging datasets
and two challenging medical datasets have substantiated the superior
performance of TITAN compared to existing state-of-the-art (SOTA)
methodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7
percent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmoothSinger: A Conditional <span class="highlight-title">Diffusion</span> Model for Singing Voice Synthesis
  with Multi-Resolution Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehan Sui, Jinxu Xiang, Fang Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice synthesis (SVS) aims to generate expressive and high-quality
vocals from musical scores, requiring precise modeling of pitch, duration, and
articulation. While diffusion-based models have achieved remarkable success in
image and video generation, their application to SVS remains challenging due to
the complex acoustic and musical characteristics of singing, often resulting in
artifacts that degrade naturalness. In this work, we propose SmoothSinger, a
conditional diffusion model designed to synthesize high quality and natural
singing voices. Unlike prior methods that depend on vocoders as a final stage
and often introduce distortion, SmoothSinger refines low-quality synthesized
audio directly in a unified framework, mitigating the degradation associated
with two-stage pipelines. The model adopts a reference-guided dual-branch
architecture, using low-quality audio from any baseline system as a reference
to guide the denoising process, enabling more expressive and context-aware
synthesis. Furthermore, it enhances the conventional U-Net with a parallel
low-frequency upsampling path, allowing the model to better capture pitch
contours and long term spectral dependencies. To improve alignment during
training, we replace reference audio with degraded ground truth audio,
addressing temporal mismatch between reference and target signals. Experiments
on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that
SmoothSinger achieves state-of-the-art results in both objective and subjective
evaluations. Extensive ablation studies confirm its effectiveness in reducing
artifacts and improving the naturalness of synthesized voices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach
  for Efficiency and Low Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gavin Lee Goodship, Luis Miralles-Pechuan, Stephen O'Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial Mental Modeling from Limited Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Domain</span> Knowledge-Enhanced <span class="highlight-title">LLM</span>s for Fraud and Concept Drift Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Şenol, Garima Agrawal, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Bayesian Low-Rank Adaptation of <span class="highlight-title">Large Language Model</span>s via
  Stochastic Variational Subspace Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at UAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in
  Multimodal Table Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwen Zhang, Pu Chen, Yin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages and 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">LLM</span>-Assisted Query Understanding for Live Retrieval-Augmented
  Generation <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Xiaoxi Li, Yuyao Zhang, Mengjie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal-Aware Graph Attention Network for Cryptocurrency Transaction
  Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zheng, Bochuan Zhou, Yuping Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pay Attention to Small Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Tom Jacobs, Advait Gadhikar, Rebekka Burkholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time and personalized product <span class="highlight-title">recommendation</span>s for large e-commerce
  platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Tolloso, Davide Bacciu, Shahab Mokarizadeh, Marco Varesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology to provide real-time and personalized product
recommendations for large e-commerce platforms, specifically focusing on
fashion retail. Our approach aims to achieve accurate and scalable
recommendations with minimal response times, ensuring user satisfaction,
leveraging Graph Neural Networks and parsimonious learning methodologies.
Extensive experimentation with datasets from one of the largest e-commerce
platforms demonstrates the effectiveness of our approach in forecasting
purchase sequences and handling multi-interaction scenarios, achieving
efficient personalized recommendations under real-world constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the International
  Conference on Artificial Neural Networks (ICANN) 2025. The final
  authenticated version will be available for purchase through the publisher's
  website. The conference proceedings will be published by Springer in the
  Lecture Notes in Computer Science (LNCS) series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ rQdia: Regularizing Q-Value Distributions With Image Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Lerman, Jing Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CA-I2P: Channel-Adaptive Registration Network with Global Optimal
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Cheng, Jiacheng Deng, Xinjun Li, Xiaotian Yin, Bohao Liao, Baoqun Yin, Wenfei Yang, Tianzhu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection-free methods typically follow a coarse-to-fine pipeline, extracting
image and point cloud features for patch-level matching and refining dense
pixel-to-point correspondences. However, differences in feature channel
attention between images and point clouds may lead to degraded matching
results, ultimately impairing registration accuracy. Furthermore, similar
structures in the scene could lead to redundant correspondences in cross-modal
matching. To address these issues, we propose Channel Adaptive Adjustment
Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances
intra-modal features and suppresses cross-modal sensitivity, while GOS replaces
local selection with global optimization. Experiments on RGB-D Scenes V2 and
7-Scenes demonstrate the superiority of our method, achieving state-of-the-art
performance in image-to-point cloud registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic <span class="highlight-title">Review</span> of Human-AI Co-Creati<span class="highlight-title">vit</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saloni Singh, Koen Hndriks, Drik Heylen, Kim Baraka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Surgical Phase Recognition with Hierarchical Input Dependent
  State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Wu, Tsun-Hsuan Wang, Mathias Lechner, Ramin Hasani, Jennifer A. Eckhoff, Paul Pak, Ozanan R. Meireles, Guy Rosman, Yutong Ban, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical workflow analysis is essential in robot-assisted surgeries, yet the
long duration of such procedures poses significant challenges for comprehensive
video analysis. Recent approaches have predominantly relied on transformer
models; however, their quadratic attention mechanism restricts efficient
processing of lengthy surgical videos. In this paper, we propose a novel
hierarchical input-dependent state space model that leverages the linear
scaling property of state space models to enable decision making on full-length
videos while capturing both local and global dynamics. Our framework
incorporates a temporally consistent visual feature extractor, which appends a
state space model head to a visual feature extractor to propagate temporal
information. The proposed model consists of two key modules: a
local-aggregation state space model block that effectively captures intricate
local dynamics, and a global-relation state space model block that models
temporal dependencies across the entire video. The model is trained using a
hybrid discrete-continuous supervision strategy, where both signals of discrete
phase labels and continuous phase progresses are propagated through the
network. Experiments have shown that our method outperforms the current
state-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on
MICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available
after paper acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Inference AI Systems for Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Duraisamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IXAII: An Interactive Explainable Artificial Intelligence Interface for
  Decision Support Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pauline Speckmann, Mario Nadj, Christian Janiesch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Uniform Weighted Deep Polynomial approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kingsley Yeon, Steven B. Damelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Adapter Design Tradeoffs for Low Resource Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Referring Expressions in Visually Grounded Dialogue with
  Autoregressive Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bram Willemsen, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at XLLM @ ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Encoders Can Rival Large Decoders in Detecting Groundedness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy
  Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Sablica, Kurt Hornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management:
  A Study on Speed Classification in Suzhou 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Fan, Yuli Zhang, Xinheng Wang, Ruiyuan Jiang, Hankang Gu, Dongyao Jia, Shangbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents and publicly releases the Suzhou Urban Road Acoustic
Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive
data-acquisition protocols and annotation guidelines to ensure transparency and
reproducibility of the experimental workflow. To model the coupling between
vehicular noise and driving speed, we propose a bimodal-feature-fusion deep
convolutional neural network (BMCNN). During preprocessing, an adaptive
denoising and normalization strategy is applied to suppress environmental
background interference; in the network architecture, parallel branches extract
Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,
which are subsequently fused via a cross-modal attention mechanism in the
intermediate feature space to fully exploit time-frequency information.
Experimental results demonstrate that BMCNN achieves a classification accuracy
of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic
dataset. Ablation studies and robustness tests on the Suzhou dataset further
validate the contributions of each module to performance improvement and
overfitting mitigation. The proposed acoustics-based speed classification
method can be integrated into smart-city traffic management systems for
real-time noise monitoring and speed estimation, thereby optimizing traffic
flow control, reducing roadside noise pollution, and supporting sustainable
urban planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiLoCoX: A Low-Communication Large-Scale Training Framework for
  Decentralized Cluster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling
  a<span class="highlight-title">cross</span> Perception, Planning, and Safety in Real-World Multimodal Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Men, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From On-chain to Macro: Assessing the Importance of Data Source
  Diversity in Cryptocurrency Market Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgos Demosthenous, Chryssis Georgiou, Eliada Polydorou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ World-aware Planning Narratives Enhance Large Vision-Language Model
  Planner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng QIu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Causal Reasoning in <span class="highlight-title">Large Language Model</span>s: Reality or Mirage? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $T^3$: Multi-level Tree-based Automatic Program Repair with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanming Liu, Xupeng Bu, Zhichao Yan, Ru Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitMark for Infinity: Watermarking Bitwise Autoregressive Image
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Kerner, Michel Meintz, Bihe Zhao, Franziska Boenisch, Adam Dziedzic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art text-to-image models like Infinity generate photorealistic
images at an unprecedented speed. These models operate in a bitwise
autoregressive manner over a discrete set of tokens that is practically
infinite in size. However, their impressive generative power comes with a
growing risk: as their outputs increasingly populate the Internet, they are
likely to be scraped and reused as training data-potentially by the very same
models. This phenomenon has been shown to lead to model collapse, where
repeated training on generated content, especially from the models' own
previous versions, causes a gradual degradation in performance. A promising
mitigation strategy is watermarking, which embeds human-imperceptible yet
detectable signals into generated images-enabling the identification of
generated content. In this work, we introduce BitMark, a robust bitwise
watermarking framework for Infinity. Our method embeds a watermark directly at
the bit level of the token stream across multiple scales (also referred to as
resolutions) during Infinity's image generation process. Our bitwise watermark
subtly influences the bits to preserve visual fidelity and generation speed
while remaining robust against a spectrum of removal techniques. Furthermore,
it exhibits high radioactivity, i.e., when watermarked generated images are
used to train another image generative model, this second model's outputs will
also carry the watermark. The radioactive traces remain detectable even when
only fine-tuning diffusion or image autoregressive models on images watermarked
with our BitMark. Overall, our approach provides a principled step toward
preventing model collapse in image generative models by enabling reliable
detection of generated outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Aware KV Compression For Cost-Effective Long Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Qin, Yan Shu, Peitian Zhang, Kun Lun, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-video understanding (LVU) remains a severe challenge for existing
multimodal large language models (MLLMs), primarily due to the prohibitive
computational cost. Recent approaches have explored KV compression to mitigate
this issue, but they often suffer from significant information loss at high
compression ratios. In this paper, we introduce Video-X^2L, which flexibly
preserves critical video information for each LVU task. Video-X^2L involves two
key operations. The first one is called bi-level KV compression. During the
MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:
low-compression KVs (L-KVs) to capture fine-grained video details and
high-compression KVs (H-KVs) to offer compact video representations. The second
one is called selective KV re-loading. During the MLLM's decoding stage,
Video-X^2L selectively re-loads L-KVs for the most critical video chunks while
using H-KVs for other less important ones. This allows the MLLM to fully
utilize task-specific information while maintaining the overall compactness.
Video-X^2L is simple yet effective: it is free from additional training and
directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L
with a variety of popular LVU benchmarks, including VideoMME, MLVU,
LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L
outperforms existing KV-compression methods by a huge advantage while
substantially saving the computation cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maintaining MTEB: Towards Long Term Usability and Reproducibility of
  Embedding Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Chung, Imene Kerboua, Marton Kardos, Roman Solomatin, Kenneth Enevoldsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical Deep Learning Approach for Minority Instrument Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sechet, Francesca Bugiotti, Matthieu Kowalski, Edouard d'Hérouville, Filip Langiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying instrument activities within audio excerpts is vital in music
information retrieval, with significant implications for music cataloging and
discovery. Prior deep learning endeavors in musical instrument recognition have
predominantly emphasized instrument classes with ample data availability.
Recent studies have demonstrated the applicability of hierarchical
classification in detecting instrument activities in orchestral music, even
with limited fine-grained annotations at the instrument level. Based on the
Hornbostel-Sachs classification, such a hierarchical classification system is
evaluated using the MedleyDB dataset, renowned for its diversity and richness
concerning various instruments and music genres. This work presents various
strategies to integrate hierarchical structures into models and tests a new
class of models for hierarchical music prediction. This study showcases more
reliable coarse-level instrument detection by bridging the gap between detailed
instrument identification and group-level recognition, paving the way for
further advancements in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Digital Audio Effects (DAFx)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver
  Tumour Ablation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuwei Xing, Derek W. Cool, David Tessier, Elvis C. S. Chen, Terry M. Peters, Aaron Fenster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D ultrasound (US) imaging has shown significant benefits in enhancing the
outcomes of percutaneous liver tumour ablation. Its clinical integration is
crucial for transitioning 3D US into the therapeutic domain. However,
challenges of tumour identification in US images continue to hinder its broader
adoption. In this work, we propose a novel framework for integrating 3D US into
the standard ablation workflow. We present a key component, a clinically viable
2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to
reduce registration complexity. To facilitate efficient verification of the
registration workflow, we also propose an intuitive multimodal image
visualization technique. In our study, 2D US-CT/MRI registration achieved a
landmark distance error of approximately 2-4 mm with a runtime of 0.22s per
image pair. Additionally, non-rigid registration reduced the mean alignment
error by approximately 40% compared to rigid registration. Results demonstrated
the efficacy of the proposed 2D US-CT/MRI registration workflow. Our
integration framework advanced the capabilities of 3D US imaging in improving
percutaneous tumour ablation, demonstrating the potential to expand the
therapeutic role of 3D US in clinical interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-Based Spatial-Temporal Counterfactual Outcomes Estimation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Li, Haoang Chi, Mingyu Liu, Wanrong Huang, Liyang Xu, Wenjing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI
  with Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aida Moafi, Danial Moafi, Evgeny M. Mirkes, Gerry P. McCann, Abbas S. Alatrany, Jayanth R. Arnold, Mostafa Mehdipour Ghazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate segmentation of myocardial scars from cardiac MRI is essential
for clinical assessment and treatment planning. In this study, we propose a
robust deep-learning pipeline for fully automated myocardial scar detection and
segmentation by fine-tuning state-of-the-art models. The method explicitly
addresses challenges of label noise from semi-automatic annotations, data
heterogeneity, and class imbalance through the use of Kullback-Leibler loss and
extensive data augmentation. We evaluate the model's performance on both acute
and chronic cases and demonstrate its ability to produce accurate and smooth
segmentations despite noisy labels. In particular, our approach outperforms
state-of-the-art models like nnU-Net and shows strong generalizability in an
out-of-distribution test set, highlighting its robustness across various
imaging conditions and clinical tasks. These results establish a reliable
foundation for automated myocardial scar quantification and support the broader
clinical adoption of deep learning in cardiac imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linearity-based neural network compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silas Dobler, Florian Lemmerich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBConformer: Dual-Branch Convolutional <span class="highlight-title">Transformer</span> for EEG Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Wang, Hongbin Wang, Tianwang Jia, Xingyi He, Siyang Li, Dongrui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Good Are Synthetic Requirements ? Evaluating <span class="highlight-title">LLM</span>-Generated Datasets
  for AI4RE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelkarim El-Hajjami, Camille Salinesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV
  Deconfliction under Observation-Space Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Kumar Panda, Adolfo Perrusquia, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Policy Switching for Antifragile Reinforcement Learning for UAV
  Deconfliction in Adversarial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Kumar Panda, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progtuning: Progressive Fine-tuning Framework for <span class="highlight-title">Transformer</span>-based
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuang Ji, Zhendong Zhao, Xiaojun Chen, Xin Zhao, Zeyao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICONIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPFormer-Video<span class="highlight-title">LLM</span>: Enhancing <span class="highlight-title">Multi-modal</span> Video Understanding for
  Multi-shot Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Liang, Jile Jiao, Zhicheng Wang, Xuetao Feng, Zixuan Ye, Yuan Wang, Hao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Large Language Models (VideoLLMs) have demonstrated remarkable
understanding capabilities, but are found struggling to tackle multi-shot
scenarios,e.g., video clips with varying camera angles or scene changes. This
challenge can render failures such as instance identity forgetting and key
frame negligence. In this work, we first attribute the challenge to the lack of
multi-shot annotations among existing datasets and therefore we introduce a new
dataset termed MultiClip-Bench, featuring dense descriptions and
instruction-based question-answering pairs tailored for multi-shot scenarios.
We empirically find that the training set significantly boosts the multi-shot
performance, while the testing benchmark provides a reliable measure of the
model capability in multi-shot scenarios. By further analyzing and discovering
that current models only encode instance features in a discrete or lossy
manner, at the risk of missing identity information, we then contribute a new
model IPFormer-VideoLLM. Its key idea is the injection of instance-level
features as instance prompts through an efficient attention-based connector.
This allows for the aggregation of instance-specific information across scenes.
Experiments demonstrate that our proposed dataset and model not only enhance
the multi-scene video understanding significantly, but also offer distinct
advantages across various video benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing
  Detection Using Adaptive HTML Component Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Castaño, Eduardo Fidalgo, Enrique Alegre, Rocio Alaiz-Rodríguez, Raul Orduna, Francesco Zola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phishing attacks pose a significant cybersecurity threat, evolving rapidly to
bypass detection mechanisms and exploit human vulnerabilities. This paper
introduces PhishKey to address the challenges of adaptability, robustness, and
efficiency. PhishKey is a novel phishing detection method using automatic
feature extraction from hybrid sources. PhishKey combines character-level
processing with Convolutional Neural Networks (CNN) for URL classification, and
a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at
the word level. CAPE reduces noise and ensures complete sample processing
avoiding crop operations on the input data. The predictions from both modules
are integrated using a soft-voting ensemble to achieve more accurate and
reliable classifications. Experimental evaluations on four state-of-the-art
datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1
Score and shows strong resistance to adversarial manipulations such as
injection attacks with minimal performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Hierarchical Concept Reasoning through Attention-Guided
  Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Debot, Pietro Barbiero, Gabriele Dominici, Giuseppe Marra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for
  Real-time Community Question Answering in Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinwen Chen, Wenbiao Tao, Zhiwei Zhu, Mingfan Xi, Liangzhong Guo, Yuan Wang, Wei Wang, Yunshi Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures. Accepted at ACL 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xenia Heilmann, Luca Corbucci, Mattia Cerrato, Anna Monreale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and
  Solutions <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangzhe Peng, Kaiyuan Gao, Liang He, Yuheng Cong, Haiguang Liu, Kun He, Lijun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for
  Efficient Egocentric Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjoy Chowdhury, Subrata Biswas, Sayan Nag, Tushar Nagarajan, Calvin Murdock, Ishwarya Ananthabhotla, Yijun Qian, Vamsi Krishna Ithapu, Dinesh Manocha, Ruohan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-supervised Scalable Unified Framework for E-commerce Query
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyuan Yuan, Chong Zhang, Zheng Fang, Ming Pang, Xue Jiang, Changping Peng, Zhangang Lin, Ching Law
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Diffusion</span>-Based Image Editing Faithfulness via Guidance and
  Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansam Cho, Seoung Bum Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Skill Discovery via Regret-Aware Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Ming Zhou, Shaopeng Zhai, Ying Sun, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative
  Autonomous Driving with Adaptive Long-Tail Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei You, Pei Li, Zhuoyu Jiang, Zilin Huang, Rui Gan, Haotian Shi, Bin Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring robust planning and decision-making under rare, diverse, and
visually degraded long-tail scenarios remains a fundamental challenge for
autonomous driving in urban environments. This issue becomes more critical in
cooperative settings, where vehicles and infrastructure jointly perceive and
reason across complex environments. To address this challenge, we propose
V2X-REALM, a vision-language model (VLM)-based framework with adaptive
multimodal learning for robust cooperative autonomous driving under long-tail
scenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven
long-tail scenario generation and evaluation pipeline that leverages foundation
models to synthesize realistic long-tail conditions such as snow and fog across
vehicle- and infrastructure-side views, enriching training diversity
efficiently; (ii) a gated multi-scenario adaptive attention module that
modulates the visual stream using scenario priors to recalibrate ambiguous or
corrupted features; and (iii) a multi-task scenario-aware contrastive learning
objective that improves multimodal alignment and promotes cross-scenario
feature separability. Extensive experiments demonstrate that V2X-REALM
significantly outperforms existing baselines in robustness, semantic reasoning,
safety, and planning accuracy under complex, challenging driving conditions,
advancing the scalability of end-to-end cooperative autonomous driving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 technical page followed by references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s Acing Chartered Accountancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jatin Gupta, Akhil Sharma, Saransh Singhania, Mohammad Adnan, Sakshi Deo, Ali Imam Abidi, Keshav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at MoStart 2025: International Conference on
  Digital Transformation in Education and Applications of Artificial
  Intelligence, Bosnia and Herzegovina, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Prompt Alignment for Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyan Ma, Yiran He, Bin Sun, Shutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has been widely adopted to efficiently adapt vision-language
models (VLMs) like CLIP for various downstream tasks. Despite their success,
current VLM-based facial expression recognition (FER) methods struggle to
capture fine-grained textual-visual relationships, which are essential for
distinguishing subtle differences between facial expressions. To address this
challenge, we propose a multimodal prompt alignment framework for FER, called
MPA-FER, that provides fine-grained semantic guidance to the learning process
of prompted visual features, resulting in more precise and interpretable
representations. Specifically, we introduce a multi-granularity hard prompt
generation strategy that utilizes a large language model (LLM) like ChatGPT to
generate detailed descriptions for each facial expression. The LLM-based
external knowledge is injected into the soft prompts by minimizing the feature
discrepancy between the soft prompts and the hard prompts. To preserve the
generalization abilities of the pretrained CLIP model, our approach
incorporates prototype-guided visual feature alignment, ensuring that the
prompted visual features from the frozen image encoder align closely with
class-specific prototypes. Additionally, we propose a cross-modal global-local
alignment module that focuses on expression-relevant facial features, further
improving the alignment between textual and visual features. Extensive
experiments demonstrate our framework outperforms state-of-the-art methods on
three FER benchmark datasets, while retaining the benefits of the pretrained
model and minimizing computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAC: A Framework for Measuring and Inducing Personality Traits in <span class="highlight-title">LLM</span>s
  with Dynamic Intensity Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adithya Chittem, Aishna Shrivastava, Sai Tarun Pendela, Jagat Sesh Challa, Dhruv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Segment Anything</span> in Pathology Images with Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Chen, Junlin Hou, Liqi Lin, Yihui Wang, Yequan Bie, Xi Wang, Yanning Zhou, Ronald Cheong Kin Chan, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in
  Heterogeneous Graphs <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Zheng, Yaming Yang, Ziyu Guan, Wei Zhao, Weigang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world networks usually have a property of node heterophily, that is, the
connected nodes usually have different features or different labels. This
heterophily issue has been extensively studied in homogeneous graphs but
remains under-explored in heterogeneous graphs, where there are multiple types
of nodes and edges. Capturing node heterophily in heterogeneous graphs is very
challenging since both node/edge heterogeneity and node heterophily should be
carefully taken into consideration. Existing methods typically convert
heterogeneous graphs into homogeneous ones to learn node heterophily, which
will inevitably lose the potential heterophily conveyed by heterogeneous
relations. To bridge this gap, we propose Relation-Aware Separation of
Homophily and Heterophily (RASH), a novel contrastive learning framework that
explicitly models high-order semantics of heterogeneous interactions and
adaptively separates homophilic and heterophilic patterns. Particularly, RASH
introduces dual heterogeneous hypergraphs to encode multi-relational bipartite
subgraphs and dynamically constructs homophilic graphs and heterophilic graphs
based on relation importance. A multi-relation contrastive loss is designed to
align heterogeneous and homophilic/heterophilic views by maximizing mutual
information. In this way, RASH simultaneously resolves the challenges of
heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on
benchmark datasets demonstrate the effectiveness of RASH across various
downstream tasks. The code is available at:
https://github.com/zhengziyu77/RASH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan
  Face Aging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Liu, Dafeng Zhang, Gengchen Li, Shizhuo Liu, Yongqi Song, Senmao Li, Shiqi Yang, Boqian Li, Kai Wang, Yaxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingling Cai, Kang Zhao, Hangjie Yuan, Xiang Wang, Yingya Zhang, Kejie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Zero-shot video editing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallels Between VLA Model Post-Training and Human Motor Learning:
  Progress, Challenges, and Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duan, Fu-Chao Xie, Wen-Kai Wang, Si-Cheng Wang, Ling-Yun Li, Tian Tu, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language-action (VLA) models extend vision-language models (VLM) by
integrating action generation modules for robotic manipulation. Leveraging
strengths of VLM in vision perception and instruction understanding, VLA models
exhibit promising generalization across diverse manipulation tasks. However,
applications demanding high precision and accuracy reveal performance gaps
without further adaptation. Evidence from multiple domains highlights the
critical role of post-training to align foundational models with downstream
applications, spurring extensive research on post-training VLA models. VLA
model post-training aims to address the challenge of improving an embodiment's
ability to interact with the environment for the given tasks, analogous to the
process of humans motor skills acquisition. Accordingly, this paper reviews
post-training strategies for VLA models through the lens of human motor
learning, focusing on three dimensions: environments, embodiments, and tasks. A
structured taxonomy is introduced aligned with human learning mechanisms: (1)
enhancing environmental perception, (2) improving embodiment awareness, (3)
deepening task comprehension, and (4) multi-component integration. Finally, key
challenges and trends in post-training VLA models are identified, establishing
a conceptual framework to guide future research. This work delivers both a
comprehensive overview of current VLA model post-training methods from a human
motor learning perspective and practical insights for VLA model development.
(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidence-based diagnostic reasoning with multi-agent copilot for human
  pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkuan Chen, Luca L. Weishaupt, Drew F. K. Williamson, Richard J. Chen, Tong Ding, Bowen Chen, Anurag Vaidya, Long Phi Le, Guillaume Jaume, Ming Y. Lu, Faisal Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual,
  Auditory, and Textual Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiman Zhang, Ziheng Luo, Qiangyu Yan, Wei He, Borui Jiang, Xinghao Chen, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antibody Design and Optimization with Multi-scale Equivariant Graph
  <span class="highlight-title">Diffusion</span> Models for Accurate Complex Antigen Binding <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiameng Chen, Xiantao Cai, Jia Wu, Wenbin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, accepted at IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Reactive Safety: Risk-Aware <span class="highlight-title">LLM</span> Alignment via Long-Horizon
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenkai Sun, Denghui Zhang, ChengXiang Zhai, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware <span class="highlight-title">Diffusion</span>
  and Temporal Video Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donggoo Kang, Jangyeong Kim, Dasol Jeong, Junyoung Choi, Jeonga Wi, Hyunmin Lee, Joonho Gwon, Joonki Paik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Representation Learning for Additive Rule Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahrzad Behzadimanesh, Pierre Le Bodic, Geoffrey I. Webb, Mario Boley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-guided Chemical Process Optimization with a Multi-Agent Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (main manuscript without references), 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising Language Models for Downstream Tasks: A Post-Training
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Namazi, Alexander Nemecek, Erman Ayday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting with Phonemes: Enhancing <span class="highlight-title">LLM</span>s' Multilinguality for Non-Latin
  Script Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang H Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multilingual LLMs have achieved remarkable performance across
benchmarks, we find they continue to underperform on non-Latin script languages
across contemporary LLM families. This discrepancy arises from the fact that
LLMs are pretrained with orthographic scripts, which are dominated by Latin
characters that obscure their shared phonology with non-Latin scripts. We
propose leveraging phonemic transcriptions as complementary signals to induce
script-invariant representations. Our study demonstrates that integrating
phonemic signals improves performance across both non-Latin and Latin script
languages, with a particularly significant impact on closing the performance
gap between the two. Through detailed experiments, we show that phonemic and
orthographic scripts retrieve distinct examples for in-context learning (ICL).
This motivates our proposed Mixed-ICL retrieval strategy, where further
aggregation from both leads to our significant performance improvements for
both Latin script languages (up to 12.6%) and non-Latin script languages (up to
15.1%) compared to randomized ICL retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 (Main Conference). This version contains minor
  improvements to the camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhay Kumar, Kunal Verma, Omkar More
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in audio deepfake technology offers benefits like AI assistants,
better accessibility for speech impairments, and enhanced entertainment.
However, it also poses significant risks to security, privacy, and trust in
digital communications. Detecting and mitigating these threats requires
comprehensive datasets. Existing datasets lack diverse ethnic accents, making
them inadequate for many real-world scenarios. Consequently, models trained on
these datasets struggle to detect audio deepfakes in diverse linguistic and
cultural contexts such as in South-Asian countries. Ironically, there is a
stark lack of South-Asian speaker samples in the existing datasets despite
constituting a quarter of the worlds population. This work introduces the
IndieFake Dataset (IFD), featuring 27.17 hours of bonafide and deepfake audio
from 50 English speaking Indian speakers. IFD offers balanced data distribution
and includes speaker-level characterization, absent in datasets like ASVspoof21
(DF). We evaluated various baselines on IFD against existing ASVspoof21 (DF)
and In-The-Wild (ITW) datasets. IFD outperforms ASVspoof21 (DF) and proves to
be more challenging compared to benchmark ITW dataset. The complete dataset,
along with documentation and sample reference clips, is publicly accessible for
research use on project website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://indie-fake-dataset.netlify.app/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Memories to Maps: Mechanisms of In-Context Reinforcement Learning
  in <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Fang, Kanaka Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans and animals show remarkable learning efficiency, adapting to new
environments with minimal experience. This capability is not well captured by
standard reinforcement learning algorithms that rely on incremental value
updates. Rapid adaptation likely depends on episodic memory -- the ability to
retrieve specific past experiences to guide decisions in novel contexts.
Transformers provide a useful setting for studying these questions because of
their ability to learn rapidly in-context and because their key-value
architecture resembles episodic memory systems in the brain. We train a
transformer to in-context reinforcement learn in a distribution of planning
tasks inspired by rodent behavior. We then characterize the learning algorithms
that emerge in the model. We first find that representation learning is
supported by in-context structure learning and cross-context alignment, where
representations are aligned across environments with different sensory stimuli.
We next demonstrate that the reinforcement learning strategies developed by the
model are not interpretable as standard model-free or model-based planning.
Instead, we show that in-context reinforcement learning is supported by caching
intermediate computations within the model's memory tokens, which are then
accessed at decision time. Overall, we find that memory may serve as a
computational resource, storing both raw experience and cached computations to
support flexible behavior. Furthermore, the representations developed in the
model resemble computations associated with the hippocampal-entorhinal system
in the brain, suggesting that our findings may be relevant for natural
cognition. Taken together, our work offers a mechanistic hypothesis for the
rapid adaptation that underlies in-context learning in artificial and natural
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updates: added other funding sources; formatted title correctly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Strategies Emerge Rationally 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wurgaft, Ekdeep Singh Lubana, Core Francisco Park, Hidenori Tanaka, Gautam Reddy, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, where the prior matches
the underlying task distribution. Adopting the normative lens of rational
analysis, where a learner's behavior is explained as an optimal adaptation to
data given computational constraints, we develop a hierarchical Bayesian
framework that almost perfectly predicts Transformer next-token predictions
throughout training -- without assuming access to its weights. Under this
framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transitioning from generalization to memorization as task
diversity increases. Overall, our work advances an explanatory and predictive
account of ICL grounded in tradeoffs between strategy loss and complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, Irwin King, Fakhri Karray, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fake it till You Make it: Reward Modeling as Discriminative Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Materialist: Physically Based Editing Using Single-Image Inverse
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lezhong Wang, Duc Minh Tran, Ruiqi Cui, Thomson TG, Anders Bjorholm Dahl, Siavash Arjomand Bigdeli, Jeppe Revall Frisvad, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving physically consistent image editing remains a significant challenge
in computer vision. Existing image editing methods typically rely on neural
networks, which struggle to accurately handle shadows and refractions.
Conversely, physics-based inverse rendering often requires multi-view
optimization, limiting its practicality in single-image scenarios. In this
paper, we propose Materialist, a method combining a learning-based approach
with physically based progressive differentiable rendering. Given an image, our
method leverages neural networks to predict initial material properties.
Progressive differentiable rendering is then used to optimize the environment
map and refine the material properties with the goal of closely matching the
rendered result to the input image. Our approach enables a range of
applications, including material editing, object insertion, and relighting,
while also introducing an effective method for editing material transparency
without requiring full scene geometry. Furthermore, Our envmap estimation
method also achieves state-of-the-art performance, further enhancing the
accuracy of image editing task. Experiments demonstrate strong performance
across synthetic and real-world datasets, excelling even on challenging
out-of-domain images. Project website:
https://lez-s.github.io/materialist_project/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add acknowledgements, more authors and more results. Project website:
  https://lez-s.github.io/materialist_project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability of <span class="highlight-title">Large Language Model</span>s using SMILE: Statistical
  Model-agnostic Interpretability with Local Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The submission contains incorrect references that require substantial
  revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved
  Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08005v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08005v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection holds significant importance across many
applications. While semantic and domain-shift OOD problems are well-studied,
this work focuses on covariate shifts - subtle variations in the data
distribution that can degrade machine learning performance. We hypothesize that
detecting these subtle shifts can improve our understanding of in-distribution
boundaries, ultimately improving OOD detection. In adversarial discriminators
trained with Batch Normalization (BN), real and adversarial samples form
distinct domains with unique batch statistics - a property we exploit for OOD
detection. We introduce DisCoPatch, an unsupervised Adversarial Variational
Autoencoder (VAE) framework that harnesses this mechanism. During inference,
batches consist of patches from the same image, ensuring a consistent data
distribution that allows the model to rely on batch statistics. DisCoPatch uses
the VAE's suboptimal outputs (generated and reconstructed) as negative samples
to train the discriminator, thereby improving its ability to delineate the
boundary between in-distribution samples and covariate shifts. By tightening
this boundary, DisCoPatch achieves state-of-the-art results in public OOD
detection benchmarks. The proposed model not only excels in detecting covariate
shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior
methods on public Near-OOD (95.0%) benchmarks. With a compact model size of
25MB, it achieves high OOD detection performance at notably lower latency than
existing methods, making it an efficient and practical solution for real-world
OOD detection applications. The code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trac<span class="highlight-title">LLM</span>: A Generic Framework for Attributing Long Context <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long context large language models (LLMs) are deployed in many real-world
applications such as RAG, agent, and broad LLM-integrated applications. Given
an instruction and a long context (e.g., documents, PDF files, webpages), a
long context LLM can generate an output grounded in the provided context,
aiming to provide more accurate, up-to-date, and verifiable outputs while
reducing hallucinations and unsupported claims. This raises a research
question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)
in the context that contribute most to or are responsible for the generated
output by an LLM? This process, which we call context traceback, has various
real-world applications, such as 1) debugging LLM-based systems, 2) conducting
post-attack forensic analysis for attacks (e.g., prompt injection attack,
knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources
to enhance the trust of users towards outputs generated by LLMs. When applied
to context traceback for long context LLMs, existing feature attribution
methods such as Shapley have sub-optimal performance and/or incur a large
computational cost. In this work, we develop TracLLM, the first generic context
traceback framework tailored to long context LLMs. Our framework can improve
the effectiveness and efficiency of existing feature attribution methods. To
improve the efficiency, we develop an informed search based algorithm in
TracLLM. We also develop contribution score ensemble/denoising techniques to
improve the accuracy of TracLLM. Our evaluation results show TracLLM can
effectively identify texts in a long context that lead to the output of an LLM.
Our code and data are at: https://github.com/Wang-Yanting/TracLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in USENIX Security Symposium 2025. The code and data are
  at: https://github.com/Wang-Yanting/TracLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning as Computationally Constrained Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning of Lab Values via Masked AutoEncoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Restrepo, Chenwei Wu, Yueran Jia, Jaden K. Sun, Jack Gallifant, Catherine G. Bielick, Yugang Jia, Leo A. Celi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate imputation of missing laboratory values in electronic health records
(EHRs) is critical to enable robust clinical predictions and reduce biases in
AI systems in healthcare. Existing methods, such as XGBoost, softimpute, GAIN,
Expectation Maximization (EM), and MICE, struggle to model the complex temporal
and contextual dependencies in EHR data, particularly in underrepresented
groups. In this work, we propose Lab-MAE, a novel transformer-based masked
autoencoder framework that leverages self-supervised learning for the
imputation of continuous sequential lab values. Lab-MAE introduces a structured
encoding scheme that jointly models laboratory test values and their
corresponding timestamps, enabling explicit capturing temporal dependencies.
Empirical evaluation on the MIMIC-IV dataset demonstrates that Lab-MAE
significantly outperforms state-of-the-art baselines such as XGBoost,
softimpute, GAIN, EM, and MICE across multiple metrics, including root mean
square error (RMSE), R-squared (R2), and Wasserstein distance (WD). Notably,
Lab-MAE achieves equitable performance across demographic groups of patients,
advancing fairness in clinical predictions. We further investigate the role of
follow-up laboratory values as potential shortcut features, revealing Lab-MAE's
robustness in scenarios where such data is unavailable. The findings suggest
that our transformer-based architecture, adapted to the characteristics of EHR
data, offers a foundation model for more accurate and fair clinical imputation.
In addition, we measure and compare the carbon footprint of Lab-MAE with the a
XGBoost model, highlighting its environmental requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages of main text, 11 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Preprocessing for <span class="highlight-title">LLM</span>-based Malware Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Marais, Tony Quertier, Grégoire Barrue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a context of malware analysis, numerous approaches rely on Artificial
Intelligence to handle a large volume of data. However, these techniques focus
on data view (images, sequences) and not on an expert's view. Noticing this
issue, we propose a preprocessing that focuses on expert knowledge to improve
malware semantic analysis and result interpretability. We propose a new
preprocessing method which creates JSON reports for Portable Executable files.
These reports gather features from both static and behavioral analysis, and
incorporate packer signature detection, MITRE ATT\&CK and Malware Behavior
Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a
semantic representation of binary files, understandable by malware analysts,
and that can enhance AI models' explainability for malicious files analysis.
Using this preprocessing to train a Large Language Model for Malware
classification, we achieve a weighted-average F1-score of 0.94 on a complex
dataset, representative of market reality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PuriDefense: Randomized Local Implicit Adversarial Purification for
  Defending Black-box Query-based Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Guo, Xiang Li, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model's architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recall and Refine: A Simple but Effective Source-free Open-set <span class="highlight-title">Domain</span>
  Adaptation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Nejjar, Hao Dong, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source
domain to an unlabeled target domain, where novel classes - also referred to as
target-private unknown classes - are present. Source-free Open-set Domain
Adaptation (SF-OSDA) methods address OSDA without accessing labeled source
data, making them particularly relevant under privacy constraints. However,
SF-OSDA presents significant challenges due to distribution shifts and the
introduction of novel classes. Existing SF-OSDA methods typically rely on
thresholding the prediction entropy of a sample to identify it as either a
known or unknown class, but fail to explicitly learn discriminative features
for the target-private unknown classes. We propose Recall and Refine (RRDA), a
novel SF-OSDA framework designed to address these limitations by explicitly
learning features for target-private unknown classes. RRDA employs a two-stage
process. First, we enhance the model's capacity to recognize unknown classes by
training a target classifier with an additional decision boundary,guided by
synthetic samples generated from target domain features. This enables the
classifier to effectively separate known and unknown classes. Second, we adapt
the entire model to the target domain, addressing both domain shifts and
distinguishability to unknown classes. Any off-the-shelf source-free domain
adaptation method (e.g. SHOT, AaD) can be seamlessly integrated into our
framework at this stage. Extensive experiments on three benchmark datasets
demonstrate that RRDA significantly outperforms existing SF-OSDA and OSDA
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Scene Graph for Ultrasound Image Explanation and Scanning
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuesong Li, Dianye Huang, Yameng Zhang, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding medical ultrasound imaging remains a long-standing challenge
due to significant visual variability caused by differences in imaging and
acquisition parameters. Recent advancements in large language models (LLMs)
have been used to automatically generate terminology-rich summaries orientated
to clinicians with sufficient physiological knowledge. Nevertheless, the
increasing demand for improved ultrasound interpretability and basic scanning
guidance among non-expert users, e.g., in point-of-care settings, has not yet
been explored. In this study, we first introduce the scene graph (SG) for
ultrasound images to explain image content to ordinary and provide guidance for
ultrasound scanning. The ultrasound SG is first computed using a
transformer-based one-stage method, eliminating the need for explicit object
detection. To generate a graspable image explanation for ordinary, the user
query is then used to further refine the abstract SG representation through
LLMs. Additionally, the predicted SG is explored for its potential in guiding
ultrasound scanning toward missing anatomies within the current imaging view,
assisting ordinary users in achieving more standardized and complete anatomical
exploration. The effectiveness of this SG-based image explanation and scanning
guidance has been validated on images from the left and right neck regions,
including the carotid and thyroid, across five volunteers. The results
demonstrate the potential of the method to maximally democratize ultrasound by
enhancing its interpretability and usability for ordinaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thinkless: <span class="highlight-title">LLM</span> Learns When to Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongfan Fang, Xinyin Ma, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning Language Models, capable of extended chain-of-thought reasoning,
have demonstrated remarkable performance on tasks requiring complex logical
inference. However, applying elaborate reasoning for all queries often results
in substantial computational inefficiencies, particularly when many problems
admit straightforward solutions. This motivates an open question: Can LLMs
learn when to think? To answer this, we propose Thinkless, a learnable
framework that empowers an LLM to adaptively select between short-form and
long-form reasoning, based on both task complexity and the model's ability.
Thinkless is trained under a reinforcement learning paradigm and employs two
control tokens, <short> for concise responses and <think> for detailed
reasoning. At the core of our method is a Decoupled Group Relative Policy
Optimization (DeGRPO) algorithm, which decomposes the learning objective of
hybrid reasoning into two components: (1) a control token loss that governs the
selection of the reasoning mode, and (2) a response loss that improves the
accuracy of the generated answers. This decoupled formulation enables
fine-grained control over the contributions of each objective, stabilizing
training and effectively preventing collapse observed in vanilla GRPO.
Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and
GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -
90%, significantly improving the efficiency of Reasoning Language Models. The
code is available at https://github.com/VainF/Thinkless
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy Matching: Unifying Flow Matching and Energy-Based Models for
  Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10612v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10612v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Lea Bogensperger, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most widely used generative models map noise and data distributions by
matching flows or scores. However, they struggle to incorporate partial
observations and additional priors--something energy-based models (EBMs) handle
elegantly by simply adding corresponding scalar energy terms. We address this
issue by proposing Energy Matching, a framework that endows flow-based
approaches with the flexibility of EBMs. Far from the data manifold, samples
move along curl-free, optimal transport paths from noise to data. As they
approach the data manifold, an entropic energy term guides the system into a
Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in
terms of fidelity, while retaining simulation-free training of transport-based
approaches away from the data manifold. Furthermore, we leverage the method's
flexibility to introduce an interaction energy that supports diverse mode
exploration, which we demonstrate in a controlled protein-generation setting.
Our approach focuses on learning a scalar potential energy--without
time-conditioning, auxiliary generators, or additional networks--which marks a
significant departure from recent EBM methods. We believe that this simplified
framework significantly advances EBMs capabilities and paves the way for their
wider adoption in generative modeling across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lagrangian Index Policy for Restless Bandits with Average Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Avrachenkov, Vivek S. Borkar, Pratik Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Lagrange Index Policy (LIP) for restless multi-armed bandits
with long-run average reward. In particular, we compare the performance of LIP
with the performance of the Whittle Index Policy (WIP), both heuristic policies
known to be asymptotically optimal under certain natural conditions. Even
though in most cases their performances are very similar, in the cases when WIP
shows bad performance, LIP continues to perform very well. We then propose
reinforcement learning algorithms, both tabular and NN-based, to obtain online
learning schemes for LIP in the model-free setting. The proposed reinforcement
learning schemes for LIP require significantly less memory than the analogous
schemes for WIP. We calculate analytically the Lagrange index for the restart
model, which applies to the optimal web crawling and the minimization of the
weighted age of information. We also give a new proof of asymptotic optimality
in case of homogeneous arms as the number of arms goes to infinity, based on
exchangeability and de Finetti's theorem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A GREAT Architecture for Edge-Based Graph Problems Like TSP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Attila Lischka, Filip Rydin, Jiaming Wu, Morteza Haghir Chehreghani, Balázs Kulcsár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last years, many learning-based approaches have been proposed to
tackle combinatorial optimization problems such as routing problems. Many of
these approaches are based on graph neural networks (GNNs) or related
transformers, operating on the Euclidean coordinates representing the routing
problems. However, models operating on Euclidean coordinates are ill-suited for
non-Euclidean, asymmetric problem instances that are often found in real-world
settings. To overcome this limitation, we propose a novel GNN-based and
edge-focused neural model called Graph Edge Attention Network (GREAT). Using
GREAT as an encoder to capture the properties of a routing problem instance, we
build a reinforcement learning framework which we apply to Euclidean and
non-Euclidean variants of vehicle routing problems such as Traveling Salesman
Problem, Capacitated Vehicle Routing Problem and Orienteering Problem. Our
framework is among the first to tackle non-Euclidean variants of these problems
and achieves competitive results among learning-based solvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ These Are Not All the Features You Are Looking For: A Fundamental
  Bottleneck in Supervised Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Alice Yang, Jianyu Zhang, Léon Bottou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid Gyroscope Calibration: A Deep Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Stolero, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-cost gyroscope calibration is essential for ensuring the accuracy and
reliability of gyroscope measurements. Stationary calibration estimates the
deterministic parts of measurement errors. To this end, a common practice is to
average the gyroscope readings during a predefined period and estimate the
gyroscope bias. Calibration duration plays a crucial role in performance,
therefore, longer periods are preferred. However, some applications require
quick startup times and calibration is therefore allowed only for a short time.
In this work, we focus on reducing low-cost gyroscope calibration time using
deep learning methods. We propose an end-to-end convolutional neural network
for the application of gyroscope calibration. We explore the possibilities of
using multiple real and virtual gyroscopes to improve the calibration
performance of single gyroscopes. To train and validate our approach, we
recorded a dataset consisting of 186.6 hours of gyroscope readings, using 36
gyroscopes of four different brands. We also created a virtual dataset
consisting of simulated gyroscope readings. The six datasets were used to
evaluate our proposed approach. One of our key achievements in this work is
reducing gyroscope calibration time by up to 89% using three low-cost
gyroscopes. Our dataset is publicly available to allow reproducibility of our
work and to increase research in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 14 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have witnessed a surge in
the development of advanced reasoning paradigms, which are now being integrated
into multimodal large language models (MLLMs). However, existing approaches
often fall short: methods solely employing reinforcement learning (RL) can
struggle with sample inefficiency and activating entirely absent reasoning
capabilities, while conventional pipelines that initiate with a cold-start
supervised fine-tuning (SFT) phase before RL may restrict the model's
exploratory capacity and face suboptimal convergence. In this work, we
introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and
\textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike
conventional approaches, Metis-RISE distinctively omits an initial SFT stage,
beginning instead with an RL phase (e.g., using a Group Relative Policy
Optimization variant) to incentivize and activate the model's latent reasoning
capacity. Subsequently, the targeted SFT stage addresses two key challenges
identified during RL: (1) \textit{inefficient trajectory sampling} for tasks
where the model possesses but inconsistently applies correct reasoning, which
we tackle using self-distilled reasoning trajectories from the RL model itself;
and (2) \textit{fundamental capability absence}, which we address by injecting
expert-augmented knowledge for prompts where the model entirely fails. This
strategic application of RL for incentivization followed by SFT for enhancement
forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B
parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard
demonstrate that both models achieve state-of-the-art performance among
similar-sized models, with the 72B version ranking fourth overall. Please refer
to our project page for open-source information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/MM-Thinking/Metis-RISE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is my Data in your AI Model? Membership Inference Test with Application
  to Face Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel DeAlcala, Aythami Morales, Julian Fierrez, Gonzalo Mancera, Ruben Tolosana, Javier Ortega-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces the Membership Inference Test (MINT), a novel
approach that aims to empirically assess if given data was used during the
training of AI/ML models. Specifically, we propose two MINT architectures
designed to learn the distinct activation patterns that emerge when an Audited
Model is exposed to data used during its training process. These architectures
are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks
(CNNs). The experimental framework focuses on the challenging task of Face
Recognition, considering three state-of-the-art Face Recognition systems.
Experiments are carried out using six publicly available databases, comprising
over 22 million face images in total. Different experimental scenarios are
considered depending on the context of the AI model to test. Our proposed MINT
approach achieves promising results, with up to 90\% accuracy, indicating the
potential to recognize if an AI model has been trained with specific data. The
proposed MINT approach can serve to enforce privacy and fairness in several AI
applications, e.g., revealing if sensitive or private data was used for
training or tuning Large Language Models (LLMs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages main text and 2 pages appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HERMES: temporal-coHERent long-forM understanding with Episodes and
  Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17443v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17443v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Shang-Hong Lai, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form video understanding presents unique challenges that extend beyond
traditional short-video analysis approaches, particularly in capturing
long-range dependencies, processing redundant information efficiently, and
extracting high-level semantic concepts. To address these challenges, we
propose a novel approach that more accurately reflects human cognition. This
paper introduces HERMES: temporal-coHERent long-forM understanding with
Episodes and Semantics, featuring two versatile modules that can enhance
existing video-language models or operate as a standalone system. Our Episodic
COmpressor (ECO) efficiently aggregates representations from micro to
semi-macro levels, reducing computational overhead while preserving temporal
dependencies. Our Semantics ReTRiever (SeTR) enriches these representations
with semantic information by focusing on broader context, dramatically reducing
feature dimensionality while preserving relevant macro-level information. We
demonstrate that these modules can be seamlessly integrated into existing SOTA
models, consistently improving their performance while reducing inference
latency by up to 43% and memory usage by 46%. As a standalone system, HERMES
achieves state-of-the-art performance across multiple long-video understanding
benchmarks in both zero-shot and fully-supervised settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICCV 2025. Project page:
  https://joslefaure.github.io/assets/html/hermes.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Provable (In)Secure Model Weight Release Schemes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yang, Bintao Tang, Yuhao Wang, Zimo Ji, Terry Jingchen Zhang, Wenyuan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent secure weight release schemes claim to enable open-source model
distribution while protecting model ownership and preventing misuse. However,
these approaches lack rigorous security foundations and provide only informal
security guarantees. Inspired by established works in cryptography, we
formalize the security of weight release schemes by introducing several
concrete security definitions. We then demonstrate our definition's utility
through a case study of TaylorMLP, a prominent secure weight release scheme.
Our analysis reveals vulnerabilities that allow parameter extraction thus
showing that TaylorMLP fails to achieve its informal security goals. We hope
this work will advocate for rigorous research at the intersection of machine
learning and security communities and provide a blueprint for how future weight
release schemes should be designed and evaluated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures; author name typos and institutions corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning
  of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11277v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11277v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Adaptive Memory-Based Optimization for Enhanced
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitao Qin, Yucong Luo, Yihang Lu, Zhibo Chu, Xianwei Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge
from external knowledge bases into models, has emerged as a promising approach
to enhancing response accuracy while mitigating factual errors and
hallucinations. This method has been widely applied in tasks such as Question
Answering (QA). However, existing RAG methods struggle with open-domain QA
tasks because they perform independent retrieval operations and directly
incorporate the retrieved information into generation without maintaining a
summarizing memory or using adaptive retrieval strategies, leading to noise
from redundant information and insufficient information integration. To address
these challenges, we propose Adaptive memory-based optimization for enhanced
RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory
Updater, an Adaptive Information Collector, and a Multi-granular Content
Filter, working together within an iterative memory updating paradigm.
Specifically, Amber integrates and optimizes the language model's memory
through a multi-agent collaborative approach, ensuring comprehensive knowledge
integration from previous retrieval steps. It dynamically adjusts retrieval
queries and decides when to stop retrieval based on the accumulated knowledge,
enhancing retrieval efficiency and effectiveness. Additionally, it reduces
noise by filtering irrelevant content at multiple levels, retaining essential
information to improve overall model performance. We conduct extensive
experiments on several open-domain QA datasets, and the results demonstrate the
superiority and effectiveness of our method and its components. The source code
is available \footnote{https://anonymous.4open.science/r/Amber-B203/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages. arXiv admin note: text overlap with arXiv:2410.08821 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CREStE: Scalable Mapless Navigation with Internet Scale Priors and
  Counterfactual Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Zhang, Harshit Sikchi, Amy Zhang, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CREStE, a scalable learning-based mapless navigation framework
to address the open-world generalization and robustness challenges of outdoor
urban navigation. Key to achieving this is learning perceptual representations
that generalize to open-set factors (e.g. novel semantic classes, terrains,
dynamic entities) and inferring expert-aligned navigation costs from limited
demonstrations. CREStE addresses both these issues, introducing 1) a visual
foundation model (VFM) distillation objective for learning open-set structured
bird's-eye-view perceptual representations, and 2) counterfactual inverse
reinforcement learning (IRL), a novel active learning formulation that uses
counterfactual trajectory demonstrations to reason about the most important
cues when inferring navigation costs. We evaluate CREStE on the task of
kilometer-scale mapless navigation in a variety of city, offroad, and
residential environments and find that it outperforms all state-of-the-art
approaches with 70% fewer human interventions, including a 2-kilometer mission
in an unseen environment with just 1 intervention; showcasing its robustness
and effectiveness for long-horizon mapless navigation. Videos and additional
materials can be found on the project page: https://amrl.cs.utexas.edu/creste
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mock<span class="highlight-title">LLM</span>: A Multi-Agent Behavior Collaboration Framework for Online Job
  Seeking and Recruiting <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongda Sun, Hongzhan Lin, Haiyu Yan, Yang Song, Xin Gao, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online recruitment platforms have reshaped job-seeking and recruiting
processes, driving increased demand for applications that enhance person-job
matching. Traditional methods generally rely on analyzing textual data from
resumes and job descriptions, limiting the dynamic, interactive aspects crucial
to effective recruitment. Recent advances in Large Language Models (LLMs) have
revealed remarkable potential in simulating adaptive, role-based dialogues,
making them well-suited for recruitment scenarios. In this paper, we propose
\textbf{MockLLM}, a novel framework to generate and evaluate mock interview
interactions. The system consists of two key components: mock interview
generation and two-sided evaluation in handshake protocol. By simulating both
interviewer and candidate roles, MockLLM enables consistent and collaborative
interactions for real-time and two-sided matching. To further improve the
matching quality, MockLLM further incorporates reflection memory generation and
dynamic strategy modification, refining behaviors based on previous experience.
We evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment
platform. The experimental results indicate that MockLLM outperforms existing
methods in matching accuracy, scalability, and adaptability across job domains,
highlighting its potential to advance candidate assessment and online
recruitment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JointDiT: Enhancing RGB-Depth Joint Modeling with <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwon Byung-Ki, Qi Dai, Lee Hyoseok, Chong Luo, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present JointDiT, a diffusion transformer that models the joint
distribution of RGB and depth. By leveraging the architectural benefit and
outstanding image prior of the state-of-the-art diffusion transformer, JointDiT
not only generates high-fidelity images but also produces geometrically
plausible and accurate depth maps. This solid joint distribution modeling is
achieved through two simple yet effective techniques that we propose, i.e.,
adaptive scheduling weights, which depend on the noise levels of each modality,
and the unbalanced timestep sampling strategy. With these techniques, we train
our model across all noise levels for each modality, enabling JointDiT to
naturally handle various combinatorial generation tasks, including joint
generation, depth estimation, and depth-conditioned image generation by simply
controlling the timestep of each branch. JointDiT demonstrates outstanding
joint generation performance. Furthermore, it achieves comparable results in
depth estimation and depth-conditioned image generation, suggesting that joint
distribution modeling can serve as a replaceable alternative to conditional
generation. The project page is available at
https://byungki-k.github.io/JointDiT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/CVF International Conference on Computer Vision
  (ICCV) 2025. Project page: https://byungki-k.github.io/JointDiT/ Code:
  https://github.com/ByungKi-K/JointDiT-code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCDVQ: Enhancing Vector Quantization for <span class="highlight-title">Large Language Model</span>s via Polar
  Coordinate Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yue, Zukang Xu, Zhihang Yuan, Dawei Yang, Jianlong Wu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smart Ride and Delivery Services with Electric Vehicles: Leveraging
  Bidirectional Charging for Profit Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchun Du, Bojie Shen, Muhammad Aamir Cheema, Adel N. Toosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doppelganger Method: Breaking Role Consistency in <span class="highlight-title">LLM</span> Agent via
  Prompt-based Transferable Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of large language models, prompt engineering now enables the
rapid, low-effort creation of diverse autonomous agents that are already in
widespread use. Yet this convenience raises urgent concerns about the safety,
robustness, and behavioral consistency of the underlying prompts, along with
the pressing challenge of preventing those prompts from being exposed to user's
attempts. In this paper, we propose the ''Doppelganger method'' to demonstrate
the risk of an agent being hijacked, thereby exposing system instructions and
internal information. Next, we define the ''Prompt Alignment Collapse under
Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this
adversarial transfer attack. We also propose a ''Caution for Adversarial
Transfer (CAT)'' prompt to counter the Doppelganger method. The experimental
results demonstrate that the Doppelganger method can compromise the agent's
consistency and expose its internal information. In contrast, CAT prompts
enable effective defense against this adversarial attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Image Generation with Variadic Attention Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Walton, Ali Hassani, Xingqian Xu, Zhangyang Wang, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the integration of transformers in vision models have yielded
significant improvements on vision tasks they still require significant amounts
of computation for both training and inference. Restricted attention mechanisms
significantly reduce these computational burdens but come at the cost of losing
either global or local coherence. We propose a simple, yet powerful method to
reduce these trade-offs: allow the attention heads of a single transformer to
attend to multiple receptive fields.
  We demonstrate our method utilizing Neighborhood Attention (NA) and integrate
it into a StyleGAN based architecture for image generation. With this work,
dubbed StyleNAT, we are able to achieve a FID of 2.05 on FFHQ, a 6% improvement
over StyleGAN-XL, while utilizing 28% fewer parameters and with 4$\times$ the
throughput capacity. StyleNAT achieves the Pareto Frontier on FFHQ-256 and
demonstrates powerful and efficient image generation on other datasets. Our
code and model checkpoints are publicly available at:
https://github.com/SHI-Labs/StyleNAT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in eLVM @ CVPR
  (https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Walton_Efficient_Image_Generation_with_Variadic_Attention_Heads_CVPRW_2025_paper)
  | Formerly named StyleNAT: Giving Each Head a New Perspective |</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structuring the Unstructured: A Multi-Agent System for Extracting and
  Querying Financial KPIs and Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee, Jihoon Kwon, Minjae Kim, Juneha Hwang, Minsoo Ha, Chaewoon Kim, Jaeseon Ha, Suyeol Yun, Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting structured and quantitative insights from unstructured financial
filings is essential in investment research, yet remains time-consuming and
resource-intensive. Conventional approaches in practice rely heavily on
labor-intensive manual processes, limiting scalability and delaying the
research workflow. In this paper, we propose an efficient and scalable method
for accurately extracting quantitative insights from unstructured financial
documents, leveraging a multi-agent system composed of large language models.
Our proposed multi-agent system consists of two specialized agents: the
\emph{Extraction Agent} and the \emph{Text-to-SQL Agent}. The
\textit{Extraction Agent} automatically identifies key performance indicators
from unstructured financial text, standardizes their formats, and verifies
their accuracy. On the other hand, the \textit{Text-to-SQL Agent} generates
executable SQL statements from natural language queries, allowing users to
access structured data accurately without requiring familiarity with the
database schema. Through experiments, we demonstrate that our proposed system
effectively transforms unstructured text into structured data accurately and
enables precise retrieval of key information. First, we demonstrate that our
system achieves approximately 95\% accuracy in transforming financial filings
into structured data, matching the performance level typically attained by
human annotators. Second, in a human evaluation of the retrieval task -- where
natural language queries are used to search information from structured data --
91\% of the responses were rated as correct by human evaluators. In both
evaluations, our system generalizes well across financial document types,
consistently delivering reliable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, FinIR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Review</span> learning: Real world validation of privacy preserving continual
  learning a<span class="highlight-title">cross</span> medical institutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesung Yoo, Sunghyuk Choi, Ye Seul Yang, Suhyeon Kim, Jieun Choi, Dongkyeong Lim, Yaeji Lim, Hyung Joon Joo, Dae Jung Kim, Rae Woong Park, Hyeong-Jin Yoon, Kwangsoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a deep learning model is trained sequentially on different datasets, it
often forgets the knowledge learned from previous data, a problem known as
catastrophic forgetting. This damages the model's performance on diverse
datasets, which is critical in privacy-preserving deep learning (PPDL)
applications based on transfer learning (TL). To overcome this, we introduce
"review learning" (RevL), a low cost continual learning algorithm for diagnosis
prediction using electronic health records (EHR) within a PPDL framework. RevL
generates data samples from the model which are used to review knowledge from
previous datasets. Six simulated institutional experiments and one real-world
experiment involving three medical institutions were conducted to validate
RevL, using three binary classification EHR data. In the real-world experiment
with data from 106,508 patients, the mean global area under the receiver
operating curve was 0.710 for RevL and 0.655 for TL. These results demonstrate
RevL's ability to retain previously learned knowledge and its effectiveness in
real-world PPDL scenarios. Our work establishes a realistic pipeline for PPDL
research based on model transfers across institutions and highlights the
practicality of continual learning in real-world medical settings using private
EHR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretrained Reversible Generation as Unsupervised Visual Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01787v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01787v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongkun Xue, Jinouwen Zhang, Yazhe Niu, Dazhong Shen, Bingqi Ma, Yu Liu, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models based on score matching and flow matching have
significantly advanced generation tasks, but their potential in discriminative
tasks remains underexplored. Previous approaches, such as generative
classifiers, have not fully leveraged the capabilities of these models for
discriminative tasks due to their intricate designs. We propose Pretrained
Reversible Generation (PRG), which extracts unsupervised representations by
reversing the generative process of a pretrained continuous generation model.
PRG effectively reuses unsupervised generative models, leveraging their high
capacity to serve as robust and generalizable feature extractors for downstream
tasks. This framework enables the flexible selection of feature hierarchies
tailored to specific downstream tasks. Our method consistently outperforms
prior approaches across multiple benchmarks, achieving state-of-the-art
performance among generative model based methods, including 78% top-1 accuracy
on ImageNet at a resolution of 64*64. Extensive ablation studies, including
out-of-distribution evaluations, further validate the effectiveness of our
approach. Code is available at https://github.com/opendilab/PRG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SACL: Understanding and Combating Textual Bias in Code Retrieval with
  Semantic-Augmented Reranking and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Gupta, Gayathri Ganesh Lakshmy, Yiqing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant. Based on our discoveries, we propose SACL, a framework that
enriches textual information and reduces bias by augmenting code or structural
knowledge with semantic information. Extensive experiments show that SACL
substantially improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on
HumanEval / MBPP / SWE-Bench-Lite), which also leads to better code generation
performance (e.g., by 4.88% Pass@1 on HumanEval).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Will <span class="highlight-title">LLM</span>s be Professional at Fund Investment? DeepFund: A Live Arena
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changlun Li, Yao Shi, Yuyu Luo, Nan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities across
various domains, but their effectiveness in financial decision-making remains
inadequately evaluated. Current benchmarks primarily assess LLMs' understanding
on financial documents rather than the ability to manage assets or dig out
trading opportunities in dynamic market conditions. Despite the release of new
benchmarks for evaluating diversified tasks on the financial domain, we
identified four major problems in these benchmarks, which are data leakage,
navel-gazing, over-intervention, and maintenance-hard. To pave the research
gap, we introduce DeepFund, a comprehensive arena platform for evaluating
LLM-based trading strategies in a live environment. Our approach implements a
multi-agent framework where they serve as multiple key roles that realize the
real-world investment decision processes. Moreover, we provide a web interface
that visualizes LLMs' performance with fund investment metrics across different
market conditions, enabling detailed comparative analysis. Through DeepFund, we
aim to provide a more realistic and fair assessment on LLM's capabilities in
fund investment, offering diversified insights and revealing their potential
applications in real-world financial markets. Our code is publicly available at
https://github.com/HKUSTDial/DeepFund.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, perspective paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WiS Platform: Enhancing Evaluation of <span class="highlight-title">LLM</span>-Based Multi-Agent Systems
  Through Game-Based Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Hu, Jianhui Zheng, Yancheng He, Hangyu Guo, Junguang Jiang, Han Zhu, Kai Sun, Yuning Jiang, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in autonomous multi-agent systems (MAS) based on large
language models (LLMs) have enhanced the application scenarios and improved the
capability of LLMs to handle complex tasks. Despite demonstrating
effectiveness, existing studies still evidently struggle to evaluate, analysis,
and reproducibility of LLM-based MAS. In this paper, to facilitate the research
on LLM-based MAS, we introduce an open, scalable, and real-time updated
platform for accessing and analyzing the LLM-based MAS based on the games Who
is Spy?" (WiS). Our platform is featured with three main worths: (1) a unified
model evaluate interface that supports models available on Hugging Face; (2)
real-time updated leaderboard for model evaluation; (3) a comprehensive
evaluation covering game-winning rates, attacking, defense strategies, and
reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments
coverage of various open- and closed-source LLMs, we find that different agents
exhibit distinct and intriguing behaviors in the game. The experimental results
demonstrate the effectiveness and efficiency of our platform in evaluating
LLM-based MAS. Our platform and its documentation are publicly available at
https://whoisspy.ai/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision-Language-Action (VLA) models have leveraged
pre-trained Vision-Language Models (VLMs) to improve the generalization
capabilities. VLMs, typically pre-trained on vision-language understanding
tasks, provide rich semantic knowledge and reasoning abilities. However, prior
research has shown that VLMs often focus on high-level semantic content and
neglect low-level features, limiting their ability to capture detailed spatial
information and understand physical dynamics. These aspects, which are crucial
for embodied control tasks, remain underexplored in existing pre-training
paradigms. In this paper, we investigate the training paradigm for VLAs, and
introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both
multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives,
enhancing both high-level semantic comprehension and low-level spatial
understanding. Experimental results show that UP-VLA achieves a 33% improvement
on the Calvin ABC-D benchmark compared to the previous state-of-the-art method.
Additionally, UP-VLA demonstrates improved success rates in real-world
manipulation tasks, particularly those requiring precise spatial information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Guided Speculative Decoding for Efficient <span class="highlight-title">LLM</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19324v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19324v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Reward-Guided Speculative Decoding (RSD), a novel framework
aimed at improving the efficiency of inference in large language models (LLMs).
RSD synergistically combines a lightweight draft model with a more powerful
target model, incorporating a controlled bias to prioritize high-reward
outputs, in contrast to existing speculative decoding methods that enforce
strict unbiasedness. RSD employs a process reward model to evaluate
intermediate decoding steps and dynamically decide whether to invoke the target
model, optimizing the trade-off between computational cost and output quality.
We theoretically demonstrate that a threshold-based mixture strategy achieves
an optimal balance between resource utilization and performance. Extensive
evaluations on challenging reasoning benchmarks, including Olympiad-level
tasks, show that RSD delivers significant efficiency gains against decoding
with the target model only (up to 4.4x fewer FLOPs), while achieving
significant better accuracy than parallel decoding method on average (up to
+3.5). These results highlight RSD as a robust and cost-effective approach for
deploying LLMs in resource-intensive scenarios. The code is available at
https://github.com/BaohaoLiao/RSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene
  Generation with World-Guided Video Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Lu, Xuanchi Ren, Jiawei Yang, Tianchang Shen, Zhangjie Wu, Jun Gao, Yue Wang, Siheng Chen, Mike Chen, Sanja Fidler, Jiahui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present InfiniCube, a scalable method for generating unbounded dynamic 3D
driving scenes with high fidelity and controllability. Previous methods for
scene generation either suffer from limited scales or lack geometric and
appearance consistency along generated sequences. In contrast, we leverage the
recent advancements in scalable 3D representation and video models to achieve
large dynamic scene generation that allows flexible controls through HD maps,
vehicle bounding boxes, and text descriptions. First, we construct a
map-conditioned sparse-voxel-based 3D generative model to unleash its power for
unbounded voxel world generation. Then, we re-purpose a video model and ground
it on the voxel world through a set of carefully designed pixel-aligned
guidance buffers, synthesizing a consistent appearance. Finally, we propose a
fast feed-forward approach that employs both voxel and pixel branches to lift
the dynamic videos to dynamic 3D Gaussians with controllable objects. Our
method can generate controllable and realistic 3D driving scenes, and extensive
experiments validate the effectiveness and superiority of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025. Project Page:
  https://research.nvidia.com/labs/toronto-ai/infinicube/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super Co-alignment for Sustainable Symbiotic Society 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17404v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17404v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zeng, Feifei Zhao, Yuwei Wang, Enmeng Lu, Yaodong Yang, Lei Wang, Chao Liu, Yitao Liang, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Boyuan Chen, Jinyu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Artificial Intelligence (AI) advances toward Artificial General
Intelligence (AGI) and eventually Artificial Superintelligence (ASI), it may
potentially surpass human control, deviate from human values, and even lead to
irreversible catastrophic consequences in extreme cases. This looming risk
underscores the critical importance of the "superalignment" problem - ensuring
that AI systems which are much smarter than humans, remain aligned with human
(compatible) intentions and values. While current scalable oversight and
weak-to-strong generalization methods demonstrate certain applicability, they
exhibit fundamental flaws in addressing the superalignment paradigm - notably,
the unidirectional imposition of human values cannot accommodate
superintelligence's autonomy or ensure AGI/ASI's stable learning. We contend
that the values for sustainable symbiotic society should be co-shaped by humans
and living AI together, achieving "Super Co-alignment." Guided by this vision,
we propose a concrete framework that integrates external oversight and
intrinsic proactive alignment. External oversight superalignment should be
grounded in human-centered ultimate decision, supplemented by interpretable
automated evaluation and correction, to achieve continuous alignment with
humanity's evolving values. Intrinsic proactive superalignment is rooted in a
profound understanding of the Self, others, and society, integrating
self-awareness, self-reflection, and empathy to spontaneously infer human
intentions, distinguishing good from evil and proactively prioritizing human
well-being. The integration of externally-driven oversight with
intrinsically-driven proactive alignment will co-shape symbiotic values and
rules through iterative human-AGI/ASI co-alignment, paving the way for
achieving safe and beneficial AGI and ASI for good, for human, and for a
symbiotic ecology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Monte Carlo Tree <span class="highlight-title">Diffusion</span>: 100x Speedup via Parallel Sparse
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesik Yoon, Hyeonseo Cho, Yoshua Bengio, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extremely Simple Streaming Forest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08483v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08483v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyin Xu, Jayanta Dey, Sambit Panda, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision forests, including random forests and gradient boosting trees,
remain the leading machine learning methods for many real-world data problems,
especially on tabular data. However, most of the current implementations only
operate in batch mode, and therefore cannot incrementally update when more data
arrive. Several previous works developed streaming trees and ensembles to
overcome this limitation. Nonetheless, we found that those state-of-the-art
algorithms suffer from a number of drawbacks, including low accuracy on some
problems and high memory usage on others. We therefore developed an extremely
simple extension of decision trees: given new data, simply update existing
trees by continuing to grow them, and replace some old trees with new ones to
control the total number of trees. In a benchmark suite containing 72
classification problems (the OpenML-CC18 data suite), we illustrate that our
approach, $\textit{Extremely Simple Streaming Forest}$ (XForest), does not
suffer from either of the aforementioned limitations. On those datasets, we
also demonstrate that our approach often performs as well as, and sometimes
even better than, conventional batch decision forest algorithms. With a
$\textit{zero-added-node}$ approach, XForest-Zero, we also further extend
existing splits to new tasks, and this very efficient method only requires
inference time. Thus, XForests establish a simple standard for streaming trees
and forests that could readily be applied to many real-world problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The Fourth Conference on Lifelong Learning Agents -
  CoLLAs 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirCache: Activating Inter-modal Relevancy KV Cache Compression for
  Efficient Large Vision-Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Visual Language Models (LVLMs) have gained
significant attention due to their remarkable reasoning capabilities and
proficiency in generalization. However, processing a large number of visual
tokens and generating long-context outputs impose substantial computational
overhead, leading to excessive demands for key-value (KV) cache. To address
this critical bottleneck, we propose AirCache, a novel KV cache compression
method aimed at accelerating LVLMs inference. This work systematically
investigates the correlations between visual and textual tokens within the
attention mechanisms of LVLMs. Our empirical analysis reveals considerable
redundancy in cached visual tokens, wherein strategically eliminating these
tokens preserves model performance while significantly accelerating context
generation. Inspired by these findings, we introduce an elite observation
window for assessing the importance of visual components in the KV cache,
focusing on stable inter-modal relevancy modeling with enhanced
multi-perspective consistency. Additionally, we develop an adaptive layer-wise
budget allocation strategy that capitalizes on the strength and skewness of
token importance distribution, showcasing superior efficiency compared to
uniform allocation. Comprehensive evaluations across multiple LVLMs and
benchmarks demonstrate that our method achieves comparable performance to the
full cache while retaining only 10% of visual KV cache, thereby reducing
decoding latency by 29% to 66% across various batch size and prompt length of
inputs. Notably, as cache retention rates decrease, our method exhibits
increasing performance advantages over existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have withdrawn this manuscript due to a critical error in the
  methodology which affects the validity of the main results. We are currently
  working to address this issue and will resubmit once the correction is
  complete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for
  M<span class="highlight-title">LLM</span>s to Conquer the Unknown 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wang, Zhouqiang Jiang, Yasuaki Susumu, Shotaro Miwa, Tianwei Chen, Yuta Nakashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PP-DocBee: Improving Multimodal Document Understanding Through a Bag of
  Tricks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Ni, Kui Huang, Yao Lu, Wenyu Lv, Guanzhong Wang, Zeyu Chen, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of digitalization, various document images are
being applied more extensively in production and daily life, and there is an
increasingly urgent need for fast and accurate parsing of the content in
document images. Therefore, this report presents PP-DocBee, a novel multimodal
large language model designed for end-to-end document image understanding.
First, we develop a data synthesis strategy tailored to document scenarios in
which we build a diverse dataset to improve the model generalization. Then, we
apply a few training techniques, including dynamic proportional sampling, data
preprocessing, and OCR postprocessing strategies. Extensive evaluations
demonstrate the superior performance of PP-DocBee, achieving state-of-the-art
results on English document understanding benchmarks and even outperforming
existing open source and commercial models in Chinese document understanding.
The source code and pre-trained models are publicly available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToolScan: A Benchmark for Characterizing Errors in Tool-Use <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirley Kokane, Ming Zhu, Tulika Awalgaonkar, Jianguo Zhang, Thai Hoang, Akshara Prabhakar, Zuxin Liu, Tian Lan, Liangwei Yang, Juntao Tan, Rithesh Murthy, Weiran Yao, Zhiwei Liu, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, Caiming Xiong, Silivo Savarese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) is one of the most critical aspects
of building a performant compound AI system. Since the output from LLMs
propagate to downstream steps, identifying LLM errors is crucial to system
performance. A common task for LLMs in AI systems is tool use. While there are
several benchmark environments for evaluating LLMs on this task, they typically
only give a success rate without any explanation of the failure cases. To solve
this problem, we introduce TOOLSCAN, a new benchmark to identify error patterns
in LLM output on tool-use tasks. Our benchmark data set comprises of queries
from diverse environments that can be used to test for the presence of seven
newly characterized error patterns. Using TOOLSCAN, we show that even the most
prominent LLMs exhibit these error patterns in their outputs. Researchers can
use these insights from TOOLSCAN to guide their error mitigation strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The State of <span class="highlight-title">Large Language Model</span>s for African Languages: Progress and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kedir Yassin Hussen, Walelign Tewabe Sewunetie, Abinew Ali Ayele, Sukairaj Hafiz Imam, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MvKeTR: Chest CT Report Generation with Multi-View Perception and
  Knowledge Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18309v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18309v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwei Deng, Xianchun He, Jianfeng Bao, Yudan Zhou, Shuhui Cai, Congbo Cai, Zhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CT report generation (CTRG) aims to automatically generate diagnostic reports
for 3D volumes, relieving clinicians' workload and improving patient care.
Despite clinical value, existing works fail to effectively incorporate
diagnostic information from multiple anatomical views and lack related clinical
expertise essential for accurate and reliable diagnosis. To resolve these
limitations, we propose a novel Multi-view perception Knowledge-enhanced
TansfoRmer (MvKeTR) to mimic the diagnostic workflow of clinicians. Just as
radiologists first examine CT scans from multiple planes, a Multi-View
Perception Aggregator (MVPA) with view-aware attention is proposed to
synthesize diagnostic information from multiple anatomical views effectively.
Then, inspired by how radiologists further refer to relevant clinical records
to guide diagnostic decision-making, a Cross-Modal Knowledge Enhancer (CMKE) is
devised to retrieve the most similar reports based on the query volume to
incorporate domain knowledge into the diagnosis procedure. Furthermore, instead
of traditional MLPs, we employ Kolmogorov-Arnold Networks (KANs) as the
fundamental building blocks of both modules, which exhibit superior parameter
efficiency and reduced spectral bias to better capture high-frequency
components critical for CT interpretation while mitigating overfitting.
Extensive experiments on the public CTRG-Chest-548 K dataset demonstrate that
our method outpaces prior state-of-the-art (SOTA) models across almost all
metrics. The code is available at https://github.com/xiweideng/MvKeTR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Journal of Biomedical and Health
  Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language
  Models in Meteorology Anomalies Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Chen, Peilin Zhou, Yining Hua, Dading Chong, Meng Cao, Yaowei Li, Wei Chen, Bing Zhu, Junwei Liang, Zixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meteorological heatmaps play a vital role in deciphering extreme weather
phenomena, yet their inherent complexities marked by irregular contours,
unstructured patterns, and complex color variations present unique analytical
hurdles for state-of-the-art Vision-Language Models (VLMs). Current
state-of-the-art models like GPT-4o, Qwen-VL, and LLaVA 1.6 struggle with tasks
such as precise color identification and spatial localization, resulting in
inaccurate or incomplete interpretations. To address these challenges, we
introduce Sparse Position and Outline Tracking (SPOT), a novel algorithm
specifically designed to process irregularly shaped colored regions in visual
data. SPOT identifies and localizes these regions by extracting their spatial
coordinates, enabling structured representations of irregular shapes. Building
on SPOT, we construct ClimateIQA, a novel meteorological visual question
answering (VQA) dataset, comprising 26,280 high-resolution heatmaps and 762,120
instruction samples for wind gust, total precipitation, wind chill index and
heat index analysis. ClimateIQA enhances VLM training by incorporating spatial
cues, geographic metadata, and reanalysis data, improving model accuracy in
interpreting and describing extreme weather features. Furthermore, we develop
Climate-Zoo, a suite of fine-tuned VLMs based on SPOT-empowered ClimateIQA,
which significantly outperforms existing models in meteorological heatmap
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-Body Conditioned Egocentric Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://dannytran123.github.io/PEVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and
  Model Selection at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaona Zhou, Constantin Brif, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where to find Grokking in <span class="highlight-title">LLM</span> Pretraining? Monitor
  Memorization-to-Generalization without Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Li, Chenrui Fan, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HalluSegBench: Counterfactual Visual Reasoning for Segmentation
  Hallucination Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhuo Li, Adheesh Juvekar, Xingyou Liu, Muntasir Wahed, Kiet A. Nguyen, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://plan-lab.github.io/hallusegbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximal Matching Matters: Preventing Representation Collapse for Robust
  <span class="highlight-title">Cross</span>-Modal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hani Alomari, Anushka Sivakumar, Andrew Zhang, Chris Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 63rd Annual Meeting of the Association for
  Computational Linguistics (ACL 2025 Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Design Space of 3D M<span class="highlight-title">LLM</span>s for CT Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Baharoon, Jun Ma, Congyu Fang, Augustin Toma, Bo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Invariant Markov Chain Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michalis K. Titsias, Angelos Alexopoulos, Siran Liu, Petros Dellaportas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skLEP: A Slovak General Language Understanding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process mining-driven modeling and simulation to enhance fault diagnosis
  in cyber-physical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Vitale, Nicola Dall'Ora, Sebastiano Gaiardelli, Enrico Fraccaroli, Nicola Mazzocca, Franco Fummi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Devising a solution to the problems of Cancer awareness in Telangana 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Avhad, Vedanti Kshirsagar, Urvi Ranjan, Mahek Nakhua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reliable Detection of Empty Space: Conditional Marked Point
  Processes for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias J. Riedlinger, Kira Maag, Hanno Gottschalk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have set the state-of-the-art in computer vision tasks
such as bounding box detection and semantic segmentation. Object detectors and
segmentation models assign confidence scores to predictions, reflecting the
model's uncertainty in object detection or pixel-wise classification. However,
these confidence estimates are often miscalibrated, as their architectures and
loss functions are tailored to task performance rather than probabilistic
foundation. Even with well calibrated predictions, object detectors fail to
quantify uncertainty outside detected bounding boxes, i.e., the model does not
make a probability assessment of whether an area without detected objects is
truly free of obstacles. This poses a safety risk in applications such as
automated driving, where uncertainty in empty areas remains unexplored. In this
work, we propose an object detection model grounded in spatial statistics.
Bounding box data matches realizations of a marked point process, commonly used
to describe the probabilistic occurrence of spatial point events identified as
bounding box centers, where marks are used to describe the spatial extension of
bounding boxes and classes. Our statistical framework enables a
likelihood-based training and provides well-defined confidence estimates for
whether a region is drivable, i.e., free of objects. We demonstrate the
effectiveness of our method through calibration assessments and evaluation of
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Traffic Signals for Daily Traffic Pattern 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shokrolah Shirazi, Hung-Fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The turning movement count data is crucial for traffic signal design,
intersection geometry planning, traffic flow, and congestion analysis. This
work proposes three methods called dynamic, static, and hybrid configuration
for TMC-based traffic signals. A vision-based tracking system is developed to
estimate the TMC of six intersections in Las Vegas using traffic cameras. The
intersection design, route (e.g. vehicle movement directions), and signal
configuration files with compatible formats are synthesized and imported into
Simulation of Urban MObility for signal evaluation with realistic data. The
initial experimental results based on estimated waiting times indicate that the
cycle time of 90 and 120 seconds works best for all intersections. In addition,
four intersections show better performance for dynamic signal timing
configuration, and the other two with lower performance have a lower ratio of
total vehicle count to total lanes of the intersection leg. Since daily traffic
flow often exhibits a bimodal pattern, we propose a hybrid signal method that
switches between dynamic and static methods, adapting to peak and off-peak
traffic conditions for improved flow management. So, a built-in traffic
generator module creates vehicle routes for 4 hours, including peak hours, and
a signal design module produces signal schedule cycles according to static,
dynamic, and hybrid methods. Vehicle count distributions are weighted
differently for each zone (i.e., West, North, East, South) to generate diverse
traffic patterns. The extended experimental results for 6 intersections with 4
hours of simulation time imply that zone-based traffic pattern distributions
affect signal design selection. Although the static method works great for
evenly zone-based traffic distribution, the hybrid method works well for highly
weighted traffic at intersection pairs of the West-East and North-South zones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach
  for Efficiency and Low Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gavin Lee Goodship, Luis Miralles-Pechuan, Stephen O'Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Spoken Dialogue Models from User Interactions <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne Wu, Laurent Mazaré, Neil Zeghidour, Alexandre Défossez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Keyword-Based Technique to Evaluate Broad Question Answer Script 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamim Al Mahmud, Md Gulzar Hussain, Sumaiya Kabir, Hasnain Ahmad, Mahmudus Sobhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Conference Proceedings (9 Pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wild refitting for black box prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin J. Wainwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an Optimal Control Perspective of ResNet Training <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens Püttschneider, Simon Heilig, Asja Fischer, Timm Faulwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the High-dimensional Learning Dynamics
  (HiLD) workshop at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Dataset for Underground Miner Detection in Diverse
  Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cyrus Addy, Ajay Kumar Gurumadaiah, Yixiang Gao, Kwame Awuah-Offei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underground mining operations face significant safety challenges that make
emergency response capabilities crucial. While robots have shown promise in
assisting with search and rescue operations, their effectiveness depends on
reliable miner detection capabilities. Deep learning algorithms offer potential
solutions for automated miner detection, but require comprehensive training
datasets, which are currently lacking for underground mining environments. This
paper presents a novel thermal imaging dataset specifically designed to enable
the development and validation of miner detection systems for potential
emergency applications. We systematically captured thermal imagery of various
mining activities and scenarios to create a robust foundation for detection
algorithms. To establish baseline performance metrics, we evaluated several
state-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,
and RT-DETR on our dataset. While not exhaustive of all possible emergency
situations, this dataset serves as a crucial first step toward developing
reliable thermal-based miner detection systems that could eventually be
deployed in real emergency scenarios. This work demonstrates the feasibility of
using thermal imaging for miner detection and establishes a foundation for
future research in this critical safety application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learnable Adaptive Time-Frequency Representation via Differentiable
  Short-Time Fourier Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Leiber, Yosra Marnissi, Axel Barrau, Sylvain Meignen, Laurent Massoulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DSTFT, STFT, spectrogram, time-frequency, IEEE Transactions on Signal
  Processing, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deception Detection in Dyadic Exchanges Using Multimodal Machine
  Learning: A Study on a Swedish Cohort 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franco Rugolon, Thomas Jack Samuels, Stephan Hau, Lennart Högman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 2 figures, 2 tables. To be submitted in Behavior Research
  Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow-Based Single-Step Completion for Efficient and Expressive Policy
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Koirala, Cody Fleming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed <span class="highlight-title">Cross</span>-Channel Hierarchical Aggregation for Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aristeidis Tsaris, Isaac Lyngaas, John Lagregren, Mohamed Wahib, Larry York, Prasanna Balaprakash, Dan Lu, Feiyi Wang, Xiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Bayesian Low-Rank Adaptation of <span class="highlight-title">Large Language Model</span>s via
  Stochastic Variational Subspace Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at UAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Stopping Tabular In-Context Learning <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaris Küken, Lennart Purucker, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML Workshop Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal-Aware Graph Attention Network for Cryptocurrency Transaction
  Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zheng, Bochuan Zhou, Yuping Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pay Attention to Small Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Tom Jacobs, Advait Gadhikar, Rebekka Burkholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN
  Hardware Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Leon, Georgios Makris, Sotirios Xydis, Kiamal Pekmestzi, Dimitrios Soudris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 13th IEEE LASCAS Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ rQdia: Regularizing Q-Value Distributions With Image Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Lerman, Jing Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Rieff, Maya Varma, Ossian Rabow, Subathra Adithan, Julie Kim, Ken Chang, Hannah Lee, Nidhi Rohatgi, Christian Bluethgen, Mohamed S. Muneer, Jean-Benoit Delbrouck, Michael Moor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex
  Insertions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Vu Anh, Mehmet Dik, Nguyen Viet Anh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynamicBench: Evaluating Real-Time Report Generation in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Li, Hao Sun, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Hong Xu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galvin Brice S. Lim, Brian Godwin S. Lim, Argel A. Bandala, John Anthony C. Jose, Timothy Scott C. Chu, Edwin Sybingco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Prototype Routing: Achieving Near-Perfect Load Balancing in
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiechen Chen, Bipin Rajendran, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic and quantum computing have recently emerged as promising
paradigms for advancing artificial intelligence, each offering complementary
strengths. Neuromorphic systems built on spiking neurons excel at processing
time-series data efficiently through sparse, event-driven computation,
consuming energy only upon input events. Quantum computing, on the other hand,
leverages superposition and entanglement to explore feature spaces that are
exponentially large in the number of qubits. Hybrid approaches combining these
paradigms have begun to show potential, but existing quantum spiking models
have important limitations. Notably, prior quantum spiking neuron
implementations rely on classical memory mechanisms on single qubits, requiring
repeated measurements to estimate firing probabilities, and they use
conventional backpropagation on classical simulators for training. Here we
propose a stochastic quantum spiking (SQS) neuron model that addresses these
challenges. The SQS neuron uses multi-qubit quantum circuits to realize a
spiking unit with internal quantum memory, enabling event-driven probabilistic
spike generation in a single shot. Furthermore, we outline how networks of SQS
neurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a
hardware-friendly local learning rule, eliminating the need for global
classical backpropagation. The proposed SQSNN model fuses the time-series
efficiency of neuromorphic computing with the exponentially large inner state
space of quantum computing, paving the way for quantum spiking neural networks
that are modular, scalable, and trainable on quantum hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Uniform Weighted Deep Polynomial approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kingsley Yeon, Steven B. Damelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Adapter Design Tradeoffs for Low Resource Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved seeding strategies for k-means and k-GMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Carrière, Frédéric Cazals
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Encoders Can Rival Large Decoders in Detecting Groundedness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy
  Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Sablica, Kurt Hornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiLoCoX: A Low-Communication Large-Scale Training Framework for
  Decentralized Cluster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From On-chain to Macro: Assessing the Importance of Data Source
  Diversity in Cryptocurrency Market Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgos Demosthenous, Chryssis Georgiou, Eliada Polydorou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Learning for Obsolescence Risk Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elie Saad, Aya Mrabah, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity-aware fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Goncharov, Daniil Vyazhev, Petr Sychev, Edvard Khalafyan, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Causal Reasoning in <span class="highlight-title">Large Language Model</span>s: Reality or Mirage? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Delegates Resolve Fairness Issues in Perpetual Voting with
  Partial Turnout 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apurva Shah, Axel Abels, Ann Nowé, Tom Lenaerts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted at the ACM Collective Intelligence
  Conference (CI 2025), August 4 to 6, 2025, San Diego, CA, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance improvement of spatial semantic segmentation with enriched
  audio features and agent-based error correction for DCASE 2025 Challenge Task
  4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongyeon Park, Joonhee Lee, Do-Hyeon Lim, Hong Kook Kim, Hyeongcheol Geum, Jeong Eun Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DCASE 2025 challenge Task4, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Mini-Batch Selection in Reinforcement Learning for Efficient
  Chemical Exploration in de novo Drug Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hampus Gummesson Svensson, Ola Engkvist, Jon Paul Janet, Christian Tyrchan, Morteza Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-Based Spatial-Temporal Counterfactual Outcomes Estimation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Li, Haoang Chi, Mingyu Liu, Wanrong Huang, Liyang Xu, Wenjing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linearity-based neural network compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silas Dobler, Florian Lemmerich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Federated Learning via Dual-Prompt Optimization and <span class="highlight-title">Cross</span>
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuguang Zhang, Kuangpu Guo, Zhihe Lu, Yunbo Wang, Jian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Adversarial Evasion and Out-of-Distribution Detection for UAV
  Cyber-Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Kumar Panda, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBConformer: Dual-Branch Convolutional <span class="highlight-title">Transformer</span> for EEG Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Wang, Hongbin Wang, Tianwang Jia, Xingyi He, Siyang Li, Dongrui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaLaFormer: Norm-Aware Linear Attention for <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Meng, Yadan Luo, Liangyu Huo, Yaowei Wang, Xin Li, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV
  Deconfliction under Observation-Space Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Kumar Panda, Adolfo Perrusquia, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Policy Switching for Antifragile Reinforcement Learning for UAV
  Deconfliction in Adversarial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Kumar Panda, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luosheng Xu, Dalin Zhang, Zhaohui Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing change detection is essential for monitoring urban expansion,
disaster assessment, and resource management, offering timely, accurate, and
large-scale insights into dynamic landscape transformations. While deep
learning has revolutionized change detection, the increasing complexity and
computational demands of modern models have not necessarily translated into
significant accuracy gains. Instead of following this trend, this study
explores a more efficient approach, focusing on lightweight models that
maintain high accuracy while minimizing resource consumption, which is an
essential requirement for on-satellite processing. To this end, we propose
FlickCD, which means quick flick then get great results, pushing the boundaries
of the performance-resource trade-off. FlickCD introduces an Enhanced
Difference Module (EDM) to amplify critical feature differences between
temporal phases while suppressing irrelevant variations such as lighting and
weather changes, thereby reducing computational costs in the subsequent change
decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion
Blocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global
Self-Attention (EGSA) to efficiently capture semantic information at multiple
scales, preserving both coarse- and fine-grained changes. Extensive experiments
on four benchmark datasets demonstrate that FlickCD reduces computational and
storage overheads by more than an order of magnitude while achieving
state-of-the-art (SOTA) performance or incurring only a minor (<1\% F1)
accuracy trade-off. The implementation code is publicly available at
https://github.com/xulsh8/FlickCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual
  Conditional <span class="highlight-title">Diffusion</span> Implicit Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changxi Chi, Jun Xia, Yufei Huang, Jingbo Zhou, Siyuan Li, Yunfan Liu, Chang Yu, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Skip the Middle Layers of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lawson, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Hierarchical Concept Reasoning through Attention-Guided
  Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Debot, Pietro Barbiero, Gabriele Dominici, Giuseppe Marra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xenia Heilmann, Luca Corbucci, Mattia Cerrato, Anna Monreale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Thought Enhanced Shallow <span class="highlight-title">Transformer</span>s for Wireless Symbol
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Fan, Peng Wang, Jing Yang, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and
  Solutions <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangzhe Peng, Kaiyuan Gao, Liang He, Yuheng Cong, Haiguang Liu, Kun He, Lijun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for
  Efficient Egocentric Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjoy Chowdhury, Subrata Biswas, Sayan Nag, Tushar Nagarajan, Calvin Murdock, Ishwarya Ananthabhotla, Yijun Qian, Vamsi Krishna Ithapu, Dinesh Manocha, Ruohan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogenization of Multi-agent Learning Dynamics in Finite-state Markov
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Kerzreho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">LLM</span> Tool Use with High-quality Instruction Data from Knowledge
  Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, Bo Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Peng, Ming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Diffusion</span>-Based Image Editing Faithfulness via Guidance and
  Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansam Cho, Seoung Bum Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Skill Discovery via Regret-Aware Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Ming Zhou, Shaopeng Zhai, Ying Sun, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 technical page followed by references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suorong Yang, Peijia Li, Furao Shen, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information-Theoretic Analysis for Federated Learning under Concept
  Drift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Peng, Meng Zhang, Ming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Little By Little: Continual Learning via Self-Activated Sparse
  Mixture-of-Rank Adaptive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Lu, Chongyang Zhao, Jason Xue, Lina Yao, Kristen Moore, Dong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic
  Annotations and Local Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Jiang, Mangal Prakash, Hehuan Ma, Jianyuan Deng, Yuzhi Guo, Amina Mollaysa, Tommaso Mansi, Rui Liao, Junzhou Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for
  Skin Disease Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyue Jiao, Kangyu Zheng, Yiyu Shi, Zhiding Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning-assisted diagnosis is gaining traction in skin disease
detection, but training effective models requires large amounts of high-quality
data. Skin disease datasets often suffer from class imbalance, privacy
concerns, and object bias, making data augmentation essential. While classical
generative models are widely used, they demand extensive computational
resources and lengthy training time. Quantum computing offers a promising
alternative, but existing quantum-based image generation methods can only yield
grayscale low-quality images. Through a novel classical-quantum latent space
fusion technique, our work overcomes this limitation and introduces the first
classical-quantum generative adversarial network (GAN) capable of generating
color medical images. Our model outperforms classical deep convolutional GANs
and existing hybrid classical-quantum GANs in both image generation quality and
classification performance boost when used as data augmentation. Moreover, the
performance boost is comparable with that achieved using state-of-the-art
classical generative models, yet with over 25 times fewer parameters and 10
times fewer training epochs. Such results suggest a promising future for
quantum image generation as quantum hardware advances. Finally, we demonstrate
the robust performance of our model on real IBM quantum machine with hardware
noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Walton, Valeriy Klyukin, Maksim Artemev, Denis Derkach, Nikita Orlov, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in eLVM @ CVPR
  (https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Walton_Distilling_Normalizing_Flows_CVPRW_2025_paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akio Hayakawa, Masato Ishii, Takashi Shibuya, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via
  Forward-Only Passes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Zhen Zhang, Rupak Vignesh Swaminathan, Jing Liu, Nathan Susanj, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Gradient Descent Simulate Prompting? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zhang, Leshem Choshen, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EraRAG: Efficient and Incremental Retrieval Augmented Generation for
  Growing Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antibody Design and Optimization with Multi-scale Equivariant Graph
  <span class="highlight-title">Diffusion</span> Models for Accurate Complex Antigen Binding <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiameng Chen, Xiantao Cai, Jia Wu, Wenbin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, accepted at IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model State Arithmetic for Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivan Rezaei, Mehrdad Saberi, Abhilasha Ravichander, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Geopolitical Events with a Sparse Temporal Fusion
  <span class="highlight-title">Transformer</span> and Gaussian Process Hybrid: A Case Study in Middle Eastern and
  U.S. Conflict Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsin-Hsiung Huang, Hayden Hampton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bounds on the Size of Markov Equivalence Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Jahn, Frederick Eberhardt, Leonard J. Schulman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Reinforcement Learning Trading Agent for Sector Rotation in the
  Taiwan Stock Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Sheng Chen, Xinyu Zhang, Ya-Chuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning for Manifold Gaussian Process Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxing Cheng, Lulu Kang, Yiwei Wang, Chun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Representation Learning for Additive Rule Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahrzad Behzadimanesh, Pierre Le Bodic, Geoffrey I. Webb, Mario Boley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-guided Chemical Process Optimization with a Multi-Agent Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (main manuscript without references), 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI for Radar Resource Management: Modified LIME in Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Namazi, Alexander Nemecek, Erman Ayday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Fixed-Point Methods for Multichain MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Zurek, Yudong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Single-Policy Sample Complexity and Transient Coverage for
  Average-Reward Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Zurek, Guy Zamir, Yudong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Structured Feedback Multimodel Ensemble Online Conformal
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Hajihashemi, Yanning Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Sketch: Enabling Global Visual Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryo Lotfi, Enrico Fini, Samy Bengio, Moin Nabi, Emmanuel Abbe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern vision models have achieved remarkable success in benchmarks where
local features provide critical information about the target. There is now a
growing interest in tackling tasks requiring more global reasoning, where local
features do not provide significant information. Minsky and Papert put forward
such tasks in 1969 with their connectivity study, exposing the limitations of
the perceptron model. In this paper, we introduce an expanded set of global
visual datasets involving graphs, strings, mazes, and image grids. We show that
large vision models still struggle to learn these tasks efficiently. Similarly,
state-of-the-art multi-modal LLMs perform poorly on these datasets. We explain
this learning inefficiency by means of the 'globality degree' measure. To
mitigate this, we propose a method called chain-of-sketch (CoS). Similar to the
chain-of-thought and scratchpad techniques used in language models, CoS breaks
the original task into intermediate visual steps to help learn a complex task.
In addition, we show that not all CoS strategies perform equally well. Our key
insight is to impose a Markovian structure on the CoS frames. This leads to the
introduction of 'inductive CoS' which achieves better out-of-distribution
generalization and performs well even with smaller models compared to
non-inductive variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>additional experiments added, title changed from "Visual Scratchpads:
  Enabling Global Reasoning in Vision" to "Chain-of-Sketch: Enabling Global
  Visual Reasoning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mesh-Informed Neural Operator : A <span class="highlight-title">Transformer</span> Generative Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozhong Shi, Zachary E. Ross, Domniki Asimaki, Kamyar Azizzadenesheli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Escaping Saddle Points under Generalized Smoothness via
  Self-Bounding Regularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Yiming Cao, August Y. Chen, Karthik Sridharan, Benjamin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the optimization of non-convex functions that are not necessarily
smooth (gradient and/or Hessian are Lipschitz) using first order methods.
Smoothness is a restrictive assumption in machine learning in both theory and
practice, motivating significant recent work on finding first order stationary
points of functions satisfying generalizations of smoothness with first order
methods. We develop a novel framework that lets us systematically study the
convergence of a large class of first-order optimization algorithms (which we
call decrease procedures) under generalizations of smoothness. We instantiate
our framework to analyze the convergence of first order optimization algorithms
to first and \textit{second} order stationary points under generalizations of
smoothness. As a consequence, we establish the first convergence guarantees for
first order methods to second order stationary points under generalizations of
smoothness. We demonstrate that several canonical examples fall under our
framework, and highlight practical implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NY Real Estate Racial Equity Analysis via Applied Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16946v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16946v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjana Chalavadi, Andrei Pastor, Terry Leitch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study analyzes tract-level real estate ownership patterns in New York
State (NYS) and New York City (NYC) to uncover racial disparities. We use an
advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering,
validated at 89.2% accuracy) to compare the predicted racial composition of
property owners to the resident population from census data. We examine both a
Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how
incorporating geospatial context affects our predictions and disparity
estimates. The results reveal significant inequities: White individuals hold a
disproportionate share of properties and property value relative to their
population, while Black, Hispanic, and Asian communities are underrepresented
as property owners. These disparities are most pronounced in minority-majority
neighborhoods, where ownership is predominantly White despite a predominantly
non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates
these gaps by reducing owner-occupied opportunities in urban minority
communities. We provide a breakdown of ownership vs. population by race for
majority-White, -Black, -Hispanic, and -Asian tracts, identify those with
extreme ownership disparities, and compare patterns in urban, suburban, and
rural contexts. The findings underscore persistent racial inequity in property
ownership, reflecting broader historical and socio-economic forces, and
highlight the importance of data-driven approaches to address these issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updated/replaced stale reference links. Added narrative covering
  gentrification, racial capitalism, financialization of housing, and
  segregation. Moved model details to appendices. Added Nivea</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Preference Lambda-weighted Listwise <span class="highlight-title">DPO</span> for Dynamic Preference
  Alignment <span class="chip">AAAI 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Sun, Xiyao Wang, Zixi Li, Jinman Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale unsupervised language models (LMs) capture broad world
knowledge and reasoning capabilities, steering their behavior toward desired
objectives remains challenging due to the lack of explicit supervision.
Existing alignment techniques, such as reinforcement learning from human
feedback (RLHF), rely on training a reward model and performing reinforcement
learning to align with human preferences. However, RLHF is often
computationally intensive, unstable, and sensitive to hyperparameters.
  To address these limitations, Direct Preference Optimization (DPO) was
introduced as a lightweight and stable alternative, enabling direct alignment
of language models with pairwise preference data via classification loss.
However, DPO and its extensions generally assume a single static preference
distribution, limiting flexibility in multi-objective or dynamic alignment
settings.
  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted
Listwise DPO, which extends DPO to incorporate multiple human preference
dimensions (e.g., helpfulness, harmlessness, informativeness) and enables
dynamic interpolation through a controllable simplex-weighted formulation. Our
method supports both listwise preference feedback and flexible alignment across
varying user intents without re-training. Empirical and theoretical analysis
demonstrates that our method is as effective as traditional DPO on static
objectives while offering greater generality and adaptability for real-world
deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, appendix included. To appear in Proceedings of
  AAAI 2026. Code:
  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Model to Forecast Them All and in Entity Distributions Bind Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Bölat, Simon Tindemans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic forecasting in power systems often involves multi-entity
datasets like households, feeders, and wind turbines, where generating reliable
entity-specific forecasts presents significant challenges. Traditional
approaches require training individual models for each entity, making them
inefficient and hard to scale. This study addresses this problem using
GUIDE-VAE, a conditional variational autoencoder that allows entity-specific
probabilistic forecasting using a single model. GUIDE-VAE provides flexible
outputs, ranging from interpretable point estimates to full probability
distributions, thanks to its advanced covariance composition structure. These
distributions capture uncertainty and temporal dependencies, offering richer
insights than traditional methods. To evaluate our GUIDE-VAE-based forecaster,
we use household electricity consumption data as a case study due to its
multi-entity and highly stochastic nature. Experimental results demonstrate
that GUIDE-VAE outperforms conventional quantile regression techniques across
key metrics while ensuring scalability and versatility. These features make
GUIDE-VAE a powerful and generalizable tool for probabilistic forecasting
tasks, with potential applications beyond household electricity consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting with Phonemes: Enhancing <span class="highlight-title">LLM</span>s' Multilinguality for Non-Latin
  Script Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang H Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multilingual LLMs have achieved remarkable performance across
benchmarks, we find they continue to underperform on non-Latin script languages
across contemporary LLM families. This discrepancy arises from the fact that
LLMs are pretrained with orthographic scripts, which are dominated by Latin
characters that obscure their shared phonology with non-Latin scripts. We
propose leveraging phonemic transcriptions as complementary signals to induce
script-invariant representations. Our study demonstrates that integrating
phonemic signals improves performance across both non-Latin and Latin script
languages, with a particularly significant impact on closing the performance
gap between the two. Through detailed experiments, we show that phonemic and
orthographic scripts retrieve distinct examples for in-context learning (ICL).
This motivates our proposed Mixed-ICL retrieval strategy, where further
aggregation from both leads to our significant performance improvements for
both Latin script languages (up to 12.6%) and non-Latin script languages (up to
15.1%) compared to randomized ICL retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 (Main Conference). This version contains minor
  improvements to the camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Web Search towards Agentic Deep Research: Incentivizing Search with
  Reasoning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Strategies Emerge Rationally 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wurgaft, Ekdeep Singh Lubana, Core Francisco Park, Hidenori Tanaka, Gautam Reddy, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, where the prior matches
the underlying task distribution. Adopting the normative lens of rational
analysis, where a learner's behavior is explained as an optimal adaptation to
data given computational constraints, we develop a hierarchical Bayesian
framework that almost perfectly predicts Transformer next-token predictions
throughout training -- without assuming access to its weights. Under this
framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transitioning from generalization to memorization as task
diversity increases. Overall, our work advances an explanatory and predictive
account of ICL grounded in tradeoffs between strategy loss and complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capacity-Constrained Online Learning with Delays: Scheduling Frameworks
  and Regret Trade-offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Ryabchenko, Idan Attias, Daniel M. Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study online learning with oblivious losses and delays under a novel
``capacity constraint'' that limits how many past rounds can be tracked
simultaneously for delayed feedback. Under ``clairvoyance'' (i.e., delay
durations are revealed upfront each round) and/or ``preemptibility'' (i.e., we
can stop tracking previously chosen round feedback), we establish matching
upper and lower bounds (up to logarithmic terms) on achievable regret,
characterizing the ``optimal capacity'' needed to match the minimax rates of
classical delayed online learning, which implicitly assume unlimited capacity.
Our algorithms achieve minimax-optimal regret across all capacity levels, with
performance gracefully degrading under suboptimal capacity. For $K$ actions and
total delay $D$ over $T$ rounds, under clairvoyance and assuming capacity $C =
\Omega(\log(T))$, we achieve regret $\widetilde{\Theta}(\sqrt{TK + DK/C +
D\log(K)})$ for bandits and $\widetilde{\Theta}(\sqrt{(D+T)\log(K)})$ for
full-information feedback. When replacing clairvoyance with preemptibility, we
require a known maximum delay bound $d_{\max}$, adding
${\widetilde{O}(d_{\max})}$ to the regret. For fixed delays $d$ (i.e., $D=Td$),
the minimax regret is $\Theta(\sqrt{TK(1+d/C)+Td\log(K)})$ and the optimal
capacity is $\Theta(\min\{K/\log(K),d\})$ in the bandit setting, while in the
full-information feedback setting, the minimax regret is
$\Theta(\sqrt{T(d+1)\log(K)})$ and the optimal capacity is $\Theta(1)$. For
round-dependent and fixed delays, our upper bounds are achieved using novel
preemptive and non-preemptive scheduling policies, based on Pareto-distributed
proxy delays, and batching techniques, respectively. Crucially, our work
unifies delayed bandits, label-efficient learning, and online scheduling
frameworks, demonstrating that robust online learning under delayed feedback is
possible with surprisingly modest tracking capacity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fake it till You Make it: Reward Modeling as Discriminative Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO). Code and data will be released at
https://github.com/Visualignment/GAN-RM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measurement to Meaning: A Validity-Centered Framework for AI Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10573v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10573v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olawale Salaudeen, Anka Reuel, Ahmed Ahmed, Suhana Bedi, Zachary Robertson, Sudharsan Sundar, Ben Domingue, Angelina Wang, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the capabilities and utility of AI systems have advanced, rigorous
norms for evaluating these systems have lagged. Grand claims, such as models
achieving general reasoning capabilities, are supported with model performance
on narrow benchmarks, like performance on graduate-level exam questions, which
provide a limited and potentially misleading assessment. We provide a
structured approach for reasoning about the types of evaluative claims that can
be made given the available evidence. For instance, our framework helps
determine whether performance on a mathematical benchmark is an indication of
the ability to solve problems on math tests or instead indicates a broader
ability to reason. Our framework is well-suited for the contemporary paradigm
in machine learning, where various stakeholders provide measurements and
evaluations that downstream users use to validate their claims and decisions.
At the same time, our framework also informs the construction of evaluations
designed to speak to the validity of the relevant claims. By leveraging
psychometrics' breakdown of validity, evaluations can prioritize the most
critical facets for a given claim, improving empirical utility and
decision-making efficacy. We illustrate our framework through detailed case
studies of vision and language model evaluations, highlighting how explicitly
considering validity strengthens the connection between evaluation evidence and
the claims being made.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correspondence to olawale@mit.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARALLELPROMPT: Extracting Parallelism from <span class="highlight-title">Large Language Model</span> Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM serving systems typically treat user prompts as monolithic inputs,
optimizing inference through decoding tricks or inter-query batching. However,
many real-world prompts contain latent semantic parallelism--decomposable
structures where subtasks can be executed independently to reduce latency while
preserving meaning. We introduce PARALLELPROMPT, the first benchmark for
measuring intra-query parallelism in natural user prompts. Our dataset
comprises over 37,000 real-world prompts from public LLM chat logs, each
annotated with a structured schema capturing task templates, shared context,
and iteration inputs. These schemas are extracted using LLM-assisted prompting
with rule-based multilingual validation. To evaluate the benefits of
decomposition, we provide an execution suite that benchmarks serial vs.
parallel strategies, measuring latency, structural adherence, and semantic
fidelity. Our results show that intra-query parallelism can be successfully
parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks
like translation, comprehension, and comparative analysis, with minimal quality
degradation. By releasing this benchmark, curation pipeline, and evaluation
suite, we provide the first standardized testbed for studying structure-aware
execution in LLM serving pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Adaptive Foundation Models: Evolving AI for Personalized and
  Efficient Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New Bounds for Sparse Variational Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michalis K. Titsias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse variational Gaussian processes (GPs) construct tractable posterior
approximations to GP models. At the core of these methods is the assumption
that the true posterior distribution over training function values ${\bf f}$
and inducing variables ${\bf u}$ is approximated by a variational distribution
that incorporates the conditional GP prior $p({\bf f} | {\bf u})$ in its
factorization. While this assumption is considered as fundamental, we show that
for model training we can relax it through the use of a more general
variational distribution $q({\bf f} | {\bf u})$ that depends on $N$ extra
parameters, where $N$ is the number of training examples. In GP regression, we
can analytically optimize the evidence lower bound over the extra parameters
and express a tractable collapsed bound that is tighter than the previous
bound. The new bound is also amenable to stochastic optimization and its
implementation requires minor modifications to existing sparse GP code.
Further, we also describe extensions to non-Gaussian likelihoods. On several
datasets we demonstrate that our method can reduce bias when learning the
hyperparameters and can lead to better predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability of <span class="highlight-title">Large Language Model</span>s using SMILE: Statistical
  Model-agnostic Interpretability with Local Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The submission contains incorrect references that require substantial
  revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Network for Neutrino Physics Event Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V Hewes, Adam Aurisano, Giuseppe Cerati, Jim Kowalkowski, Claire Lee, Wei-keng Liao, Daniel Grzenda, Kaushal Gumpula, Xiaohe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a
wealth of high-resolution information on particle interactions, and leveraging
that information to its full potential requires sophisticated automated
reconstruction techniques. This article describes NuGraph2, a Graph Neural
Network (GNN) for low-level reconstruction of simulated neutrino interactions
in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE
detector geometry are described as heterogeneous graphs, with energy
depositions on each detector plane forming nodes on planar subgraphs. The
network utilizes a multi-head attention message-passing mechanism to perform
background filtering and semantic labelling on these graph nodes, identifying
those associated with the primary physics interaction with 98.0\% efficiency
and labelling them according to particle type with 94.9\% efficiency. The
network operates directly on detector observables across multiple 2D
representations, but utilizes a 3D-context-aware mechanism to encourage
consistency between these representations. Model inference takes 0.12~s/event
on a CPU, and 0.005s/event batched on a GPU. This architecture is designed to
be a general-purpose solution for particle reconstruction in neutrino physics,
with the potential for deployment across a broad range of detector
technologies, and offers a core convolution engine that can be leveraged for a
variety of tasks beyond the two described in this article.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 14 figures, published in Physical Review D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Sample Complexity of Learning Lipschitz Operators with respect to
  Gaussian Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Adcock, Michael Griebel, Gregor Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning, the approximation of mappings between infinite-dimensional
function spaces using machine learning, has gained increasing research
attention in recent years. Approximate operators, learned from data, can serve
as efficient surrogate models for problems in computational science and
engineering, complementing traditional methods. However, despite their
empirical success, our understanding of the underlying mathematical theory is
in large part still incomplete. In this paper, we study the approximation of
Lipschitz operators with respect to Gaussian measures. We prove higher Gaussian
Sobolev regularity of Lipschitz operators and establish lower and upper bounds
on the Hermite polynomial approximation error. We then study general
reconstruction strategies of Lipschitz operators from $m$ arbitrary
(potentially adaptive) linear samples. As a key finding, we tightly
characterize the corresponding sample complexity, that is, the smallest
achievable worst-case error among all possible choices of (adaptive) sampling
and reconstruction strategies in terms of $m$. As a consequence, we identify an
inherent curse of sample complexity: No method to approximate Lipschitz
operators based on $m$ linear samples can achieve algebraic convergence rates
in $m$. On the positive side, we prove that a sufficiently fast spectral decay
of the covariance operator of the underlying Gaussian measure guarantees
convergence rates which are arbitrarily close to any algebraic rate. Overall,
by tightly characterizing the sample complexity, our work confirms the
intrinsic difficulty of learning Lipschitz operators, regardless of the data or
learning technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Section 6 about pointwise sampling in v2 of this paper has been cut
  and will appear elsewhere</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trac<span class="highlight-title">LLM</span>: A Generic Framework for Attributing Long Context <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long context large language models (LLMs) are deployed in many real-world
applications such as RAG, agent, and broad LLM-integrated applications. Given
an instruction and a long context (e.g., documents, PDF files, webpages), a
long context LLM can generate an output grounded in the provided context,
aiming to provide more accurate, up-to-date, and verifiable outputs while
reducing hallucinations and unsupported claims. This raises a research
question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)
in the context that contribute most to or are responsible for the generated
output by an LLM? This process, which we call context traceback, has various
real-world applications, such as 1) debugging LLM-based systems, 2) conducting
post-attack forensic analysis for attacks (e.g., prompt injection attack,
knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources
to enhance the trust of users towards outputs generated by LLMs. When applied
to context traceback for long context LLMs, existing feature attribution
methods such as Shapley have sub-optimal performance and/or incur a large
computational cost. In this work, we develop TracLLM, the first generic context
traceback framework tailored to long context LLMs. Our framework can improve
the effectiveness and efficiency of existing feature attribution methods. To
improve the efficiency, we develop an informed search based algorithm in
TracLLM. We also develop contribution score ensemble/denoising techniques to
improve the accuracy of TracLLM. Our evaluation results show TracLLM can
effectively identify texts in a long context that lead to the output of an LLM.
Our code and data are at: https://github.com/Wang-Yanting/TracLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in USENIX Security Symposium 2025. The code and data are
  at: https://github.com/Wang-Yanting/TracLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning as Computationally Constrained Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Stochastic Cubic Newton with Momentum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Mahdi Chayti, Nikita Doikov, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study stochastic second-order methods for solving general non-convex
optimization problems. We propose using a special version of momentum to
stabilize the stochastic gradient and Hessian estimates in Newton's method. We
show that momentum provably improves the variance of stochastic estimates and
allows the method to converge for any noise level. Using the cubic
regularization technique, we prove a global convergence rate for our method on
general non-convex problems to a second-order stationary point, even when using
only a single stochastic data sample per iteration. This starkly contrasts with
all existing stochastic second-order methods for non-convex problems, which
typically require large batches. Therefore, we are the first to demonstrate
global convergence for batches of arbitrary size in the non-convex case for the
Stochastic Cubic Newton. Additionally, we show improved speed on convex
stochastic problems for our regularized Newton methods with momentum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action-Minimization Meets Generative Modeling: Efficient Transition Path
  Sampling with the Onsager-Machlup Functional <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.18506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.18506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjeev Raja, Martin Šípka, Michael Psenka, Tobias Kreiman, Michal Pavelka, Aditi S. Krishnapriyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transition path sampling (TPS), which involves finding probable paths
connecting two points on an energy landscape, remains a challenge due to the
complexity of real-world atomistic systems. Current machine learning approaches
use expensive, task-specific, and data-free training procedures, limiting their
ability to benefit from high-quality datasets and large-scale pre-trained
models. In this work, we address TPS by interpreting candidate paths as
trajectories sampled from stochastic dynamics induced by the learned score
function of pre-trained generative models, specifically denoising diffusion and
flow matching. Under these dynamics, finding high-likelihood transition paths
becomes equivalent to minimizing the Onsager-Machlup (OM) action functional.
This enables us to repurpose pre-trained generative models for TPS in a
zero-shot manner, in contrast with bespoke, task-specific approaches in
previous work. We demonstrate our approach on varied molecular systems,
obtaining diverse, physically realistic transition pathways and generalizing
beyond the pre-trained model's original training dataset. Our method can be
easily incorporated into new generative models, making it practically relevant
as models continue to scale and improve with increased data availability. Code
is available at github.com/ASK-Berkeley/OM-TPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning of Lab Values via Masked AutoEncoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Restrepo, Chenwei Wu, Yueran Jia, Jaden K. Sun, Jack Gallifant, Catherine G. Bielick, Yugang Jia, Leo A. Celi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate imputation of missing laboratory values in electronic health records
(EHRs) is critical to enable robust clinical predictions and reduce biases in
AI systems in healthcare. Existing methods, such as XGBoost, softimpute, GAIN,
Expectation Maximization (EM), and MICE, struggle to model the complex temporal
and contextual dependencies in EHR data, particularly in underrepresented
groups. In this work, we propose Lab-MAE, a novel transformer-based masked
autoencoder framework that leverages self-supervised learning for the
imputation of continuous sequential lab values. Lab-MAE introduces a structured
encoding scheme that jointly models laboratory test values and their
corresponding timestamps, enabling explicit capturing temporal dependencies.
Empirical evaluation on the MIMIC-IV dataset demonstrates that Lab-MAE
significantly outperforms state-of-the-art baselines such as XGBoost,
softimpute, GAIN, EM, and MICE across multiple metrics, including root mean
square error (RMSE), R-squared (R2), and Wasserstein distance (WD). Notably,
Lab-MAE achieves equitable performance across demographic groups of patients,
advancing fairness in clinical predictions. We further investigate the role of
follow-up laboratory values as potential shortcut features, revealing Lab-MAE's
robustness in scenarios where such data is unavailable. The findings suggest
that our transformer-based architecture, adapted to the characteristics of EHR
data, offers a foundation model for more accurate and fair clinical imputation.
In addition, we measure and compare the carbon footprint of Lab-MAE with the a
XGBoost model, highlighting its environmental requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages of main text, 11 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in
  Mobile Health Apps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timoteo Kelly, Abdulkadir Korkmaz, Samuel Mallet, Connor Souders, Sadra Aliakbarpour, Praveen Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HARPT, a large-scale annotated corpus of mobile health app store
reviews aimed at advancing research in user privacy and trust. The dataset
comprises over 480,000 user reviews labeled into seven categories that capture
critical aspects of trust in applications, trust in providers and privacy
concerns. Creating HARPT required addressing multiple complexities, such as
defining a nuanced label schema, isolating relevant content from large volumes
of noisy data, and designing an annotation strategy that balanced scalability
with accuracy. This strategy integrated rule-based filtering, iterative manual
labeling with review, targeted data augmentation, and weak supervision using
transformer-based classifiers to accelerate coverage. In parallel, a carefully
curated subset of 7,000 reviews was manually annotated to support model
development and evaluation. We benchmark a broad range of classification
models, demonstrating that strong performance is achievable and providing a
baseline for future research. HARPT is released as a public resource to support
work in health informatics, cybersecurity, and natural language processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent <span class="highlight-title">Diffusion</span> Model Based Denoising Receiver for 6G Semantic
  Communication: From Stochastic Differential Theory to Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiucheng Wang, Honggang Jia, Nan Cheng, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel semantic communication framework empowered by
generative artificial intelligence (GAI) is proposed, to enhance the robustness
against both channel noise and transmission data distribution shifts. A
theoretical foundation is established using stochastic differential equations
(SDEs), from which a closed-form mapping between any signal-to-noise ratio
(SNR) and the optimal denoising timestep is derived. Moreover, to address
distribution mismatch, a mathematical scaling method is introduced to align
received semantic features with the training distribution of the GAI. Built on
this theoretical foundation, a latent diffusion model (LDM)-based semantic
communication framework is proposed that combines a variational autoencoder for
semantic features extraction, where a pretrained diffusion model is used for
denoising. The proposed system is a training-free framework that supports
zero-shot generalization, and achieves superior performance under low-SNR and
out-of-distribution conditions, offering a scalable and robust solution for
future 6G semantic communication systems. Experimental results demonstrate that
the proposed semantic communication framework achieves state-of-the-art
performance in both pixel-level accuracy and semantic perceptual quality,
consistently outperforming baselines across a wide range of SNRs and data
distributions without any fine-tuning or post-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Ability of Deep Networks to Learn Symmetries from Data: A Neural
  Kernel Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Perin, Stephane Deny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetries (transformations by group actions) are present in many datasets,
and leveraging them holds considerable promise for improving predictions in
machine learning. In this work, we aim to understand when and how deep networks
-- with standard architectures trained in a standard, supervised way -- learn
symmetries from data. Inspired by real-world scenarios, we study a
classification paradigm where data symmetries are only partially observed
during training: some classes include all transformations of a cyclic group,
while others -- only a subset. In the infinite-width limit, where kernel
analogies apply, we derive a neural kernel theory of symmetry learning. The
group-cyclic nature of the dataset allows us to analyze the Gram matrix of
neural kernels in the Fourier domain; here we find a simple characterization of
the generalization error as a function of class separation (signal) and
class-orbit density (noise). This characterization reveals that generalization
can only be successful when the local structure of the data prevails over its
non-local, symmetry-induced structure, in the kernel space defined by the
architecture. We extend our theoretical treatment to any finite group,
including non-abelian groups. Our framework also applies to equivariant
architectures (e.g., CNNs), and recovers their success in the special case
where the architecture matches the inherent symmetry of the data. Empirically,
our theory reproduces the generalization failure of finite-width networks (MLP,
CNN, ViT) trained on partially observed versions of rotated-MNIST. We conclude
that conventional deep networks lack a mechanism to learn symmetries that have
not been explicitly embedded in their architecture a priori. Our framework
could be extended to guide the design of architectures and training procedures
able to learn symmetries from data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JMLR accepted version, including an extension of the theory to
  general finite groups (including non-abelian groups)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Value of Information towards Joint Communication and Control in
  6G V2X 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lei, Kan Zheng,  Xuemin,  Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Cellular Vehicle-to-Everything (C-V2X) evolves towards future
sixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are
emerging to become a key application. Leveraging data-driven Machine Learning
(ML), especially Deep Reinforcement Learning (DRL), is expected to
significantly enhance CAV decision-making in both vehicle control and V2X
communication under uncertainty. These two decision-making processes are
closely intertwined, with the value of information (VoI) acting as a crucial
bridge between them. In this paper, we introduce Sequential Stochastic Decision
Process (SSDP) models to define and assess VoI, demonstrating their application
in optimizing communication systems for CAVs. Specifically, we formally define
the SSDP model and demonstrate that the MDP model is a special case of it. The
SSDP model offers a key advantage by explicitly representing the set of
information that can enhance decision-making when available. Furthermore, as
current research on VoI remains fragmented, we propose a systematic VoI
modeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal
Control theories. We define different categories of VoI and discuss their
corresponding estimation methods. Finally, we present a structured approach to
leverage the various VoI metrics for optimizing the ``When", ``What", and
``How" to communicate problems. For this purpose, SSDP models are formulated
with VoI-associated reward functions derived from VoI-based optimization
objectives. While we use a simple vehicle-following control problem to
illustrate the proposed methodology, it holds significant potential to
facilitate the joint optimization of stochastic, sequential control and
communication decisions in a wide range of networked control systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PuriDefense: Randomized Local Implicit Adversarial Purification for
  Defending Black-box Query-based Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Guo, Xiang Li, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model's architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Bounds for Robust Online Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Appel, Vanessa Kosoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a framework which generalizes "decision making with structured
observations" by allowing robust (i.e. multivalued) models. In this framework,
each model associates each decision with a convex set of probability
distributions over outcomes. Nature can choose distributions out of this set in
an arbitrary (adversarial) manner, that can be nonoblivious and depend on past
history. The resulting framework offers much greater generality than classical
bandits and reinforcement learning, since the realizability assumption becomes
much weaker and more realistic. We then derive a theory of regret bounds for
this framework. Although our lower and upper bounds are not tight, they are
sufficient to fully characterize power-law learnability. We demonstrate this
theory in two special cases: robust linear bandits and tabular robust online
reinforcement learning. In both cases, we derive regret bounds that improve
state-of-the-art (except that we do not address computational efficiency).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Scalable Quantum Neural Network for Approximate SRBB-Based Unitary
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Belli, Marco Mordacci, Michele Amoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, a scalable quantum neural network is introduced as a means to
approximate any unitary evolution through the Standard Recursive Block Basis
(SRBB) and, subsequently, redesigned with a number of CNOTs asymptotically
reduced by an exponential contribution. This algebraic approach to the problem
of unitary synthesis exploits Lie algebras and their topological features to
obtain scalable parameterizations of unitary operators. First, the original
SRBB-based scalability scheme, already known in the literature only from a
theoretical point of view, is reformulated for efficient algorithm
implementation and complexity management. Remarkably, 2-qubit operators emerge
as a special case outside the original scaling scheme. Furthermore, an
algorithm is proposed to reduce the number of CNOTs, thus deriving a new
implementable scaling scheme that requires only one layer of approximation. The
scalable CNOT-reduced quantum neural network is implemented and its performance
is assessed with a variety of different unitary matrices, both sparse and
dense, up to 6 qubits via the PennyLane library. The effectiveness of the
approximation is measured with different metrics in relation to two optimizers:
a gradient-based method and the Nelder-Mead method. The approximate
CNOT-reduced SRBB-based synthesis algorithm is also tested on real hardware and
compared with other valid approximation and decomposition methods available in
the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order
  Neighboring Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.15920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.15920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Jianpeng Qi, Haobing Liu, Yuan Cao, Guoqing Chao, Zhongying Zhao, Junyu Dong, Yanwei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated impressive performance across
diverse graph-based tasks by leveraging message passing to capture complex node
relationships. However, when applied to large-scale real-world graphs, GNNs
face two major challenges: First, it becomes increasingly difficult to ensure
both scalability and efficiency, as the repeated aggregation of large
neighborhoods leads to significant computational overhead; Second, the
over-smoothing problem arises, where excessive or deep propagation makes node
representations indistinguishable, severely hindering model expressiveness. To
tackle these issues, we propose ScaleGNN, a novel framework that adaptively
fuses multi-hop node features for both scalable and effective graph learning.
First, we construct per-hop pure neighbor matrices that capture only the
exclusive structural information at each hop, avoiding the redundancy of
conventional aggregation. Then, an enhanced feature fusion strategy
significantly balances low-order and high-order information, preserving both
local detail and global correlations without incurring excessive complexity. To
further reduce redundancy and over-smoothing, we introduce a Local Contribution
Score (LCS)-based masking mechanism to filter out less relevant high-order
neighbors, ensuring that only the most meaningful information is aggregated. In
addition, learnable sparse constraints selectively integrate multi-hop valuable
features, emphasizing the most informative high-order neighbors. Extensive
experiments on real-world datasets demonstrate that ScaleGNN consistently
outperforms state-of-the-art GNNs in both predictive accuracy and computational
efficiency, highlighting its practical value for large-scale graph learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Doubly-Robust Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Ruah, Houssem Sifaou, Osvaldo Simeone, Bashir Al-Hashimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of artificial intelligence (AI) in next-generation
communication systems is challenged by the heterogeneity of traffic and network
conditions, which call for the use of highly contextual, site-specific, data. A
promising solution is to rely not only on real-world data, but also on
synthetic pseudo-data generated by a network digital twin (NDT). However, the
effectiveness of this approach hinges on the accuracy of the NDT, which can
vary widely across different contexts. To address this problem, this paper
introduces context-aware doubly-robust (CDR) learning, a novel semi-supervised
scheme that adapts its reliance on the pseudo-data to the different levels of
fidelity of the NDT across contexts. CDR is evaluated on the task of downlink
beamforming where it outperforms previous state-of-the-art approaches,
providing a 24% loss decrease when compared to doubly-robust (DR)
semi-supervised learning in regimes with low labeled data availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication in IEEE Signal Processing
  Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Scene Graph for Ultrasound Image Explanation and Scanning
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuesong Li, Dianye Huang, Yameng Zhang, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding medical ultrasound imaging remains a long-standing challenge
due to significant visual variability caused by differences in imaging and
acquisition parameters. Recent advancements in large language models (LLMs)
have been used to automatically generate terminology-rich summaries orientated
to clinicians with sufficient physiological knowledge. Nevertheless, the
increasing demand for improved ultrasound interpretability and basic scanning
guidance among non-expert users, e.g., in point-of-care settings, has not yet
been explored. In this study, we first introduce the scene graph (SG) for
ultrasound images to explain image content to ordinary and provide guidance for
ultrasound scanning. The ultrasound SG is first computed using a
transformer-based one-stage method, eliminating the need for explicit object
detection. To generate a graspable image explanation for ordinary, the user
query is then used to further refine the abstract SG representation through
LLMs. Additionally, the predicted SG is explored for its potential in guiding
ultrasound scanning toward missing anatomies within the current imaging view,
assisting ordinary users in achieving more standardized and complete anatomical
exploration. The effectiveness of this SG-based image explanation and scanning
guidance has been validated on images from the left and right neck regions,
including the carotid and thyroid, across five volunteers. The results
demonstrate the potential of the method to maximally democratize ultrasound by
enhancing its interpretability and usability for ordinaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning
  Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longzhu He, Chaozhuo Li, Peng Tang, Li Sun, Sen Su, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy Matching: Unifying Flow Matching and Energy-Based Models for
  Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10612v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10612v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Lea Bogensperger, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most widely used generative models map noise and data distributions by
matching flows or scores. However, they struggle to incorporate partial
observations and additional priors--something energy-based models (EBMs) handle
elegantly by simply adding corresponding scalar energy terms. We address this
issue by proposing Energy Matching, a framework that endows flow-based
approaches with the flexibility of EBMs. Far from the data manifold, samples
move along curl-free, optimal transport paths from noise to data. As they
approach the data manifold, an entropic energy term guides the system into a
Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in
terms of fidelity, while retaining simulation-free training of transport-based
approaches away from the data manifold. Furthermore, we leverage the method's
flexibility to introduce an interaction energy that supports diverse mode
exploration, which we demonstrate in a controlled protein-generation setting.
Our approach focuses on learning a scalar potential energy--without
time-conditioning, auxiliary generators, or additional networks--which marks a
significant departure from recent EBM methods. We believe that this simplified
framework significantly advances EBMs capabilities and paves the way for their
wider adoption in generative modeling across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lagrangian Index Policy for Restless Bandits with Average Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Avrachenkov, Vivek S. Borkar, Pratik Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Lagrange Index Policy (LIP) for restless multi-armed bandits
with long-run average reward. In particular, we compare the performance of LIP
with the performance of the Whittle Index Policy (WIP), both heuristic policies
known to be asymptotically optimal under certain natural conditions. Even
though in most cases their performances are very similar, in the cases when WIP
shows bad performance, LIP continues to perform very well. We then propose
reinforcement learning algorithms, both tabular and NN-based, to obtain online
learning schemes for LIP in the model-free setting. The proposed reinforcement
learning schemes for LIP require significantly less memory than the analogous
schemes for WIP. We calculate analytically the Lagrange index for the restart
model, which applies to the optimal web crawling and the minimization of the
weighted age of information. We also give a new proof of asymptotic optimality
in case of homogeneous arms as the number of arms goes to infinity, based on
exchangeability and de Finetti's theorem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A GREAT Architecture for Edge-Based Graph Problems Like TSP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Attila Lischka, Filip Rydin, Jiaming Wu, Morteza Haghir Chehreghani, Balázs Kulcsár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last years, many learning-based approaches have been proposed to
tackle combinatorial optimization problems such as routing problems. Many of
these approaches are based on graph neural networks (GNNs) or related
transformers, operating on the Euclidean coordinates representing the routing
problems. However, models operating on Euclidean coordinates are ill-suited for
non-Euclidean, asymmetric problem instances that are often found in real-world
settings. To overcome this limitation, we propose a novel GNN-based and
edge-focused neural model called Graph Edge Attention Network (GREAT). Using
GREAT as an encoder to capture the properties of a routing problem instance, we
build a reinforcement learning framework which we apply to Euclidean and
non-Euclidean variants of vehicle routing problems such as Traveling Salesman
Problem, Capacitated Vehicle Routing Problem and Orienteering Problem. Our
framework is among the first to tackle non-Euclidean variants of these problems
and achieves competitive results among learning-based solvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ These Are Not All the Features You Are Looking For: A Fundamental
  Bottleneck in Supervised Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Alice Yang, Jianyu Zhang, Léon Bottou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating Hard Attention Using Soft Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Yang, Lena Strobl, David Chiang, Dana Angluin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study conditions under which transformers using soft attention can
simulate hard attention, that is, effectively focus all attention on a subset
of positions. First, we examine several subclasses of languages recognized by
hard-attention transformers, which can be defined in variants of linear
temporal logic. We demonstrate how soft-attention transformers can compute
formulas of these logics using unbounded positional embeddings or temperature
scaling. Second, we demonstrate how temperature scaling allows softmax
transformers to simulate general hard-attention transformers, using a
temperature that depends on the minimum gap between the maximum attention
scores and other attention scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wavelet <span class="highlight-title">Diffusion</span> Neural Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04833v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04833v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Hu, Rui Wang, Xiang Zheng, Tao Zhang, Haodong Feng, Ruiqi Feng, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating and controlling physical systems described by partial differential
equations (PDEs) are crucial tasks across science and engineering. Recently,
diffusion generative models have emerged as a competitive class of methods for
these tasks due to their ability to capture long-term dependencies and model
high-dimensional states. However, diffusion models typically struggle with
handling system states with abrupt changes and generalizing to higher
resolutions. In this work, we propose Wavelet Diffusion Neural Operator (WDNO),
a novel PDE simulation and control framework that enhances the handling of
these complexities. WDNO comprises two key innovations. Firstly, WDNO performs
diffusion-based generative modeling in the wavelet domain for the entire
trajectory to handle abrupt changes and long-term dependencies effectively.
Secondly, to address the issue of poor generalization across different
resolutions, which is one of the fundamental tasks in modeling physical
systems, we introduce multi-resolution training. We validate WDNO on five
physical systems, including 1D advection equation, three challenging physical
systems with abrupt changes (1D Burgers' equation, 1D compressible
Navier-Stokes equation and 2D incompressible fluid), and a real-world dataset
ERA5, which demonstrates superior performance on both simulation and control
tasks over state-of-the-art methods, with significant improvements in long-term
and detail prediction accuracy. Remarkably, in the challenging context of the
2D high-dimensional and indirect control task aimed at reducing smoke leakage,
WDNO reduces the leakage by 78% compared to the second-best baseline. The code
can be found at https://github.com/AI4Science-WestlakeU/wdno.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radio Map Estimation via Latent <span class="highlight-title">Domain</span> Plug-and-Play Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Xu, Lei Cheng, Junting Chen, Wenqiang Pu, Xiao Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio map estimation (RME), also known as spectrum cartography, aims to
reconstruct the strength of radio interference across different domains (e.g.,
space and frequency) from sparsely sampled measurements. To tackle this typical
inverse problem, state-of-the-art RME methods rely on handcrafted or
data-driven structural information of radio maps. However, the former often
struggles to model complex radio frequency (RF) environments and the latter
requires excessive training -- making it hard to quickly adapt to in situ
sensing tasks. This work presents a spatio-spectral RME approach based on
plug-and-play (PnP) denoising, a technique from computational imaging. The idea
is to leverage the observation that the denoising operations of signals like
natural images and radio maps are similar -- despite the nontrivial differences
of the signals themselves. Hence, sophisticated denoisers designed for or
learned from natural images can be directly employed to assist RME, avoiding
using radio map data for training. Unlike conventional PnP methods that operate
directly in the data domain, the proposed method exploits the underlying
physical structure of radio maps and proposes an ADMM algorithm that denoises
in a latent domain. This design significantly improves computational efficiency
and enhances noise robustness. Theoretical aspects, e.g., recoverability of the
complete radio map and convergence of the ADMM algorithm are analyzed.
Synthetic and real data experiments are conducted to demonstrate the
effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing Style in Author and Document Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Terreau, Antoine Gourru, Julien Velcin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A wide range of Deep Natural Language Processing (NLP) models integrates
continuous and low dimensional representations of words and documents.
Surprisingly, very few models study representation learning for authors. These
representations can be used for many NLP tasks, such as author identification
and classification, or in recommendation systems. A strong limitation of
existing works is that they do not explicitly capture writing style, making
them hardly applicable to literary data. We therefore propose a new
architecture based on Variational Information Bottleneck (VIB) that learns
embeddings for both authors and documents with a stylistic constraint. Our
model fine-tunes a pre-trained document encoder. We stimulate the detection of
writing style by adding predefined stylistic features making the representation
axis interpretable with respect to writing style indicators. We evaluate our
method on three datasets: a literary corpus extracted from the Gutenberg
Project, the Blog Authorship Corpus and IMDb62, for which we show that it
matches or outperforms strong/recent baselines in authorship attribution while
capturing much more accurately the authors stylistic aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid Gyroscope Calibration: A Deep Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Stolero, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-cost gyroscope calibration is essential for ensuring the accuracy and
reliability of gyroscope measurements. Stationary calibration estimates the
deterministic parts of measurement errors. To this end, a common practice is to
average the gyroscope readings during a predefined period and estimate the
gyroscope bias. Calibration duration plays a crucial role in performance,
therefore, longer periods are preferred. However, some applications require
quick startup times and calibration is therefore allowed only for a short time.
In this work, we focus on reducing low-cost gyroscope calibration time using
deep learning methods. We propose an end-to-end convolutional neural network
for the application of gyroscope calibration. We explore the possibilities of
using multiple real and virtual gyroscopes to improve the calibration
performance of single gyroscopes. To train and validate our approach, we
recorded a dataset consisting of 186.6 hours of gyroscope readings, using 36
gyroscopes of four different brands. We also created a virtual dataset
consisting of simulated gyroscope readings. The six datasets were used to
evaluate our proposed approach. One of our key achievements in this work is
reducing gyroscope calibration time by up to 89% using three low-cost
gyroscopes. Our dataset is publicly available to allow reproducibility of our
work and to increase research in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 14 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Privacy, Robustness, and Efficiency in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14712v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14712v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Allouah, Rachid Guerraoui, John Stephan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper argues that achieving robustness, privacy, and efficiency
simultaneously in machine learning systems is infeasible under prevailing
threat models. The tension between these goals arises not from algorithmic
shortcomings but from structural limitations imposed by worst-case adversarial
assumptions. We advocate for a systematic research agenda aimed at formalizing
the robustness-privacy-efficiency trilemma, exploring how principled
relaxations of threat models can unlock better trade-offs, and designing
benchmarks that expose rather than obscure the compromises made. By shifting
focus from aspirational universal guarantees to context-aware system design,
the machine learning community can build models that are truly appropriate for
real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Learning for Optimal Transport plan prediction between
  unbalanced graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia Mazelet, Rémi Flamary, Bertrand Thirion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport between graphs, based on Gromov-Wasserstein and
  other extensions, is a powerful tool for comparing and aligning
  graph structures. However, solving the associated non-convex
  optimization problems is computationally expensive, which limits the
  scalability of these methods to large graphs. In this work, we
  present Unbalanced Learning of Optimal Transport (ULOT), a deep
  learning method that predicts optimal transport plans between two
  graphs. Our method is trained by minimizing the fused unbalanced
  Gromov-Wasserstein (FUGW) loss. We propose a novel neural
  architecture with cross-attention that is conditioned on the FUGW
  tradeoff hyperparameters. We evaluate ULOT on synthetic stochastic
  block model (SBM) graphs and on real cortical surface data obtained
  from fMRI. ULOT predicts transport plans with competitive loss up to
  two orders of magnitude faster than classical solvers. Furthermore,
  the predicted plan can be used as a warm start for classical solvers
  to accelerate their convergence. Finally, the predicted transport
  plan is fully differentiable with respect to the graph inputs and
  FUGW hyperparameters, enabling the optimization of functionals of
  the ULOT plan.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-Based Human-Agent Collaboration and Interaction Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00753v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00753v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
These human-agent collaboration systems enable humans and LLM-based agents to
collaborate effectively by leveraging their complementary strengths. This paper
provides the first comprehensive and structured survey of LLM-HAS. It clarifies
fundamental concepts, systematically presents core components shaping these
systems, including environment & profiling, human feedback, interaction types,
orchestration and communication, explores emerging applications, and discusses
unique challenges and opportunities arising from human-AI collaboration. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper lists and resources are available at
  https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seal Your Backdoor with Variational Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Sabolić, Matej Grcić, Siniša Šegvić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose VIBE, a model-agnostic framework that trains classifiers resilient
to backdoor attacks. The key concept behind our approach is to treat malicious
inputs and corrupted labels from the training dataset as observed random
variables, while the actual clean labels are latent. VIBE then recovers the
corresponding latent clean label posterior through variational inference. The
resulting training procedure follows the expectation-maximization (EM)
algorithm. The E-step infers the clean pseudolabels by solving an
entropy-regularized optimal transport problem, while the M-step updates the
classifier parameters via gradient descent. Being modular, VIBE can seamlessly
integrate with recent advancements in self-supervised representation learning,
which enhance its ability to resist backdoor attacks. We experimentally
validate the method effectiveness against contemporary backdoor attacks on
standard datasets, a large-scale setup with 1$k$ classes, and a dataset
poisoned with multiple attacks. VIBE consistently outperforms previous defenses
across all tested scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance
  6-DoF Grasp 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaofeng Cheng, Fusheng Zha, Wei Guo, Pengfei Wang, Chao Zeng, Lining Sun, Chenguang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown
significant potential in enabling robots to grasp target objects. However, most
existing methods are based on the point clouds (2.5D points) generated from
single-view depth images. These point clouds only have one surface side of the
object providing incomplete geometry information, which mislead the grasping
algorithm to judge the shape of the target object, resulting in low grasping
accuracy. Humans can accurately grasp objects from a single view by leveraging
their geometry experience to estimate object shapes. Inspired by humans, we
propose a novel 6-DoF grasping framework that converts the point completion
results as object shape features to train the 6-DoF grasp network. Here, point
completion can generate approximate complete points from the 2.5D points
similar to the human geometry experience, and converting it as shape features
is the way to utilize it to improve grasp efficiency. Furthermore, due to the
gap between the network generation and actual execution, we integrate a score
filter into our framework to select more executable grasp proposals for the
real robot. This enables our method to maintain a high grasp quality in any
camera viewpoint. Extensive experiments demonstrate that utilizing complete
point features enables the generation of significantly more accurate grasp
proposals and the inclusion of a score filter greatly enhances the credibility
of real-world robot grasping. Our method achieves a 17.8\% success rate higher
than the state-of-the-art method in real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Supervised Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Wang, Jiajun Fan, Thao Nguyen, Heng Ji, Ge Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moderating the Generalization of Score-based Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wan Jiang, He Wang, Xin Zhang, Dan Guo, Zhaoxin Fan, Yunfeng Diao, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based Generative Models (SGMs) have demonstrated remarkable
generalization abilities, e.g. generating unseen, but natural data. However,
the greater the generalization power, the more likely the unintended
generalization, and the more dangerous the abuse. Research on moderated
generalization in SGMs remains limited. To fill this gap, we first examine the
current 'gold standard' in Machine Unlearning (MU), i.e., re-training the model
after removing the undesirable training data, and find it does not work in
SGMs. Further analysis of score functions reveals that the MU 'gold standard'
does not alter the original score function, which explains its ineffectiveness.
Based on this insight, we propose the first Moderated Score-based Generative
Model (MSGM), which introduces a novel score adjustment strategy that redirects
the score function away from undesirable data during the continuous-time
stochastic differential equation process. Extensive experimental results
demonstrate that MSGM significantly reduces the likelihood of generating
undesirable content while preserving high visual quality for normal image
generation. Albeit designed for SGMs, MSGM is a general and flexible MU
framework that is compatible with diverse diffusion architectures (SGM and
DDPM) and training strategies (re-training and fine-tuning), and enables
zero-shot transfer of the pre-trained models to downstream tasks, e.g. image
inpainting and reconstruction. The code will be shared upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have witnessed a surge in
the development of advanced reasoning paradigms, which are now being integrated
into multimodal large language models (MLLMs). However, existing approaches
often fall short: methods solely employing reinforcement learning (RL) can
struggle with sample inefficiency and activating entirely absent reasoning
capabilities, while conventional pipelines that initiate with a cold-start
supervised fine-tuning (SFT) phase before RL may restrict the model's
exploratory capacity and face suboptimal convergence. In this work, we
introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and
\textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike
conventional approaches, Metis-RISE distinctively omits an initial SFT stage,
beginning instead with an RL phase (e.g., using a Group Relative Policy
Optimization variant) to incentivize and activate the model's latent reasoning
capacity. Subsequently, the targeted SFT stage addresses two key challenges
identified during RL: (1) \textit{inefficient trajectory sampling} for tasks
where the model possesses but inconsistently applies correct reasoning, which
we tackle using self-distilled reasoning trajectories from the RL model itself;
and (2) \textit{fundamental capability absence}, which we address by injecting
expert-augmented knowledge for prompts where the model entirely fails. This
strategic application of RL for incentivization followed by SFT for enhancement
forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B
parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard
demonstrate that both models achieve state-of-the-art performance among
similar-sized models, with the 72B version ranking fourth overall. Please refer
to our project page for open-source information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/MM-Thinking/Metis-RISE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Regulated Neurogenesis for Online Data-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Onur Yildirim, Elif Ceren Gok Yildirim, Decebal Constantin Mocanu, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks often struggle with catastrophic forgetting when learning
sequences of tasks or data streams, unlike humans who can continuously learn
and consolidate new concepts even in the absence of explicit cues. Online
data-incremental learning seeks to emulate this capability by processing each
sample only once, without having access to task or stream cues at any point in
time since this is more realistic compared to offline setups, where all data
from novel class(es) is assumed to be readily available. However, existing
methods typically rely on storing the subsets of data in memory or expanding
the initial model architecture, resulting in significant computational
overhead. Drawing inspiration from 'self-regulated neurogenesis'-brain's
mechanism for creating specialized regions or circuits for distinct
functions-we propose a novel approach SERENA which encodes each concept in a
specialized network path called 'concept cell', integrated into a single
over-parameterized network. Once a concept is learned, its corresponding
concept cell is frozen, effectively preventing the forgetting of previously
acquired information. Furthermore, we introduce two new continual learning
scenarios that more closely reflect real-world conditions, characterized by
gradually changing sample sizes. Experimental results show that our method not
only establishes new state-of-the-art results across ten benchmarks but also
remarkably surpasses offline supervised batch learning performance. The code is
available at https://github.com/muratonuryildirim/serena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Conference on Lifelong Learning Agents (CoLLAs) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Federated Learning-Based IDS for Enhancing UAVs Privacy and
  Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozlem Ceviz, Pinar Sadioglu, Sevil Sen, Vassilios G. Vassilakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicles (UAVs) operating within Flying Ad-hoc Networks
(FANETs) encounter security challenges due to the dynamic and distributed
nature of these networks. Previous studies focused predominantly on centralized
intrusion detection, assuming a central entity responsible for storing and
analyzing data from all devices. However, these approaches face challenges
including computation and storage costs, along with a single point of failure
risk, threatening data privacy and availability. The widespread dispersion of
data across interconnected devices underscores the need for decentralized
approaches. This paper introduces the Federated Learning-based Intrusion
Detection System (FL-IDS), addressing challenges encountered by centralized
systems in FANETs. FL-IDS reduces computation and storage costs for both
clients and the central server, which is crucial for resource-constrained UAVs.
Operating in a decentralized manner, FL-IDS enables UAVs to collaboratively
train a global intrusion detection model without sharing raw data, thus
avoiding delay in decisions based on collected data, as is often the case with
traditional methods. Experimental results demonstrate FL-IDS's competitive
performance with Central IDS (C-IDS) while mitigating privacy concerns, with
the Bias Towards Specific Clients (BTSC) method further enhancing FL-IDS
performance even at lower attacker ratios. Comparative analysis with
traditional intrusion detection methods, including Local IDS (L-IDS), sheds
light on the strengths of FL-IDS. This study significantly contributes to UAV
security by introducing a privacy-aware, decentralized intrusion detection
approach tailored to UAV networks. Moreover, by introducing a realistic dataset
for FANETs and federated learning, our approach differs from others lacking
high dynamism and 3D node movements or accurate federated data federations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Internet of Things, Volume 25, 2025, Article 101592</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-convex Programming for Discrete Latent Factor Models Prototyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhu, Shengchao Yan, Jasper Hoffmann, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete latent factor models (DLFMs) are widely used in various domains such
as machine learning, economics, neuroscience, psychology, etc. Currently,
fitting a DLFM to some dataset relies on a customized solver for individual
models, which requires lots of effort to implement and is limited to the
targeted specific instance of DLFMs. In this paper, we propose a generic
framework based on CVXPY, which allows users to specify and solve the fitting
problem of a wide range of DLFMs, including both regression and classification
models, within a very short script. Our framework is flexible and inherently
supports the integration of regularization terms and constraints on the DLFM
parameters and latent factors, such that the users can easily prototype the
DLFM structure according to their dataset and application scenario. We
introduce our open-source Python implementation and illustrate the framework in
several examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Inverse Problem for Multi-armed Bandits via Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhu, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the inverse problem of multi-armed bandits (IMAB) that are widely
used in neuroscience and psychology research for behavior modelling. We first
show that the IMAB problem is not convex in general, but can be relaxed to a
convex problem via variable transformation. Based on this result, we propose a
two-step sequential heuristic for (approximately) solving the IMAB problem. We
discuss a condition where our method provides global solution to the IMAB
problem with certificate, as well as approximations to further save computing
time. Numerical experiments indicate that our heuristic method is more robust
than directly solving the IMAB problem via repeated local optimization, and can
achieve the performance of Monte Carlo methods within a significantly decreased
running time. We provide the implementation of our method based on CVXPY, which
allows straightforward application by users not well versed in convex
optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverse Reinforcement Learning via Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhu, Yuan Zhang, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the inverse reinforcement learning (IRL) problem, where an
unknown reward function of some Markov decision process is estimated based on
observed expert demonstrations. In most existing approaches, IRL is formulated
and solved as a nonconvex optimization problem, posing challenges in scenarios
where robustness and reproducibility are critical. We discuss a convex
formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and
reformulate the problem such that the domain-specific language CVXPY can be
applied directly to specify and solve the convex problem. We also extend the
CIRL problem to scenarios where the expert policy is not given analytically but
by trajectory as state-action pairs, which can be strongly inconsistent with
optimality, by augmenting some of the constraints. Theoretical analysis and
practical implementation for hyperparameter auto-selection are introduced. This
note helps the users to easily apply CIRL for their problems, without
background knowledge on convex optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic
  Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grigory Bartosh, Dmitry Vetrov, Christian A. Naesseth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Latent Stochastic Differential Equation (SDE) is a powerful tool for time
series and sequence modeling. However, training Latent SDEs typically relies on
adjoint sensitivity methods, which depend on simulation and backpropagation
through approximate SDE solutions, which limit scalability. In this work, we
propose SDE Matching, a new simulation-free method for training Latent SDEs.
Inspired by modern Score- and Flow Matching algorithms for learning generative
dynamics, we extend these ideas to the domain of stochastic dynamics for time
series and sequence modeling, eliminating the need for costly numerical
simulations. Our results demonstrate that SDE Matching achieves performance
comparable to adjoint sensitivity methods while drastically reducing
computational complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharp concentration of uniform generalization errors in binary linear
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shogo Nakakita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the concentration of uniform generalization errors around their
expectation in binary linear classification problems via an isoperimetric
argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities
for the joint distribution of the output labels and the label-weighted input
vectors, which we apply to derive concentration bounds. The derived
concentration bounds are sharp up to moderate multiplicative constants by those
under well-balanced labels. In asymptotic analysis, we also show that almost
sure convergence of uniform generalization errors to their expectation occurs
in very broad settings, such as proportionally high-dimensional regimes. Using
this convergence, we establish uniform laws of large numbers under
dimension-free conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 1 figure; minor edits to improve readability</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SceneGenAgent: Precise Industrial Scene Generation with Coding Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21909v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21909v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling of industrial scenes is essential for simulations in industrial
manufacturing. While large language models (LLMs) have shown significant
progress in generating general 3D scenes from textual descriptions, generating
industrial scenes with LLMs poses a unique challenge due to their demand for
precise measurements and positioning, requiring complex planning over spatial
arrangement. To address this challenge, we introduce SceneGenAgent, an
LLM-based agent for generating industrial scenes through C# code. SceneGenAgent
ensures precise layout planning through a structured and calculable format,
layout verification, and iterative refinement to meet the quantitative
requirements of industrial scenarios. Experiment results demonstrate that LLMs
powered by SceneGenAgent exceed their original performance, reaching up to
81.0% success rate in real-world industrial scene generation tasks and
effectively meeting most scene generation requirements. To further enhance
accessibility, we construct SceneInstruct, a dataset designed for fine-tuning
open-source LLMs to integrate into SceneGenAgent. Experiments show that
fine-tuning open-source LLMs on SceneInstruct yields significant performance
improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our
code and data are available at https://github.com/THUDM/SceneGenAgent .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCDVQ: Enhancing Vector Quantization for <span class="highlight-title">Large Language Model</span>s via Polar
  Coordinate Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yue, Zukang Xu, Zhihang Yuan, Dawei Yang, Jianlong Wu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts-augmented Deep Unfolding for Acti<span class="highlight-title">vit</span>y Detection in
  IRS-aided Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Ren, Qingfeng Lin, Jingreng Lei, Yang Li, Yik-Chung Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of activity detection for massive machine-type communications,
intelligent reflecting surfaces (IRS) have shown significant potential in
enhancing coverage for devices lacking direct connections to the base station
(BS). However, traditional activity detection methods are typically designed
for a single type of channel model, which does not reflect the complexities of
real-world scenarios, particularly in systems incorporating IRS. To address
this challenge, this paper introduces a novel approach that combines
model-driven deep unfolding with a mixture of experts (MoE) framework. By
automatically selecting one of three expert designs and applying it to the
unfolded projected gradient method, our approach eliminates the need for prior
knowledge of channel types between devices and the BS. Simulation results
demonstrate that the proposed MoE-augmented deep unfolding method surpasses the
traditional covariance-based method and black-box neural network design,
delivering superior detection performance under mixed channel fading
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, Accepted in IEEE Wireless Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Image Generation with Variadic Attention Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Walton, Ali Hassani, Xingqian Xu, Zhangyang Wang, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the integration of transformers in vision models have yielded
significant improvements on vision tasks they still require significant amounts
of computation for both training and inference. Restricted attention mechanisms
significantly reduce these computational burdens but come at the cost of losing
either global or local coherence. We propose a simple, yet powerful method to
reduce these trade-offs: allow the attention heads of a single transformer to
attend to multiple receptive fields.
  We demonstrate our method utilizing Neighborhood Attention (NA) and integrate
it into a StyleGAN based architecture for image generation. With this work,
dubbed StyleNAT, we are able to achieve a FID of 2.05 on FFHQ, a 6% improvement
over StyleGAN-XL, while utilizing 28% fewer parameters and with 4$\times$ the
throughput capacity. StyleNAT achieves the Pareto Frontier on FFHQ-256 and
demonstrates powerful and efficient image generation on other datasets. Our
code and model checkpoints are publicly available at:
https://github.com/SHI-Labs/StyleNAT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in eLVM @ CVPR
  (https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Walton_Efficient_Image_Generation_with_Variadic_Attention_Heads_CVPRW_2025_paper)
  | Formerly named StyleNAT: Giving Each Head a New Perspective |</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proximal Point Method for Online Saddle Point Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing-xin Meng, Jian-wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the online saddle point problem, which involves a
sequence of two-player time-varying convex-concave games. Considering the
nonstationarity of the environment, we adopt the duality gap and the dynamic
Nash equilibrium regret as performance metrics for algorithm design. We present
three variants of the proximal point method: the Online Proximal Point Method
(OPPM), the Optimistic OPPM (OptOPPM), and the OptOPPM with multiple
predictors. Each algorithm guarantees upper bounds for both the duality gap and
dynamic Nash equilibrium regret, achieving near-optimality when measured
against the duality gap. Specifically, in certain benign environments, such as
sequences of stationary payoff functions, these algorithms maintain a nearly
constant metric bound. Experimental results further validate the effectiveness
of these algorithms. Lastly, this paper discusses potential reliability
concerns associated with using dynamic Nash equilibrium regret as a performance
metric. The technical appendix and code can be found at
https://github.com/qingxin6174/PPM-for-OSP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Review</span> learning: Real world validation of privacy preserving continual
  learning a<span class="highlight-title">cross</span> medical institutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesung Yoo, Sunghyuk Choi, Ye Seul Yang, Suhyeon Kim, Jieun Choi, Dongkyeong Lim, Yaeji Lim, Hyung Joon Joo, Dae Jung Kim, Rae Woong Park, Hyeong-Jin Yoon, Kwangsoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a deep learning model is trained sequentially on different datasets, it
often forgets the knowledge learned from previous data, a problem known as
catastrophic forgetting. This damages the model's performance on diverse
datasets, which is critical in privacy-preserving deep learning (PPDL)
applications based on transfer learning (TL). To overcome this, we introduce
"review learning" (RevL), a low cost continual learning algorithm for diagnosis
prediction using electronic health records (EHR) within a PPDL framework. RevL
generates data samples from the model which are used to review knowledge from
previous datasets. Six simulated institutional experiments and one real-world
experiment involving three medical institutions were conducted to validate
RevL, using three binary classification EHR data. In the real-world experiment
with data from 106,508 patients, the mean global area under the receiver
operating curve was 0.710 for RevL and 0.655 for TL. These results demonstrate
RevL's ability to retain previously learned knowledge and its effectiveness in
real-world PPDL scenarios. Our work establishes a realistic pipeline for PPDL
research based on model transfers across institutions and highlights the
practicality of continual learning in real-world medical settings using private
EHR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Genetic Algorithm with Innovative Chromosome Patterns in the Breeding
  Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18184v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18184v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Genetic Algorithm with Border Trades (GAB), a novel
modification of the standard genetic algorithm that enhances exploration by
incorporating new chromosome patterns in the breeding process. This approach
significantly mitigates premature convergence and improves search diversity.
Empirically, GAB achieves up to 8x higher fitness and 10x faster convergence on
complex job scheduling problems compared to standard Genetic Algorithms,
reaching average fitness scores of 888 versus 106 in under 20 seconds. On the
classic Flip-Flop problem, GAB consistently finds optimal or near-optimal
solutions in fewer generations, even as input sizes scale to thousands of bits.
These results highlight GAB as a highly effective and computationally efficient
alternative for solving large-scale combinatorial optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretrained Reversible Generation as Unsupervised Visual Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01787v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01787v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongkun Xue, Jinouwen Zhang, Yazhe Niu, Dazhong Shen, Bingqi Ma, Yu Liu, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models based on score matching and flow matching have
significantly advanced generation tasks, but their potential in discriminative
tasks remains underexplored. Previous approaches, such as generative
classifiers, have not fully leveraged the capabilities of these models for
discriminative tasks due to their intricate designs. We propose Pretrained
Reversible Generation (PRG), which extracts unsupervised representations by
reversing the generative process of a pretrained continuous generation model.
PRG effectively reuses unsupervised generative models, leveraging their high
capacity to serve as robust and generalizable feature extractors for downstream
tasks. This framework enables the flexible selection of feature hierarchies
tailored to specific downstream tasks. Our method consistently outperforms
prior approaches across multiple benchmarks, achieving state-of-the-art
performance among generative model based methods, including 78% top-1 accuracy
on ImageNet at a resolution of 64*64. Extensive ablation studies, including
out-of-distribution evaluations, further validate the effectiveness of our
approach. Code is available at https://github.com/opendilab/PRG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap Between Approximation and Learning via Optimal
  Approximation by ReLU <span class="highlight-title">MLP</span>s of Maximal Regularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12335v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12335v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Hong, Anastasis Kratsios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The foundations of deep learning are supported by the seemingly opposing
perspectives of approximation or learning theory. The former advocates for
large/expressive models that need not generalize, while the latter considers
classes that generalize but may be too small/constrained to be universal
approximators. Motivated by real-world deep learning implementations that are
both expressive and statistically reliable, we ask: "Is there a class of neural
networks that is both large enough to be universal but structured enough to
generalize?" This paper constructively provides a positive answer to this
question by identifying a highly structured class of ReLU multilayer
perceptions (MLPs), which are optimal function approximators and are
statistically well-behaved. We show that any $(L,\alpha)$-H\"{o}lder function
from $[0,1]^d$ to $[-n,n]$ can be approximated to a uniform $\mathcal{O}(1/n)$
error on $[0,1]^d$ with a sparsely connected ReLU MLP with the same H\"{o}lder
exponent $\alpha$ and coefficient $L$, of width $\mathcal{O}(dn^{d/\alpha})$,
depth $\mathcal{O}(\log(d))$, with $\mathcal{O}(dn^{d/\alpha})$ nonzero
parameters, and whose weights and biases take values in $\{0,\pm 1/2\}$ except
in the first and last layers which instead have magnitude at-most $n$. Further,
our class of MLPs achieves a near-optimal sample complexity of
$\mathcal{O}(\log(N)/\sqrt{N})$ when given $N$ i.i.d. normalized sub-Gaussian
training samples. We achieve this through a new construction that perfectly
fits together linear pieces using Kuhn triangulations, along with a new proof
technique which shows that our construction preserves the regularity of not
only the H\"{o}lder functions, but also any uniformly continuous function. Our
results imply that neural networks can solve the McShane extension problem on
suitable finite sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages main body, 40 pages proofs, 10 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Split-Merge: A Difference-based Approach for Dominant Eigenvalue Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhi Liu, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computation of the dominant eigenvector of symmetric positive
semidefinite matrices is a cornerstone operation in numerous
optimization-driven applications. Traditional methods, typically based on the
\textit{Quotient} formulation, often suffer from challenges related to
computational efficiency and reliance on prior spectral knowledge. In this
work, we leverage the alternative \textit{Difference} formulation to
reinterpret the classical power method as a first-order optimization algorithm.
This perspective allows for a novel convergence analysis and facilitates the
development of accelerated variants with larger step-sizes, achieving faster
convergence without additional computational cost. Building on this insight, we
introduce a generalized family of Difference-based methods, with the power
method as a special case. Within this family, we propose Split-Merge, an
algorithm that attains accelerated convergence without requiring spectral
knowledge and operates solely via matrix-vector products. Extensive experiments
on both synthetic and real-world datasets demonstrate that Split-Merge
consistently outperforms state-of-the-art methods in both efficiency and
scalability. In particular, it achieves more than a $\boldsymbol{10\times}$
speedup over the classical power method, underscoring its practical
effectiveness for large-scale problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.00851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.00851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongjie Si, Zhiyi Shi, Xuehui Wang, Yichen Xiao, Xiaokang Yang, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting pre-trained foundation models for diverse downstream tasks is a core
practice in artificial intelligence. However, the wide range of tasks and high
computational costs make full fine-tuning impractical. To overcome this,
parameter-efficient fine-tuning (PEFT) methods like LoRA have emerged and are
becoming a growing research focus. Despite the success of these methods, they
are primarily designed for linear layers, focusing on two-dimensional matrices
while largely ignoring higher-dimensional parameter spaces like convolutional
kernels. Moreover, directly applying these methods to higher-dimensional
parameter spaces often disrupts their structural relationships. Given the rapid
advancements in matrix-based PEFT methods, rather than designing a specialized
strategy, we propose a generalization that extends matrix-based PEFT methods to
higher-dimensional parameter spaces without compromising their structural
properties. Specifically, we treat parameters as elements of a Lie group, with
updates modeled as perturbations in the corresponding Lie algebra. These
perturbations are mapped back to the Lie group through the exponential map,
ensuring smooth, consistent updates that preserve the inherent structure of the
parameter space. Extensive experiments on computer vision and natural language
processing validate the effectiveness and versatility of our approach,
demonstrating clear improvements over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 ICCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable quantum regression algorithm with encoded data structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03334v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03334v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. -C. Joseph Wang, F. Perkkola, I. Salmenperä, A. Meijer-van de Griend, J. K. Nurminen, R. S. Bennink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid variational quantum algorithms (VQAs) are promising for solving
practical problems such as combinatorial optimization, quantum chemistry
simulation, quantum machine learning, and quantum error correction on noisy
quantum computers. However, with typical random ansatz or quantum alternating
operator ansatz, derived variational quantum algorithms become a black box that
cannot be trusted for model interpretation, not to mention deploying as
applications in informing critical decisions: the results of these variational
parameters are just rotational angles for the quantum gates and have nothing to
do with interpretable values that a model can provide directly. In this paper,
we construct the first interpretable quantum regression algorithm, in which the
quantum state exactly encodes the classical data table and the variational
parameters correspond directly to the regression coefficients, which are real
numbers by construction, providing a high degree of model interpretability and
minimal cost to optimize due to the right expressiveness. We also take
advantage of the encoded data structure to reduce the time complexity of
computing the regression map. To shorten the circuit depth for nonlinear
regression, our algorithm can be extended by building nonlinear features by
classical preprocessing as the independent encoded column vectors. Even though
the realization of compressed encoding in superconducting qubits has been
achieved by the less noisy compressed encoding recently by the authors, we
envision potential quantum utilities with multi-qubit gates implemented in
neutral cold atoms and ions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extremely Simple Streaming Forest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08483v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08483v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyin Xu, Jayanta Dey, Sambit Panda, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision forests, including random forests and gradient boosting trees,
remain the leading machine learning methods for many real-world data problems,
especially on tabular data. However, most of the current implementations only
operate in batch mode, and therefore cannot incrementally update when more data
arrive. Several previous works developed streaming trees and ensembles to
overcome this limitation. Nonetheless, we found that those state-of-the-art
algorithms suffer from a number of drawbacks, including low accuracy on some
problems and high memory usage on others. We therefore developed an extremely
simple extension of decision trees: given new data, simply update existing
trees by continuing to grow them, and replace some old trees with new ones to
control the total number of trees. In a benchmark suite containing 72
classification problems (the OpenML-CC18 data suite), we illustrate that our
approach, $\textit{Extremely Simple Streaming Forest}$ (XForest), does not
suffer from either of the aforementioned limitations. On those datasets, we
also demonstrate that our approach often performs as well as, and sometimes
even better than, conventional batch decision forest algorithms. With a
$\textit{zero-added-node}$ approach, XForest-Zero, we also further extend
existing splits to new tasks, and this very efficient method only requires
inference time. Thus, XForests establish a simple standard for streaming trees
and forests that could readily be applied to many real-world problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The Fourth Conference on Lifelong Learning Agents -
  CoLLAs 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine learning of microstructure--property relationships in materials
  leveraging microstructure representation from foundational vision
  <span class="highlight-title">transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheila E. Whitman, Marat I. Latypov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning of microstructure--property relationships from data is an
emerging approach in computational materials science. Most existing machine
learning efforts focus on the development of task-specific models for each
microstructure--property relationship. We propose utilizing pre-trained
foundational vision transformers for the extraction of task-agnostic
microstructure features and subsequent light-weight machine learning of a
microstructure-dependent property. We demonstrate our approach with pre-trained
state-of-the-art vision transformers (CLIP, DINOv2, SAM) in two case studies on
machine-learning: (i) elastic modulus of two-phase microstructures based on
simulations data; and (ii) Vicker's hardness of Ni-base and Co-base superalloys
based on experimental data published in literature. Our results show the
potential of foundational vision transformers for robust microstructure
representation and efficient machine learning of microstructure--property
relationships without the need for expensive task-specific training or
fine-tuning of bespoke deep learning models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-25T00:00:00Z">2025-06-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering RAG Systems for Real-World Applications: Design,
  Development, and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Toufique Hasan, Muhammad Waseem, Kai-Kristian Kemell, Ayman Asad Khan, Mika Saari, Pekka Abrahamsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper to the 51st Euromicro Conference on Software
  Engineering and Advanced Applications (SEAA 2025). 9 pages, 4 figures. This
  is the preprint version and not the final camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Two-Stage Counterfactual Learning to Rank <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Gupta, Yiming Liao, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual learning to rank (CLTR) aims to learn a ranking policy from
user interactions while correcting for the inherent biases in interaction data,
such as position bias. Existing CLTR methods assume a single ranking policy
that selects top-K ranking from the entire document candidate set. In
real-world applications, the candidate document set is on the order of
millions, making a single-stage ranking policy impractical. In order to scale
to millions of documents, real-world ranking systems are designed in a
two-stage fashion, with a candidate generator followed by a ranker. The
existing CLTR method for a two-stage offline ranking system only considers the
top-1 ranking set-up and only focuses on training the candidate generator, with
the ranker fixed. A CLTR method for training both the ranker and candidate
generator jointly is missing from the existing literature. In this paper, we
propose a two-stage CLTR estimator that considers the interaction between the
two stages and estimates the joint value of the two policies offline. In
addition, we propose a novel joint optimization method to train the candidate
and ranker policies, respectively. To the best of our knowledge, we are the
first to propose a CLTR estimator and learning method for two-stage ranking.
Experimental results on a semi-synthetic benchmark demonstrate the
effectiveness of the proposed joint CLTR method over baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICTIR 2025 (co-located with SIGIR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval
  from Complex Structured Academic Papers <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Deng, Xi Wang, Mark Stevenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ACM SIGIR Conference on Innovative Concepts and Theories
  in Information Retrieval (ICTIR'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced
  Retrieval-Augmented Generation in <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Tourani, Fatemeh Nazary, Yashar Deldjoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unidentified and Confounded? Understanding Two-Tower Models for Unbiased
  Learning to Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hager, Onno Zoeter, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCode: Updating Code API Knowledge with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Aware Diverse Reranking for <span class="highlight-title">Cross</span>-Source Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigong Zhou, Ning Ding, Xiaochuan Fan, Yue Shang, Yiming Qiu, Jingwei Zhuo, Zhiwei Ge, Songlin Wang, Lin Liu, Sulong Xu, Han Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval, which retrieves semantically matched items given a
textual query, has been an essential component to enhance system effectiveness
in e-commerce search. In this paper, we study the multimodal retrieval problem,
where the visual information (e.g, image) of item is leveraged as supplementary
of textual information to enrich item representation and further improve
retrieval performance. Though learning from cross-modality data has been
studied extensively in tasks such as visual question answering or media
summarization, multimodal retrieval remains a non-trivial and unsolved problem
especially in the asymmetric scenario where the query is unimodal while the
item is multimodal. In this paper, we propose a novel model named SMAR, which
stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the
problem of modality fusion and alignment in this kind of asymmetric scenario.
Extensive experimental results on an industrial dataset show that the proposed
model outperforms baseline models significantly in retrieval accuracy. We have
open sourced our industrial dataset for the sake of reproducibility and future
research works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in sigir2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Producer-Fairness in Sequential Bundle <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Rio, Marta Soare, Sihem Amer-Yahia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Literature <span class="highlight-title">Review</span> on Simulation in Conversational Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Xin Zhao, Jinze Chen, Junpeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRSs) have garnered attention as a novel
approach to delivering personalized recommendations through multi-turn
dialogues. This review developed a taxonomy framework to systematically
categorize relevant publications into four groups: dataset construction,
algorithm design, system evaluation, and empirical studies, providing a
comprehensive analysis of simulation methods in CRSs research. Our analysis
reveals that simulation methods play a key role in tackling CRSs' main
challenges. For example, LLM-based simulation methods have been used to create
conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.
Despite several challenges, such as dataset bias, the limited output
flexibility of LLM-based simulations, and the gap between text semantic space
and behavioral semantics, persist due to the complexity in Human-Computer
Interaction (HCI) of CRSs, simulation methods hold significant potential for
advancing CRS research. This review offers a thorough summary of the current
research landscape in this domain and identifies promising directions for
future inquiry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figures, accepted as a poster for CSWIM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Irec: A Metacognitive Scaffolding for Self-Regulated Learning through
  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefei Hou, Xizhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 1 of a work in progress. Finalized system flowcharts, a
  public GitHub repository with the source code, and a full reproducibility
  package detailing the prompts, models, and testing guidelines will be
  provided in v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections
  over 11 Years of ICLR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Zhao Song, Jiahao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Information Retrieval for Open World with Edit Distance Weak
  Supervision <span class="chip">ICDE'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KMA Solaiman, Bharat Bhargava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICDE'24. An earlier version of this paper appeared on
  TechRxiv: https://www.techrxiv.org/doi/full/10.36227/techrxiv.21990284.v1,
  uploaded on February 05, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Sentiment Analytics: Unlocking Business Value in the
  E-Commerce Landscape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianye Wu, Chengxuan Xia, Sixuan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of e-commerce has led to an overwhelming volume of customer
feedback, from product reviews to service interactions. Extracting meaningful
insights from this data is crucial for businesses aiming to improve customer
satisfaction and optimize decision-making. This paper presents an AI-driven
sentiment analysis system designed specifically for e-commerce applications,
balancing accuracy with interpretability. Our approach integrates traditional
machine learning techniques with modern deep learning models, allowing for a
more nuanced understanding of customer sentiment while ensuring transparency in
decision-making. Experimental results show that our system outperforms standard
sentiment analysis methods, achieving an accuracy of 89.7% on diverse,
large-scale datasets. Beyond technical performance, real-world implementation
across multiple e-commerce platforms demonstrates tangible improvements in
customer engagement and operational efficiency. This study highlights both the
potential and the challenges of applying AI to sentiment analysis in a
commercial setting, offering insights into practical deployment strategies and
areas for future refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterFormer: Effective Heterogeneous Interaction Learning for
  Click-Through Rate Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09852v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09852v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Zeng, Xiaolong Liu, Mengyue Hang, Xiaoyi Liu, Qinghai Zhou, Chaofei Yang, Yiqun Liu, Yichen Ruan, Laming Chen, Yuxin Chen, Yujia Hao, Jiaqi Xu, Jade Nie, Xi Liu, Buyun Zhang, Wei Wen, Siyang Yuan, Hang Yin, Xin Zhang, Kai Wang, Wen-Yen Chen, Yiping Han, Huayu Li, Chunzhi Yang, Bo Long, Philip S. Yu, Hanghang Tong, Jiyan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction, which predicts the probability of a user
clicking an ad, is a fundamental task in recommender systems. The emergence of
heterogeneous information, such as user profile and behavior sequences, depicts
user interests from different aspects. A mutually beneficial integration of
heterogeneous information is the cornerstone towards the success of CTR
prediction. However, most of the existing methods suffer from two fundamental
limitations, including (1) insufficient inter-mode interaction due to the
unidirectional information flow between modes, and (2) aggressive information
aggregation caused by early summarization, resulting in excessive information
loss. To address the above limitations, we propose a novel module named
InterFormer to learn heterogeneous information interaction in an interleaving
style. To achieve better interaction learning, InterFormer enables
bidirectional information flow for mutually beneficial learning across
different modes. To avoid aggressive information aggregation, we retain
complete information in each data mode and use a separate bridging arch for
effective information selection and summarization. Our proposed InterFormer
achieves state-of-the-art performance on three public datasets and a
large-scale industrial dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forgetful by Design? A Critical Audit of YouTube's Search API for
  Academic Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard Rieder, Adrian Padilla, Oscar Coromina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper critically audits the search endpoint of YouTube's Data API (v3),
a common tool for academic research. Through systematic weekly searches over
six months using eleven queries, we identify major limitations regarding
completeness, representativeness, consistency, and bias. Our findings reveal
substantial differences between ranking parameters like relevance and date in
terms of video recall and precision, with relevance often retrieving numerous
off-topic videos. We also find severe temporal decay, as the number of findable
videos for a specific period dramatically decreases after just 20-60 days from
the publication date, potentially hampering many different research designs.
Furthermore, search results lack consistency, with identical queries yielding
different video sets over time, compromising replicability. A case study on the
European Parliament elections highlights how these issues impact research
outcomes. While the paper offers several mitigation strategies, it concludes
that the API's search function, potentially prioritizing "freshness" over
comprehensive retrieval, is not adequate for robust academic research,
especially concerning Digital Services Act requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 tables and 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Recommender Model <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as Generative Adversarial Networks (GANs) and
Variational Auto-Encoders (VAEs) are widely utilized to model the generative
process of user interactions. However, these generative models suffer from
intrinsic limitations such as the instability of GANs and the restricted
representation ability of VAEs. Such limitations hinder the accurate modeling
of the complex user interaction generation procedure, such as noisy
interactions caused by various interference factors. In light of the impressive
advantages of Diffusion Models (DMs) over traditional generative models in
image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec)
to learn the generative process in a denoising manner. To retain personalized
information in user interactions, DiffRec reduces the added noises and avoids
corrupting users' interactions into pure noises like in image synthesis. In
addition, we extend traditional DMs to tackle the unique challenges in
practical recommender systems: high resource costs for large-scale item
prediction and temporal shifts of user preference. To this end, we propose two
extensions of DiffRec: L-DiffRec clusters items for dimension compression and
conducts the diffusion processes in the latent space; and T-DiffRec reweights
user interactions based on the interaction timestamps to encode temporal
information. We conduct extensive experiments on three datasets under multiple
settings (e.g. clean training, noisy training, and temporal training). The
empirical results and in-depth analysis validate the superiority of DiffRec
with two extensions over competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, accepted for publication in SIGIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Channel Multiplex Graph Neural Networks for <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11624v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11624v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interactive relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant challenges: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations within behavior patterns on the target relation in recommender system
scenarios. In this work, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interactive relations, and includes a relation chain representation
learner and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our DCMGNN
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06% and 12.15% on average across all datasets in terms of
Recall@10 and NDCG@10, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping the Evolution of Research Contributions using KnoVo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajratul Y. Rubaiat, Syed N. Sakib, Hasan M. Jamil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Reliable Adverse event Profiles for Health through Automated
  Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srikar Reddy Gadusu, Larry Callahan, Samir Lababidi, Arunasri Nishtala, Sophia Healey, Hande McGinty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical and Accurate Local Edge Differentially Private Graph
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranay Mundra, Charalampos Papamanthou, Julian Shun, Quanquan C. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Codicology to Code: A Comparative Study of <span class="highlight-title">Transformer</span> and
  YOLO-based Detectors for Layout Analysis in Historical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio Torres Aguilar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Piecewise Linear Approximation in Learned Index Structures: Theoretical
  and Empirical Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayong Qin, Xianyu Zhu, Qiyu Liu, Guangyi Zhang, Zhigang Cai, Jianwei Liao, Sha Hu, Jingshu Peng, Yingxia Shao, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Biases in Record Matching Through Scores Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hossein Moslemi, Mostafa Milani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Record matching is the task of identifying records that refer to the same
real-world entity across datasets. While most existing models optimize for
accuracy, fairness has become an important concern due to the potential for
unequal outcomes across demographic groups. Prior work typically focuses on
binary outcomes evaluated at fixed decision thresholds. However, such
evaluations can miss biases in matching scores--biases that persist across
thresholds and affect downstream tasks. We propose a threshold-independent
framework for measuring and reducing score bias, defined as disparities in the
distribution of matching scores across groups. We show that several
state-of-the-art matching methods exhibit substantial score bias, even when
appearing fair under standard threshold-based metrics. To address this, we
introduce two post-processing score calibration algorithms. The first, calib,
aligns group-wise score distributions using the Wasserstein barycenter,
targeting demographic parity. The second, ccalib, conditions on predicted
labels to further reduce label-dependent biases, such as equal opportunity.
Both methods are model-agnostic and require no access to model training data.
calib also offers theoretical guarantees, ensuring reduced bias with minimal
deviation from original scores. Experiments across real-world datasets and
matching models confirm that calib and ccalib substantially reduce score bias
while minimally impacting model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>: Graph Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel E. Coimbra, Lucie Svitáková, Alexandre P. Francisco, Luís Veiga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph databases have become essential tools for managing complex and
interconnected data, which is common in areas like social networks,
bioinformatics, and recommendation systems. Unlike traditional relational
databases, graph databases offer a more natural way to model and query
intricate relationships, making them particularly effective for applications
that demand flexibility and efficiency in handling interconnected data.
  Despite their increasing use, graph databases face notable challenges. One
significant issue is the irregular nature of graph data, often marked by
structural sparsity, such as in its adjacency matrix representation, which can
lead to inefficiencies in data read and write operations. Other obstacles
include the high computational demands of traversal-based queries, especially
within large-scale networks, and complexities in managing transactions in
distributed graph environments. Additionally, the reliance on traditional
centralized architectures limits the scalability of Online Transaction
Processing (OLTP), creating bottlenecks due to contention, CPU overhead, and
network bandwidth constraints.
  This paper presents a thorough survey of graph databases. It begins by
examining property models, query languages, and storage architectures,
outlining the foundational aspects that users and developers typically engage
with. Following this, it provides a detailed analysis of recent advancements in
graph database technologies, evaluating these in the context of key aspects
such as architecture, deployment, usage, and development, which collectively
define the capabilities of graph database solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping the Evolution of Research Contributions using KnoVo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajratul Y. Rubaiat, Syed N. Sakib, Hasan M. Jamil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omniwise: Predicting GPU Kernels Performance with <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex Model Transformations by Reinforcement Learning with Uncertain
  Human Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyanna Dagenais, Istvan David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ACM/IEEE MODELS'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired
  Multi-Stage Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calin Teodor Ioan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering RAG Systems for Real-World Applications: Design,
  Development, and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Toufique Hasan, Muhammad Waseem, Kai-Kristian Kemell, Ayman Asad Khan, Mika Saari, Pekka Abrahamsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper to the 51st Euromicro Conference on Software
  Engineering and Advanced Applications (SEAA 2025). 9 pages, 4 figures. This
  is the preprint version and not the final camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Reliable Adverse event Profiles for Health through Automated
  Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srikar Reddy Gadusu, Larry Callahan, Samir Lababidi, Arunasri Nishtala, Sophia Healey, Hande McGinty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FixCLR: Negative-Class Contrastive Learning for Semi-Supervised <span class="highlight-title">Domain</span>
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha Min Son, Shahbaz Rezaei, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Vision-Language Models to Select Trustworthy Super-Resolution
  Samples Generated by <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cansu Korkmaz, Ahmet Murat Tekalp, Zafer Dogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on
  Circuits and Systems for Video Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Hidden Violent Tendencies in <span class="highlight-title">LLM</span>s: A Demographic Analysis via
  Behavioral Vignettes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quintin Myers, Yanjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation
  (RAG) Framework for Financial Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Copy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Context-Aware Prompt <span class="highlight-title">Recommendation</span> for <span class="highlight-title">Domain</span>-Specific AI
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Tang, Haijun Zhai, Chaitanya Belwal, Vineeth Thayanithi, Philip Baumann, Yogesh K Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated
  LSTMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashwat Khandelwal, Jakoba Petri-Koenig, Thomas B. Preußer, Michaela Blott, Shreejith Shanker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 5 tables, Accepted for publication in IEEE
  FPL-2025 (https://2025.fpl.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU Kernel Scientist: An <span class="highlight-title">LLM</span>-Driven Framework for Iterative Kernel
  Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Andrews, Sam Witteveen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 page paper plus Appendices. Accepted to the ES-FoMo "Efficient
  Systems for Foundation Models" workshop at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poster: Enhancing GNN Robustness for Network Intrusion Detection via
  Agent-based Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghao Zhan, Huichi Zhou, Hamed Haddadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) show great promise for Network Intrusion
Detection Systems (NIDS), particularly in IoT environments, but suffer
performance degradation due to distribution drift and lack robustness against
realistic adversarial attacks. Current robustness evaluations often rely on
unrealistic synthetic perturbations and lack demonstrations on systematic
analysis of different kinds of adversarial attack, which encompass both
black-box and white-box scenarios. This work proposes a novel approach to
enhance GNN robustness and generalization by employing Large Language Models
(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These
agents scrutinize graph structures derived from network flow data, identifying
and potentially mitigating suspicious or adversarially perturbed elements
before GNN processing. Our experiments, using a framework designed for
realistic evaluation and testing with a variety of adversarial attacks
including a dataset collected from physical testbed experiments, demonstrate
that integrating LLM analysis can significantly improve the resilience of
GNN-based NIDS against challenges, showcasing the potential of LLM agent as a
complementary layer in intrusion detection architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Poster accepted at the 10th IEEE European Symposium on Security and
  Privacy (Euro S&P 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Ideation-Execution Gap: Execution Outcomes of <span class="highlight-title">LLM</span>-Generated versus
  Human Research Ideas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglei Si, Tatsunori Hashimoto, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper is 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Parameter Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius Bushnaq, Dan Braun, Lee Sharkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agile Management for Machine Learning: A Systematic Mapping Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Romao, Hugo Villamizar, Romeu Oliveira, Silvio Alonso, Marcos Kalinowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 51st Euromicro Conference Series on
  Software Engineering and Advanced Applications (SEAA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on
  Human Prosocial Behavior Toward Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingshu Li, Zicheng Zhu, Renwen Zhang, Yi-Chieh Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurusha Juneja, Alon Albalak, Wenyue Hua, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Scaling Techniques in Theoretical Physics -- A Comparison of
  Methods on the TPBench Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Gao, Tianyi Li, Yurii Kvasiuk, Sai Chaitanya Tadepalli, Maja Rudolph, Daniel J. H. Chung, Frederic Sala, Moritz Münchmeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Convolutions, Intrinsic Dimension, and <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kin Kwan Leung, Rasa Hosseinzadeh, Gabriel Loaiza-Ganem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Singapore Consensus on Global AI Safety Research Priorities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshua Bengio, Tegan Maharaj, Luke Ong, Stuart Russell, Dawn Song, Max Tegmark, Lan Xue, Ya-Qin Zhang, Stephen Casper, Wan Sie Lee, Sören Mindermann, Vanessa Wilfred, Vidhisha Balachandran, Fazl Barez, Michael Belinsky, Imane Bello, Malo Bourgon, Mark Brakel, Siméon Campos, Duncan Cass-Beggs, Jiahao Chen, Rumman Chowdhury, Kuan Chua Seah, Jeff Clune, Juntao Dai, Agnes Delaborde, Nouha Dziri, Francisco Eiras, Joshua Engels, Jinyu Fan, Adam Gleave, Noah Goodman, Fynn Heide, Dan Hendrycks, Cyrus Hodes, Bryan Low Kian Hsiang, Minlie Huang, Sami Jawhar, Wang Jingyu, Adam Tauman Kalai, Meindert Kamphuis, Mohan Kankanhalli, Subhash Kantamneni, Mathias Bonde Kirk, Thomas Kwa, Jeffrey Ladish, Kwok-Yan Lam, Wan Lee Sie, Taewhi Lee, Xiaojian Li, Jiajun Liu, Chaochao Lu, Yifan Mai, Richard Mallah, Julian Michael, Nick Moës, Simon Möller, Kihyuk Nam, Kwan Yee Ng, Mark Nitzberg, Besmira Nushi, Seán O hÉigeartaigh, Alejandro Ortega, Pierre Peigné, James Petrie, Benjamin Prud'Homme, Reihaneh Rabbany, Nayat Sanchez-Pi, Sarah Schwettmann, Buck Shlegeris, Saad Siddiqui, Aradhana Sinha, Martín Soto, Cheston Tan, Dong Ting, Robert Trager, Brian Tse, Anthony Tung K. H., Vanessa Wilfred, John Willes, Denise Wong, Wei Xu, Rongwu Xu, Yi Zeng, HongJiang Zhang, Djordje Žikelić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final report from the "2025 Singapore Conference on AI (SCAI)" held
  April 26: https://www.scai.gov.sg/2025/scai2025-report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Tree Sampling: Scalable inference-time alignment of <span class="highlight-title">diffusion</span>
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vineet Jain, Kusha Sareen, Mohammad Pedramfar, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inside you are many wolves: Using cognitive models to interpret value
  trade-offs in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia K. Murthy, Rosie Zhao, Jennifer Hu, Sham Kakade, Markus Wulfmeier, Peng Qian, Tomer Ullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Lupu, Timon Willi, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled representations of microscopy images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo Dapueto, Vito Paolo Pastore, Nicoletta Noceti, Francesca Odone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: International Joint Conference on Neural Networks
  (IJCNN 2025). Project page:
  https://github.com/JacopoDapueto/disentangled_microscopy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Community-Driven Agents for Machine Learning Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Li, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Define-ML: An Approach to Ideate Machine Learning-Enabled Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvio Alonso, Antonio Pedro Santos Alves, Lucas Romao, Hélio Lopes, Marcos Kalinowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 51st Euromicro Conference Series on
  Software Engineering and Advanced Applications (SEAA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Perrin, Sébastien Levilly, Huajun Sun, Harold Mouchère, Jean-Michel Serfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot
  Recordings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Shah, Rita Singh, Bhiksha Raj, Alexander Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages + 1 References</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Assistants to Enhance and Exploit the PETSc Knowledge Base 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CogGen: A Learner-Centered Generative AI Architecture for Intelligent
  Tutoring with Programming Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wengxi Li, Roy Pea, Nick Haber, Hari Subramonyam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning and Prompt Engineering of <span class="highlight-title">LLM</span>s, for the Creation of
  Multi-Agent AI for Addressing Sustainable Protein Production Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander D. Kalian, Jaewook Lee, Stefan P. Johannesson, Lennart Otte, Christer Hogstrand, Miao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI in the Writing Process: How Purposeful AI Support Fosters Student
  Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Momin N. Siddiqui, Roy Pea, Hari Subramonyam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquity of technologies like ChatGPT has raised concerns about their
impact on student writing, particularly regarding reduced learner agency and
superficial engagement with content. While standalone chat-based LLMs often
produce suboptimal writing outcomes, evidence suggests that purposefully
designed AI writing support tools can enhance the writing process. This paper
investigates how different AI support approaches affect writers' sense of
agency and depth of knowledge transformation. Through a randomized control
trial with 90 undergraduate students, we compare three conditions: (1) a
chat-based LLM writing assistant, (2) an integrated AI writing tool to support
diverse subprocesses, and (3) a standard writing interface (control). Our
findings demonstrate that, among AI-supported conditions, students using the
integrated AI writing tool exhibited greater agency over their writing process
and engaged in deeper knowledge transformation overall. These results suggest
that thoughtfully designed AI writing support targeting specific aspects of the
writing process can help students maintain ownership of their work while
facilitating improved engagement with content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Video Captioning using Graph-based Sentence Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwang Zhang, Dong Xu, Wanli Ouyang, Luping Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Representation Learning with Observational Grouping for CXR
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajat Rasal, Avinash Kori, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks
  on NIDS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrine Ennaji, Elhadj Benkhelifa, Luigi V. Mancini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks, wherein slight inputs are carefully crafted to mislead
intelligent models, have attracted increasing attention. However, a critical
gap persists between theoretical advancements and practical application,
particularly in structured data like network traffic, where interdependent
features complicate effective adversarial manipulations. Moreover, ambiguity in
current approaches restricts reproducibility and limits progress in this field.
Hence, existing defenses often fail to handle evolving adversarial attacks.
This paper proposes a novel approach for black-box adversarial attacks, that
addresses these limitations. Unlike prior work, which often assumes system
access or relies on repeated probing, our method strictly respect black-box
constraints, reducing interaction to avoid detection and better reflect
real-world scenarios. We present an adaptive feature selection strategy using
change-point detection and causality analysis to identify and target sensitive
features to perturbations. This lightweight design ensures low computational
cost and high deployability. Our comprehensive experiments show the attack's
effectiveness in evading detection with minimal interaction, enhancing its
adaptability and applicability in real-world scenarios. By advancing the
understanding of adversarial attacks in network traffic, this work lays a
foundation for developing robust defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided
  Sentence Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwang Zhang, Dong Xu, Wanli Ouyang, Chuanqi Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepQuark: deep-neural-network approach to multiquark bound states 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Lin Wu, Lu Meng, Shi-Lin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the first time, we implement the deep-neural-network-based variational
Monte Carlo approach for the multiquark bound states, whose complexity
surpasses that of electron or nucleon systems due to strong SU(3) color
interactions. We design a novel and high-efficiency architecture, DeepQuark, to
address the unique challenges in multiquark systems such as stronger
correlations, extra discrete quantum numbers, and intractable confinement
interaction. Our method demonstrates competitive performance with
state-of-the-art approaches, including diffusion Monte Carlo and Gaussian
expansion method, in the nucleon, doubly heavy tetraquark, and fully heavy
tetraquark systems. Notably, it outperforms existing calculations for
pentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we
successfully incorporate three-body flux-tube confinement interactions without
additional computational costs. In tetraquark systems, we consistently describe
hadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased
form of wave function ansatz. In the pentaquark sector, we obtain weakly bound
$\bar D^*\Xi_{cc}^*$ molecule $P_{cc\bar c}(5715)$ with $S=\frac{5}{2}$ and its
bottom partner $P_{bb\bar b}(15569)$. They can be viewed as the analogs of the
molecular $T_{cc}$. We recommend experimental search of $P_{cc\bar c}(5715)$ in
the D-wave $J/\psi \Lambda_c$ channel. DeepQuark holds great promise for
extension to larger multiquark systems, overcoming the computational barriers
in conventional methods. It also serves as a powerful framework for exploring
confining mechanism beyond two-body interactions in multiquark states, which
may offer valuable insights into nonperturbative QCD and general many-body
physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>-Driven Code Compliance Checking in Building
  Information Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Madireddy, Lu Gao, Zia Din, Kinam Kim, Ahmed Senouci, Zhe Han, Yunpeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pay Less Attention to Deceptive Artifacts: Robust Detection of
  Compressed Deepfakes on Online Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manyi Li, Renshuai Tao, Yufan Liu, Chuangchuang Tan, Haotong Qin, Bing Li, Yunchao Wei, Yao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Life Gives You Samples: The Benefits of Scaling up Inference
  Compute for Multilingual <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon
  Footprint of AI Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhen Huang, Kunming Zhang, Hanlong Liao, Kui Wu, Guoming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Case-based Reasoning Augmented <span class="highlight-title">Large Language Model</span> Framework for
  Decision Making in Realistic Safety-Critical Driving Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbin Gan, Minh-Son Dao, Koji Zettsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, under-review conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Industrial Energy Disaggregation with Digital Twin-generated Dataset and
  Efficient Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Internò, Andrea Castellani, Sebastian Schmitt, Fabio Stella, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages; The first three authors contribute to this work equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering Sentience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Demin, Taylor Webb, Eric Elmoznino, Hakwan Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCode: Updating Code API Knowledge with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth
  Modelling and Self-Organization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvatore Milite, Giulio Caravagna, Andrea Sottoriva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Influence as a Distributional Quantity <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Igor Shilov, Georgios Kaissis, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on The Impact of Memorization on Trustworthy Foundation
  Models (MemFM) @ ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Demonstration Selection for <span class="highlight-title">LLM</span>-based Tabular Data
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchu Han, Wolfgang Bruckner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Agentic System for Rare Disease Diagnosis with Traceable Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Off-Policy Evaluation and Learning for the Future under Non-Stationarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuhiro Shimizu, Kazuki Kawamura, Takanori Muroi, Yusuke Narita, Kei Tateno, Takuma Udagawa, Yuta Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SV-<span class="highlight-title">LLM</span>: An Agentic Approach for SoC Security Verification using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipayan Saha, Shams Tarek, Hasan Al Shaikh, Khan Thamid Hasan, Pavan Sai Nalluri, Md. Ajoad Hasan, Nashmin Alam, Jingbo Zhou, Sujan Kumar Saha, Mark Tehranipoor, Farimah Farahmandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the security of complex system-on-chips (SoCs) designs is a critical
imperative, yet traditional verification techniques struggle to keep pace due
to significant challenges in automation, scalability, comprehensiveness, and
adaptability. The advent of large language models (LLMs), with their remarkable
capabilities in natural language understanding, code generation, and advanced
reasoning, presents a new paradigm for tackling these issues. Moving beyond
monolithic models, an agentic approach allows for the creation of multi-agent
systems where specialized LLMs collaborate to solve complex problems more
effectively. Recognizing this opportunity, we introduce SV-LLM, a novel
multi-agent assistant system designed to automate and enhance SoC security
verification. By integrating specialized agents for tasks like verification
question answering, security asset identification, threat modeling, test plan
and property generation, vulnerability detection, and simulation-based bug
validation, SV-LLM streamlines the workflow. To optimize their performance in
these diverse tasks, agents leverage different learning paradigms, such as
in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The
system aims to reduce manual intervention, improve accuracy, and accelerate
security analysis, supporting proactive identification and mitigation of risks
early in the design cycle. We demonstrate its potential to transform hardware
security practices through illustrative case studies and experiments that
showcase its applicability and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Client Clustering Meets Knowledge Sharing: Enhancing Privacy and
  Robustness in Personalized Peer-to-Peer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Maheri, Denys Herasymuk, Hamed Haddadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GymPN: A Library for Decision-Making in Process Management Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Lo Bianco, Willem van Jaarsveld, Remco Dijkman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paladin-mini: A Compact and Efficient Grounding Model Excelling in
  Real-World Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dror Ivry, Oran Nahum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CARMA: Context-Aware Situational Grounding of Human-Robot Group
  Interactions by Combining Vision-Language Models with Object and Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joerg Deigmoeller, Stephan Hasler, Nakul Agarwal, Daniel Tanneberg, Anna Belardinelli, Reza Ghoddoosian, Chao Wang, Felix Ocker, Fan Zhang, Behzad Dariush, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CARMA, a system for situational grounding in human-robot group
interactions. Effective collaboration in such group settings requires
situational awareness based on a consistent representation of present persons
and objects coupled with an episodic abstraction of events regarding actors and
manipulated objects. This calls for a clear and consistent assignment of
instances, ensuring that robots correctly recognize and track actors, objects,
and their interactions over time. To achieve this, CARMA uniquely identifies
physical instances of such entities in the real world and organizes them into
grounded triplets of actors, objects, and actions.
  To validate our approach, we conducted three experiments, where multiple
humans and a robot interact: collaborative pouring, handovers, and sorting.
These scenarios allow the assessment of the system's capabilities as to role
distinction, multi-actor awareness, and consistent instance identification. Our
experiments demonstrate that the system can reliably generate accurate
actor-action-object triplets, providing a structured and robust foundation for
applications requiring spatiotemporal reasoning and situated decision-making in
collaborative settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Graph Learning via Spectral Bootstrapping and
  Laplacian-Based Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Bini, Stephane Marchand-Maillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LaplaceGNN is a novel graph learning framework that employs a
  bootstrapped teacher-student architecture. Its precomputed spectral
  augmentations and adversarial training enable robust performance,
  outperforming SOTA methods while scaling linearly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tabular Feature Discovery With Reasoning Type Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwon Han, Sungkyu Park, Seungeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A foundation model with multi-variate parallel attention to generate
  neuronal acti<span class="highlight-title">vit</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Carzaniga, Michael Hersche, Abu Sebastian, Kaspar Schindler, Abbas Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at
  https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
  dataset is available at
  https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DipSVD: Dual-importance Protected SVD for Efficient <span class="highlight-title">LLM</span> Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Hallucination for Self-supervised Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in International Journal of Computer Vision
  (IJCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left
  Ventricular Finite Element Modeling with Image Motion Consistency and
  Biomechanical Parameter Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Mu, Wei Xuan Chan, Choon Hwai Yap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based
  Mobile Agent via Task-Level Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Deep Learning Models for Crop Disease Detection:
  A Transfer Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saundarya Subramaniam, Shalini Majumdar, Shantanu Nadar, Kaustubh Kulkarni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond-Expert Performance with Limited Demonstrations: Efficient
  Imitation Learning with Double Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enterprise <span class="highlight-title">Large Language Model</span> Evaluation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liya Wang, David Yi, Damien Jose, John Passarelli, James Gao, Jordan Leventis, Kang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argumentative Ensembling for Robust Recourse under Model Multiplicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Jiang, Antonio Rago, Francesco Leofante, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2312.15097</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating and Customizing Robotic Arm Trajectories using Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrej Lúčny, Matilde Antonj, Carlo Mazzola, Hana Hornáčková, Igor Farkaš
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a neural network approach for generating and customizing the
trajectory of a robotic arm, that guarantees precision and repeatability. To
highlight the potential of this novel method, we describe the design and
implementation of the technique and show its application in an experimental
setting of cognitive robotics. In this scenario, the NICO robot was
characterized by the ability to point to specific points in space with precise
linear movements, increasing the predictability of the robotic action during
its interaction with humans. To achieve this goal, the neural network computes
the forward kinematics of the robot arm. By integrating it with a generator of
joint angles, another neural network was developed and trained on an artificial
dataset created from suitable start and end poses of the robotic arm. Through
the computation of angular velocities, the robot was characterized by its
ability to perform the movement, and the quality of its action was evaluated in
terms of shape and accuracy. Thanks to its broad applicability, our approach
successfully generates precise trajectories that could be customized in their
shape and adapted to different settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is released at
  https://github.com/andylucny/nico2/tree/main/generate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-series surrogates from energy consumers generated by machine
  learning approaches for long-term forecasting scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Gerhards, Nikita Popkov, Annekatrin König, Marcel Arpogaus, Bastian Schäfermeier, Leonie Riedl, Stephan Vogt, Philip Hehlert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching
  for Quantized <span class="highlight-title">Large Language Model</span>s <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Modeling by Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Cheng, Peter Clark, Kyle Richardson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating PDE discovery methods for multiscale modeling of biological
  signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andréa Ducos, Audrey Denizot, Thomas Guyet, Hugues Berry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological systems are non-linear, include unobserved variables and the
physical principles that govern their dynamics are partly unknown. This makes
the characterization of their behavior very challenging. Notably, their
activity occurs on multiple interdependent spatial and temporal scales that
require linking mechanisms across scales. To address the challenge of bridging
gaps between scales, we leverage partial differential equations (PDE)
discovery. PDE discovery suggests meso-scale dynamics characteristics from
micro-scale data. In this article, we present our framework combining
particle-based simulations and PDE discovery and conduct preliminary
experiments to assess equation discovery in controlled settings. We evaluate
five state-of-the-art PDE discovery methods on particle-based simulations of
calcium diffusion in astrocytes. The performances of the methods are evaluated
on both the form of the discovered equation and the forecasted temporal
variations of calcium concentration. Our results show that several methods
accurately recover the diffusion term, highlighting the potential of PDE
discovery for capturing macroscopic dynamics in biological systems from
microscopic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBKD: Distilled Federated Learning to Embrace Gerneralization and
  Personalization on Non-IID Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Zhao, Jinyuan He, Donglai Chen, Weijie Luo, Chong Xie, Ri Zhang, Yonghong Chen, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Papa Séga Wade, Mihai Andries, Ioannis Kanellos, Thierry Moudenc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted for presentation at EUSIPCO 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Large Language Model</span>s through Structured Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Dong, Hehe Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Directed Link Prediction using GNN with Local and Global Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhang, Xu Shen, Yu Xie, Ka-Chun Wong, Weidun Xie, Chengbin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspectives in Play: A Multi-Perspective Approach for More Inclusive
  NLP Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedetta Muscato, Lucia Passaro, Gizem Gezici, Fosca Giannotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Affective Priming Score: A Data-Driven Method to Detect Priming in
  Sequential Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Gutierrez Maestro, Hadi Banaee, Amy Loutfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Retrieve Examples in In-context Learning to Improve
  Conversational Emotion Recognition using <span class="highlight-title">Large Language Model</span>s? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengqi Wang, Tiantian Feng, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Attribution for <span class="highlight-title">Large Language Model</span>s: A Distribution Testing
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément L. Canonne, Yash Pote, Uddalok Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Alignment Degradation Learning for Pansharpening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzhe Zhao, Zhichang Guo, Yao Li, Fanghui Song, Boying Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COIN: Uncertainty-Guarding Selective Question Answering for Foundation
  Models with Provable Risk Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Wang, Jinhao Duan, Qingni Wang, Xiaofeng Zhu, Tianlong Chen, Xiaoshuang Shi, Kaidi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Valid Selection among Conformal Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Hegazy, Liviu Aolaritei, Michael I. Jordan, Aymeric Dieuleveut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series
  Prediction with <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengze Li, Yue Wang, Yangle Liu, Ming Huang, Dou Hong, Jieming Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do psychic cells generate consciousness? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mototaka Suzuki, Jaan Aru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advances in the past decades have begun to enable
neuroscientists to address fundamental questions about consciousness in an
unprecedented way. Here we review remarkable recent progress in our
understanding of cellular-level mechanisms of conscious processing in the
brain. Of particular interest are the cortical pyramidal neurons -- or "psychic
cells" called by Ram\'on y Cajal more than 100 years ago -- which have an
intriguing cellular mechanism that accounts for selective disruption of
feedback signaling in the brain upon anesthetic-induced loss of consciousness.
Importantly, a particular class of metabotropic receptors distributed over the
dendrites of pyramidal cells are highlighted as the key cellular mechanism.
After all, Cajal's instinct over a century ago may turn out to be correct -- we
may have just begun to understand whether and how psychic cells indeed generate
and control our consciousness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI and Agile Software Development: From Frustration to Success -- XP2025
  Workshop Summary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Herda, Victoria Pichler, Zheying Zhang, Pekka Abrahamsson, Geir K. Hanssen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Irec: A Metacognitive Scaffolding for Self-Regulated Learning through
  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefei Hou, Xizhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 1 of a work in progress. Finalized system flowcharts, a
  public GitHub repository with the source code, and a full reproducibility
  package detailing the prompts, models, and testing guidelines will be
  provided in v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep
  Neural Network Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Ghimire, Kilho Lee, Seong-heum Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAR: Erasing Concepts from Unified Autoregressive Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Fan, Shiyuan Zhang,  Baohunesitu, Zihang Guo, Huaiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 1 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Copilots for Reproducibility in Science: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Bibal, Steven N. Minton, Deborah Khider, Yolanda Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCRS: A Zero-Shot <span class="highlight-title">LLM</span>-as-a-Judge Framework for Comprehensive RAG
  Evaluation <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashiq Muhamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LLM4Eval @ SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and
  Vision <span class="highlight-title">Transformer</span> for Accurate Semantic Segmentation of CMRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Racheal Mukisa, Arvind K. Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization
  in AI-Generated Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Lin, Weixuan Peng, Bojia Zi, Yifeng Gao, Xianbiao Qi, Xingjun Ma, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 page,4 figures,2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in
  Agricultural Expert-Guided Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, Vikram S. Adve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages, 32 figures, 23 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Modular Multitask Reasoning Framework Integrating Spatio-temporal
  Models and <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kethmi Hirushini Hettige, Jiahao Ji, Cheng Long, Shili Xiang, Gao Cong, Jingyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Big Five Personality and AI Capability Effects in
  <span class="highlight-title">LLM</span>-Simulated Negotiation Dialogues <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myke C. Cohen, Zhe Su, Hsien-Te Kao, Daniel Nguyen, Spencer Lynch, Maarten Sap, Svitlana Volkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for KDD 2025 Workshop on Evaluation and Trustworthiness
  of Agentic and Generative AI Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Human-AI Coordination through Online Adversarial Training and
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.15457v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.15457v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paresh Chaudhary, Yancheng Liang, Daphne Chen, Simon S. Du, Natasha Jaques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is a promising method
that allows dynamic data generation and ensures that agents are robust. It
creates a feedback loop where the agent's performance influences the generation
of new adversarial data, which can be used immediately to train the agent.
However, adversarial training is difficult to apply in a cooperative task; how
can we train an adversarial cooperator? We propose a novel strategy that
combines a pretrained generative model to simulate valid cooperative agent
policies with adversarial training to maximize regret. We call our method GOAT:
Generative Online Adversarial Training. In this framework, the GOAT dynamically
searches the latent space of the generative model for coordination strategies
where the learning policy, the Cooperator agent, underperforms. GOAT enables
better generalization by exposing the Cooperator to various challenging
interaction scenarios. We maintain realistic coordination strategies by keeping
the generative model frozen, thus avoiding adversarial exploitation. We
evaluate GOAT with real human partners, and the results demonstrate state of
the art performance on the Overcooked benchmark, highlighting its effectiveness
in generalizing to diverse human behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A3 : an Analytical Low-Rank Approximation Framework for Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like pruning and quantization, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, KV cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and
mixed-rank assignments for enhanced performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Sentiment Analytics: Unlocking Business Value in the
  E-Commerce Landscape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianye Wu, Chengxuan Xia, Sixuan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of e-commerce has led to an overwhelming volume of customer
feedback, from product reviews to service interactions. Extracting meaningful
insights from this data is crucial for businesses aiming to improve customer
satisfaction and optimize decision-making. This paper presents an AI-driven
sentiment analysis system designed specifically for e-commerce applications,
balancing accuracy with interpretability. Our approach integrates traditional
machine learning techniques with modern deep learning models, allowing for a
more nuanced understanding of customer sentiment while ensuring transparency in
decision-making. Experimental results show that our system outperforms standard
sentiment analysis methods, achieving an accuracy of 89.7% on diverse,
large-scale datasets. Beyond technical performance, real-world implementation
across multiple e-commerce platforms demonstrates tangible improvements in
customer engagement and operational efficiency. This study highlights both the
potential and the challenges of applying AI to sentiment analysis in a
commercial setting, offering insights into practical deployment strategies and
areas for future refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06285v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06285v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaike Sa Teles Rocha Alves, Eduardo Pestana de Aguiar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models, despite their popularity, face challenges such as long
training times and a lack of interpretability. In contrast, fuzzy inference
systems offer a balance of accuracy and transparency. This paper addresses the
limitations of traditional Takagi-Sugeno-Kang fuzzy models by extending the
recently proposed New Takagi-Sugeno-Kang model to a new Mamdani-based
regressor. These models are data-driven, allowing users to define the number of
rules to balance accuracy and interpretability. To handle the complexity of
large datasets, this research integrates wrapper and ensemble techniques. A
Genetic Algorithm is used as a wrapper for feature selection, creating genetic
versions of the models. Furthermore, ensemble models, including the Random New
Mamdani Regressor, Random New Takagi-Sugeno-Kang, and Random Forest New
Takagi-Sugeno-Kang, are introduced to improve robustness. The proposed models
are validated on photovoltaic energy forecasting datasets, a critical
application due to the intermittent nature of solar power. Results demonstrate
that the genetic and ensemble fuzzy models, particularly the Genetic New
Takagi-Sugeno-Kang and Random Forest New Takagi-Sugeno-Kang, achieve superior
performance. They often outperform both traditional machine learning and deep
learning models while providing a simpler and more interpretable rule-based
structure. The models are available online in a library called nfisis
(https://pypi.org/project/nfisis/).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterFormer: Effective Heterogeneous Interaction Learning for
  Click-Through Rate Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09852v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09852v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Zeng, Xiaolong Liu, Mengyue Hang, Xiaoyi Liu, Qinghai Zhou, Chaofei Yang, Yiqun Liu, Yichen Ruan, Laming Chen, Yuxin Chen, Yujia Hao, Jiaqi Xu, Jade Nie, Xi Liu, Buyun Zhang, Wei Wen, Siyang Yuan, Hang Yin, Xin Zhang, Kai Wang, Wen-Yen Chen, Yiping Han, Huayu Li, Chunzhi Yang, Bo Long, Philip S. Yu, Hanghang Tong, Jiyan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction, which predicts the probability of a user
clicking an ad, is a fundamental task in recommender systems. The emergence of
heterogeneous information, such as user profile and behavior sequences, depicts
user interests from different aspects. A mutually beneficial integration of
heterogeneous information is the cornerstone towards the success of CTR
prediction. However, most of the existing methods suffer from two fundamental
limitations, including (1) insufficient inter-mode interaction due to the
unidirectional information flow between modes, and (2) aggressive information
aggregation caused by early summarization, resulting in excessive information
loss. To address the above limitations, we propose a novel module named
InterFormer to learn heterogeneous information interaction in an interleaving
style. To achieve better interaction learning, InterFormer enables
bidirectional information flow for mutually beneficial learning across
different modes. To avoid aggressive information aggregation, we retain
complete information in each data mode and use a separate bridging arch for
effective information selection and summarization. Our proposed InterFormer
achieves state-of-the-art performance on three public datasets and a
large-scale industrial dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIDA: Social Media Image Deepfake Detection, Localization and
  Explanation with Large Multimodal Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04292v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04292v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenglin Huang, Jinwei Hu, Xiangtai Li, Yiwei He, Xingyu Zhao, Bei Peng, Baoyuan Wu, Xiaowei Huang, Guangliang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of generative models in creating highly realistic
images poses substantial risks for misinformation dissemination. For instance,
a synthetic image, when shared on social media, can mislead extensive audiences
and erode trust in digital content, resulting in severe repercussions. Despite
some progress, academia has not yet created a large and diversified deepfake
detection dataset for social media, nor has it devised an effective solution to
address this issue. In this paper, we introduce the Social media Image
Detection dataSet (SID-Set), which offers three key advantages: (1) extensive
volume, featuring 300K AI-generated/tampered and authentic images with
comprehensive annotations, (2) broad diversity, encompassing fully synthetic
and tampered images across various classes, and (3) elevated realism, with
images that are predominantly indistinguishable from genuine ones through mere
visual inspection. Furthermore, leveraging the exceptional capabilities of
large multimodal models, we propose a new image deepfake detection,
localization, and explanation framework, named SIDA (Social media Image
Detection, localization, and explanation Assistant). SIDA not only discerns the
authenticity of images, but also delineates tampered regions through mask
prediction and provides textual explanations of the model's judgment criteria.
Compared with state-of-the-art deepfake detection models on SID-Set and other
benchmarks, extensive experiments demonstrate that SIDA achieves superior
performance among diversified settings. The code, model, and dataset will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version revises and corrects the metric calculations in the
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-TIG: Temporal Consistency-Aware Zero-Shot Illumination-Guided
  Low-light Video Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yini Li, Nantheera Anantrasirichai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light and underwater videos suffer from poor visibility, low contrast,
and high noise, necessitating enhancements in visual quality. However, existing
approaches typically rely on paired ground truth, which limits their
practicality and often fails to maintain temporal consistency. To overcome
these obstacles, this paper introduces a novel zero-shot learning approach
named Zero-TIG, leveraging the Retinex theory and optical flow techniques. The
proposed network consists of an enhancement module and a temporal feedback
module. The enhancement module comprises three subnetworks: low-light image
denoising, illumination estimation, and reflection denoising. The temporal
enhancement module ensures temporal consistency by incorporating histogram
equalization, optical flow computation, and image warping to align the enhanced
previous frame with the current frame, thereby maintaining continuity.
Additionally, we address color distortion in underwater data by adaptively
balancing RGB channels. The experimental results demonstrate that our method
achieves low-light video enhancement without the need for paired training data,
making it a promising and applicable method for real-world scenario
enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxa<span class="highlight-title">Diffusion</span>: Progressively Trained <span class="highlight-title">Diffusion</span> Model for Fine-Grained
  Species Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Karimi Monsefi, Mridul Khurana, Rajiv Ramnath, Anuj Karpatne, Wei-Lun Chao, Cheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose TaxaDiffusion, a taxonomy-informed training framework for
diffusion models to generate fine-grained animal images with high morphological
and identity accuracy. Unlike standard approaches that treat each species as an
independent category, TaxaDiffusion incorporates domain knowledge that many
species exhibit strong visual similarities, with distinctions often residing in
subtle variations of shape, pattern, and color. To exploit these relationships,
TaxaDiffusion progressively trains conditioned diffusion models across
different taxonomic levels -- starting from broad classifications such as Class
and Order, refining through Family and Genus, and ultimately distinguishing at
the Species level. This hierarchical learning strategy first captures
coarse-grained morphological traits shared by species with common ancestors,
facilitating knowledge transfer before refining fine-grained differences for
species-level distinction. As a result, TaxaDiffusion enables accurate
generation even with limited training samples per species. Extensive
experiments on three fine-grained animal datasets demonstrate that outperforms
existing approaches, achieving superior fidelity in fine-grained animal image
generation. Project page: https://amink8.github.io/TaxaDiffusion/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced computer vision for extracting georeferenced vehicle
  trajectories from drone imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02136v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02136v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Fonod, Haechan Cho, Hwasoo Yeo, Nikolas Geroliminis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework for extracting georeferenced vehicle
trajectories from high-altitude drone imagery, addressing key challenges in
urban traffic monitoring and the limitations of traditional ground-based
systems. Our approach integrates several novel contributions, including a
tailored object detector optimized for high-altitude bird's-eye view
perspectives, a unique track stabilization method that uses detected vehicle
bounding boxes as exclusion masks during image registration, and an orthophoto
and master frame-based georeferencing strategy that enhances consistent
alignment across multiple drone viewpoints. Additionally, our framework
features robust vehicle dimension estimation and detailed road segmentation,
enabling comprehensive traffic analysis. Conducted in the Songdo International
Business District, South Korea, the study utilized a multi-drone experiment
covering 20 intersections, capturing approximately 12TB of 4K video data over
four days. The framework produced two high-quality datasets: the Songdo Traffic
dataset, comprising approximately 700,000 unique vehicle trajectories, and the
Songdo Vision dataset, containing over 5,000 human-annotated images with about
300,000 vehicle instances in four classes. Comparisons with high-precision
sensor data from an instrumented probe vehicle highlight the accuracy and
consistency of our extraction pipeline in dense urban environments. The public
release of Songdo Traffic and Songdo Vision, and the complete source code for
the extraction pipeline, establishes new benchmarks in data quality,
reproducibility, and scalability in traffic research. Results demonstrate the
potential of integrating drone technology with advanced computer vision for
precise and cost-effective urban traffic monitoring, providing valuable
resources for developing intelligent transportation systems and enhancing
traffic management strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing higher-order neural representations of uncertainty with the
  Noise Estimation through Reinforcement-based <span class="highlight-title">Diffusion</span> (NERD) model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojjat Azimi Asrari, Megan A. K. Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies often aim to reveal ``first-order" representations (FORs), which
encode aspects of an observer's environment, such as contents or structure. A
less-common target is ``higher-order" representations (HORs), which are
``about" FORs -- e.g., their strength or uncertainty -- and which may
contribute to learning. HORs about uncertainty are unlikely to be direct
``read-outs" of FOR characteristics, instead reflecting noisy estimation
processes incorporating prior expectations about uncertainty, but how the brain
represents such expected uncertainty distributions remains largely unexplored.
Here, we study ``noise expectation" HORs using neural data from a task which
may require the brain to learn about its own noise: decoded neurofeedback,
wherein human subjects learn to volitionally produce target neural patterns. We
develop and apply a Noise Estimation through Reinforcement-based Diffusion
(NERD) model to characterize how brains may undertake this process, and show
that NERD offers high explanatory power for human behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 7 figures, 12 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GASP: Efficient Black-Box Generation of Adversarial Suffixes for
  Jailbreaking <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Advik Raj Basani, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have shown impressive capabilities across various natural language
processing tasks, yet remain vulnerable to input prompts, known as jailbreak
attacks, carefully designed to bypass safety guardrails and elicit harmful
responses. Traditional methods rely on manual heuristics but suffer from
limited generalizability. Despite being automatic, optimization-based attacks
often produce unnatural prompts that can be easily detected by safety filters
or require high computational costs due to discrete token optimization. In this
paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel
automated framework that can efficiently generate human-readable jailbreak
prompts in a fully black-box setting. In particular, GASP leverages latent
Bayesian optimization to craft adversarial suffixes by efficiently exploring
continuous latent embedding spaces, gradually optimizing the suffix prompter to
improve attack efficacy while balancing prompt coherence via a targeted
iterative refinement procedure. Through comprehensive experiments, we show that
GASP can produce natural adversarial prompts, significantly improving jailbreak
success over baselines, reducing training times, and accelerating inference
speed, thus making it an efficient and scalable solution for red-teaming LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 8 tables, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs.
  No-Regret Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Easley, Yoav Kolumbus, Eva Tardos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the performance of heterogeneous learning agents in asset markets
with stochastic payoffs. Our main focus is on comparing Bayesian learners and
no-regret learners who compete in markets and identifying the conditions under
which each approach is more effective. Surprisingly, we find that low regret is
not sufficient for survival: an agent can have regret as low as $O(\log T)$ but
still vanish when competing against a Bayesian with a finite prior and any
positive prior probability on the correct model. On the other hand, we show
that Bayesian learning is fragile, while no-regret learning requires less
knowledge of the environment and is therefore more robust. Motivated by the
strengths and weaknesses of both approaches, we propose a balanced strategy for
utilizing Bayesian updates that improves robustness and adaptability to
distribution shifts, providing a step toward a best-of-both-worlds learning
approach. The method is general, efficient, and easy to implement. Finally, we
formally establish the relationship between the notions of survival and market
dominance studied in economics and the framework of regret minimization, thus
bridging these theories. More broadly, our work contributes to the
understanding of dynamics with heterogeneous types of learning agents and their
impact on markets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Learning in Markets, Heterogeneous Agents, Regret and Survival,
  Bayesian Learning, No-Regret Learning, Portfolio Optimization, Kelly Rule,
  Distribution Shifts, Robust Bayesian Updates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniGen2: Exploration to Advanced Multimodal Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Models Through a Global Lens: Are They Culturally Inclusive? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Bayramli, Ayhan Suleymanzade, Na Min An, Huzama Ahmad, Eunsu Kim, Junyeong Park, James Thorne, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently enabled the creation of visually
compelling, detailed images from textual prompts. However, their ability to
accurately represent various cultural nuances remains an open question. In our
work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion
models whether they can generate culturally specific images spanning ten
countries. We show that these models often fail to generate cultural artifacts
in architecture, clothing, and food, especially for underrepresented country
regions, by conducting a fine-grained analysis of different similarity aspects,
revealing significant disparities in cultural relevance, description fidelity,
and realism compared to real-world reference images. With the collected human
evaluations, we develop a neural-based image-image similarity metric, namely,
CultDiff-S, to predict human judgment on real and generated images with
cultural artifacts. Our work highlights the need for more inclusive generative
AI systems and equitable dataset representation over a wide range of cultures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 17 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recycling the Web: A Method to Enhance Pre-training Data Quality and
  Quantity for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Concept Bottleneck Models Respect Localities? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01259v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01259v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based explainability methods use human-understandable intermediaries
to produce explanations for machine learning models. These methods assume
concept predictions can help understand a model's internal reasoning. In this
work, we assess the degree to which such an assumption is true by analyzing
whether concept predictors leverage "relevant" features to make predictions, a
term we call locality. Concept-based models that fail to respect localities
also fail to be explainable because concept predictions are based on spurious
features, making the interpretation of the concept predictions vacuous. To
assess whether concept-based models respect localities, we construct and use
three metrics to characterize when models respect localities, complementing our
analysis with theoretical results. Each of our metrics captures a different
notion of perturbation and assess whether perturbing "irrelevant" features
impacts the predictions made by a concept predictors. We find that many
concept-based models used in practice fail to respect localities because
concept predictors cannot always clearly distinguish distinct concepts. Based
on these findings, we propose suggestions for alleviating this issue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From $\mathcal{O}(n^{2})$ to $\mathcal{O}(n)$ Parameters: Quantum
  Self-Attention in Vision <span class="highlight-title">Transformer</span>s for Biomedical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Boucher, John Whittle, Evangelos B. Mazomenos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate that quantum vision transformers (QViTs), vision transformers
(ViTs) with self-attention (SA) mechanisms replaced by quantum self-attention
(QSA) mechanisms, can match state-of-the-art (SOTA) biomedical image
classifiers while using 99.99% fewer parameters. QSAs are produced by replacing
linear SA layers with parameterised quantum neural networks (QNNs), producing a
QSA mechanism and reducing parameter scaling from $\mathcal{O}(n^2)$ to
$\mathcal{O}(n)$. On RetinaMNIST, our ultra parameter-efficient QViT
outperforms 13/14 SOTA methods including CNNs and ViTs, achieving 56.5%
accuracy, just 0.88% below the top MedMamba model while using 99.99% fewer
parameters (1K vs 14.5M) and 89% fewer GFLOPs. We present the first
investigation of knowledge distillation (KD) from classical to quantum vision
transformers in biomedical image classification, showing that QViTs maintain
comparable performance to classical ViTs across eight diverse datasets spanning
multiple modalities, with improved QSA parameter-efficiency. Our higher-qubit
architecture benefitted more from KD pre-training, suggesting a scaling
relationship between QSA parameters and KD effectiveness. These findings
establish QSA as a practical architectural choice toward parameter-efficient
biomedical image analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for EMA4MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin D. Killeen, Liam J. Wang, Blanca Inigo, Han Zhang, Mehran Armand, Russell H. Taylor, Greg Osgood, Mathias Unberath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language promptable X-ray image segmentation would enable greater flexibility
for human-in-the-loop workflows in diagnostic and interventional precision
medicine. Prior efforts have contributed task-specific models capable of
solving problems within a narrow scope, but expanding to broader use requires
additional data, annotations, and training time. Recently, language-aligned
foundation models (LFMs) -- machine learning models trained on large amounts of
highly variable image and text data thus enabling broad applicability -- have
emerged as promising tools for automated image analysis. Existing foundation
models for medical image analysis focus on scenarios and modalities where
large, richly annotated datasets are available. However, the X-ray imaging
modality features highly variable image appearance and applications, from
diagnostic chest X-rays to interventional fluoroscopy, with varying
availability of data. To pave the way toward an LFM for comprehensive and
language-aligned analysis of arbitrary medical X-ray images, we introduce
FluoroSAM, a language-promptable variant of the Segment Anything Model, trained
from scratch on 3M synthetic X-ray images from a wide variety of human
anatomies, imaging geometries, and viewing angles. These include pseudo-ground
truth masks for 128 organ types and 464 tools with associated text
descriptions. FluoroSAM is capable of segmenting myriad anatomical structures
and tools based on natural language prompts, thanks to the novel incorporation
of vector quantization (VQ) of text embeddings in the training process. We
demonstrate FluoroSAM's performance quantitatively on real X-ray images and
showcase on several applications how FluoroSAM is a key enabler for rich
human-machine interaction in the X-ray image acquisition and analysis context.
Code is available at https://github.com/arcadelab/fluorosam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Early Stopping: Refine, Then Calibrate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugène Berta, David Holzmüller, Michael I. Jordan, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning classifiers often produce probabilistic predictions that are
critical for accurate and interpretable decision-making in various domains. The
quality of these predictions is generally evaluated with proper losses, such as
cross-entropy, which decompose into two components: calibration error assesses
general under/overconfidence, while refinement error measures the ability to
distinguish different classes. In this paper, we present a novel variational
formulation of the calibration-refinement decomposition that sheds new light on
post-hoc calibration, and enables rapid estimation of the different terms.
Equipped with this new perspective, we provide theoretical and empirical
evidence that calibration and refinement errors are not minimized
simultaneously during training. Selecting the best epoch based on validation
loss thus leads to a compromise point that is suboptimal for both terms. To
address this, we propose minimizing refinement error only during training
(Refine,...), before minimizing calibration error post hoc, using standard
techniques (...then Calibrate). Our method integrates seamlessly with any
classifier and consistently improves performance across diverse classification
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Various Software Artifacts for Better <span class="highlight-title">LLM</span>-based Bug
  Localization and Program Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Feng, Xiaotian Ma, Jiayi Sheng, Ziyuan Feng, Wei Song, Peng Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have garnered considerable attention for their potential to streamline
Automated Program Repair (APR). LLM-based approaches can either insert the
correct code or directly generate patches when provided with buggy methods.
However, most of LLM-based APR methods rely on a single type of software
information, without fully leveraging different software artifacts. Despite
this, many LLM-based approaches do not explore which specific types of
information best assist in APR. Addressing this gap is crucial for advancing
LLM-based APR techniques. We propose DEVLoRe to use issue content (description
and message) and stack error traces to localize buggy methods, then rely on
debug information in buggy methods and issue content and stack error to
localize buggy lines and generate plausible patches which can pass all unit
tests. The results show that while issue content is particularly effective in
assisting LLMs with fault localization and program repair, different types of
software artifacts complement each other. By incorporating different artifacts,
DEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy
methods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0
dataset, respectively. This outperforms current state-of-the-art APR methods.
Furthermore, we re-implemented and evaluated our framework, demonstrating its
effectiveness in its effectiveness in resolving 9 unique issues compared to
other state-of-the-art frameworks using the same or more advanced models on
SWE-bench Lite.We also discussed whether a leading framework for Python code
can be directly applied to Java code, or vice versa. The source code and
experimental results of this work for replication are available at
https://github.com/XYZboom/DEVLoRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 images, 10 tables, Manuscript revision submitted to a
  journal (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking In-Context Learning for Natural Datasets Beyond Language
  Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelena Bratulić, Sudhanshu Mittal, David T. Hoffmann, Samuel Böhm, Robin Tibor Schirrmeister, Tonio Ball, Christian Rupprecht, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables
the model to perform new tasks conditioning only on the examples provided in
the context without updating the model's weights. While ICL offers fast
adaptation across natural language tasks and domains, its emergence is less
straightforward for modalities beyond text. In this work, we systematically
uncover properties present in LLMs that support the emergence of ICL for
autoregressive models and various modalities by promoting the learning of the
needed mechanisms for ICL. We identify exact token repetitions in the training
data sequences as an important factor for ICL. Such repetitions further improve
stability and reduce transiency in ICL performance. Moreover, we emphasise the
significance of training task difficulty for the emergence of ICL. Finally, by
applying our novel insights on ICL emergence, we unlock ICL capabilities for
various visual datasets and a more challenging EEG classification task in a
few-shot learning regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabArena: A Living Benchmark for Machine Learning on Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzmüller, Prateek Mutalik Desai, David Salinas, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: fixed author list. 51 pages. Code available at
  https://tabarena.ai/code; examples at https://tabarena.ai/code-examples;
  dataset curation at https://tabarena.ai/data-tabular-ml-iid-study and
  https://tabarena.ai/dataset-curation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Reasoning at Jailbreaking Time <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, Hamed Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are becoming more capable and widespread, the
study of their failure cases is becoming increasingly important. Recent
advances in standardizing, measuring, and scaling test-time compute suggest new
methodologies for optimizing models to achieve high performance on hard tasks.
In this paper, we apply these advances to the task of model jailbreaking:
eliciting harmful responses from aligned LLMs. We develop an adversarial
reasoning approach to automatic jailbreaking that leverages a loss signal to
guide the test-time compute, achieving SOTA attack success rates against many
aligned LLMs, even those that aim to trade inference-time compute for
adversarial robustness. Our approach introduces a new paradigm in understanding
LLM vulnerabilities, laying the foundation for the development of more robust
and trustworthy AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 42nd International Conference on Machine Learning
  (ICML 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separating Tongue from Thought: Activation Patching Reveals
  Language-Agnostic Concept Representations in <span class="highlight-title">Transformer</span>s <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central question in multilingual language modeling is whether large
language models (LLMs) develop a universal concept representation, disentangled
from specific languages. In this paper, we address this question by analyzing
latent representations (latents) during a word-translation task in
transformer-based LLMs. We strategically extract latents from a source
translation prompt and insert them into the forward pass on a target
translation prompt. By doing so, we find that the output language is encoded in
the latent at an earlier layer than the concept to be translated. Building on
this insight, we conduct two key experiments. First, we demonstrate that we can
change the concept without changing the language and vice versa through
activation patching alone. Second, we show that patching with the mean
representation of a concept across different languages does not affect the
models' ability to translate it, but instead improves it. Finally, we
generalize to multi-token generation and demonstrate that the model can
generate natural language description of those mean representations. Our
results provide evidence for the existence of language-agnostic concept
representations within the investigated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 14 figures, previous version published under the title "How
  Do Llamas Process Multilingual Text? A Latent Exploration through Activation
  Patching" at the ICML 2024 mechanistic interpretability workshop at
  https://openreview.net/forum?id=0ku2hIm4BS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proximal Control of UAVs with Federated Learning for Human-Robot
  Collaborative <span class="highlight-title">Domain</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Nogueira Nobrega, Ewerton de Oliveira, Martin Saska, Tiago Nascimento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human-robot interaction (HRI) is a growing area of research. In HRI,
complex command (action) classification is still an open problem that usually
prevents the real applicability of such a technique. The literature presents
some works that use neural networks to detect these actions. However, occlusion
is still a major issue in HRI, especially when using uncrewed aerial vehicles
(UAVs), since, during the robot's movement, the human operator is often out of
the robot's field of view. Furthermore, in multi-robot scenarios, distributed
training is also an open problem. In this sense, this work proposes an action
recognition and control approach based on Long Short-Term Memory (LSTM) Deep
Neural Networks with two layers in association with three densely connected
layers and Federated Learning (FL) embedded in multiple drones. The FL enabled
our approach to be trained in a distributed fashion, i.e., access to data
without the need for cloud or other repositories, which facilitates the
multi-robot system's learning. Furthermore, our multi-robot approach results
also prevented occlusion situations, with experiments with real robots
achieving an accuracy greater than 96%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>version 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRAIL: Vectorized Reward-based Attribution for Interpretable Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16014v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16014v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jina Kim, Youjin Jang, Jeongjin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Plug-n-Play Knowledge Modules with Deep Context Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08727v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08727v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan Vulić, Alessandro Sordoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamically integrating new or rapidly evolving information after (Large)
Language Model pre-training remains challenging, particularly in low-data
scenarios or when dealing with private and specialized documents. In-context
learning and retrieval-augmented generation (RAG) face limitations, including
their high inference costs and their inability to capture global document
information. In this paper, we propose a way of modularizing knowledge by
training document-level Knowledge Modules (KMs). KMs are lightweight components
implemented as parameter-efficient LoRA modules, which are trained to store
information about new documents and can be easily plugged into models on
demand. We show that next-token prediction performs poorly as the training
objective for KMs. We instead propose Deep Context Distillation: we learn KMs
parameters such as to simulate hidden states and logits of a teacher that takes
the document in context. Our method outperforms standard next-token prediction
and pre-instruction training techniques, across two datasets. Finally, we
highlight synergies between KMs and RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated
  Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guinan Su, Jonas Geiping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning capabilities represent a critical frontier for large language
models (LLMs), but developing them requires extensive proprietary datasets and
computational resources. One way to efficiently supplement capabilities with is
by model merging, which offers a promising alternative by combining multiple
models without retraining. However, current merging approaches rely on
manually-designed strategies for merging hyperparameters, limiting the
exploration of potential model combinations and requiring significant human
effort. We propose an Automated Model Merging Framework that enables
fine-grained exploration of merging strategies while reducing costs through
multi-fidelity approximations. We support both single and multi-objective
optimization and introduce two novel search spaces: layerwise fusion (LFS) and
depth-wise integration (DIS). Evaluating across a number of benchmarks, we find
that the search autonomously finds 1) Merges that further boost
single-objective performance, even on tasks the model has already been
finetuned on, and 2) Merges that optimize multi-objective frontiers across
tasks. Effective merges are found with limited compute, e.g. within less than
500 search steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-equilibrium Annealed Adjoint Sampler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemoo Choi, Yongxin Chen, Molei Tao, Guan-Horng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse
  Myocardial Scar Synthesis and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Shahnaz Jamil-Copley, Richard H. Clayton, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based myocardial scar segmentation from late gadolinium
enhancement (LGE) cardiac MRI has shown great potential for accurate and timely
diagnosis and treatment planning for structural cardiac diseases. However, the
limited availability and variability of LGE images with high-quality scar
labels restrict the development of robust segmentation models. To address this,
we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE
\textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial
Scar Synthesis and Segmentation framework, a framework for anatomically
grounded scar generation and segmentation. At its core is the SMILE module
(Scar Mask generation guided by cLinical knowledgE), which conditions a
diffusion-based generator on the clinically adopted AHA 17-segment model to
synthesize images with anatomically consistent and spatially diverse scar
patterns. In addition, CLAIM employs a joint training strategy in which the
scar segmentation network is optimized alongside the generator, aiming to
enhance both the realism of synthesized scars and the accuracy of the scar
segmentation performance. Experimental results show that CLAIM produces
anatomically coherent scar patterns and achieves higher Dice similarity with
real scar distributions compared to baseline models. Our approach enables
controllable and realistic myocardial scar synthesis and has demonstrated
utility for downstream medical imaging task. Code is available at
https://github.com/farheenjabeen/CLAIM-Scar-Synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing
  Framework Based on <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzheng Dai, Yuanliang Li, Jun Yan, Zhibo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated penetration testing (AutoPT) powered by large language models
(LLMs) has gained attention for its ability to automate ethical hacking
processes and identify vulnerabilities in target systems by leveraging the
inherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often
underperform compared to human experts in challenging tasks for several
reasons: the imbalanced knowledge used in LLM training, short-sightedness in
the planning process, and hallucinations during command generation. Moreover,
the trial-and-error nature of the PT process is constrained by existing
frameworks lacking mechanisms to learn from previous failures, restricting
adaptive improvement of PT strategies. To address these limitations, we propose
a knowledge-informed, self-reflective PT framework powered by LLMs, called
RefPentester. This AutoPT framework is designed to assist human operators in
identifying the current stage of the PT process, selecting appropriate tactics
and techniques for each stage, choosing suggested actions, providing
step-by-step operational guidance, and reflecting on and learning from previous
failed operations. We also modeled the PT process as a seven-state Stage
Machine to integrate the proposed framework effectively. The evaluation shows
that RefPentester can successfully reveal credentials on Hack The Box's Sau
machine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages,
RefPentester also demonstrates superior success rates on PT stage transitions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scientists' First Exam: Probing Cognitive Abilities of M<span class="highlight-title">LLM</span> via
  Perception, Understanding, and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10521v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10521v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>82 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-informed Imitative Reinforcement Learning for Real-world Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhou, Yihao Qin, Dan Xu, Yiding Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in imitative reinforcement learning (IRL) have considerably
enhanced the ability of autonomous agents to assimilate expert demonstrations,
leading to rapid skill acquisition in a range of demanding tasks. However, such
learning-based agents face significant challenges when transferring knowledge
to highly dynamic closed-loop environments. Their performance is significantly
impacted by the conflicting optimization objectives of imitation learning (IL)
and reinforcement learning (RL), sample inefficiency, and the complexity of
uncovering the hidden world model and physics. To address this challenge, we
propose a physics-informed IRL that is entirely data-driven. It leverages both
expert demonstration data and exploratory data with a joint optimization
objective, allowing the underlying physical principles of vehicle dynamics to
emerge naturally from the training process. The performance is evaluated
through empirical experiments and results exceed popular IL, RL and IRL
algorithms in closed-loop settings on Waymax benchmark. Our approach exhibits
37.8% reduction in collision rate and 22.2% reduction in off-road rate compared
to the baseline method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogniBench: A Legal-inspired Framework and Dataset for Assessing
  Cognitive Faithfulness of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20767v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20767v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness hallucinations are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standards, existing benchmarks focus on "factual statements" that rephrase
source materials while overlooking "cognitive statements" that involve making
inferences from the given context. Consequently, evaluating and detecting the
hallucination of cognitive statements remains challenging. Inspired by how
evidence is assessed in the legal domain, we design a rigorous framework to
assess different levels of faithfulness of cognitive statements and introduce
the CogniBench dataset where we reveal insightful statistics. To keep pace with
rapidly evolving LLMs, we further develop an automatic annotation pipeline that
scales easily across different models. This results in a large-scale
CogniBench-L dataset, which facilitates training accurate detectors for both
factual and cognitive hallucinations. We release our model and datasets at:
https://github.com/FUTUREEEEEE/CogniBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Free Lunch: Rethinking Internal Feedback for <span class="highlight-title">LLM</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, Jiyan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WyckoffDiff -- A Generative <span class="highlight-title">Diffusion</span> Model for Crystal Symmetry <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Ekström Kelvinius, Oskar B. Andersson, Abhijith S. Parackal, Dong Qian, Rickard Armiento, Fredrik Lindsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystalline materials often exhibit a high level of symmetry. However, most
generative models do not account for symmetry, but rather model each atom
without any constraints on its position or element. We propose a generative
model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based
descriptions of crystals. This is enabled by considering a crystal structure
representation that encodes all symmetry, and we design a novel neural network
architecture which enables using this representation inside a discrete
generative model framework. In addition to respecting symmetry by construction,
the discrete nature of our model enables fast generation. We additionally
present a new metric, Fr\'echet Wrenformer Distance, which captures the
symmetry aspects of the materials generated, and we benchmark WyckoffDiff
against recently proposed generative models for crystal generation. As a
proof-of-concept study, we use WyckoffDiff to find new materials below the
convex hull of thermodynamical stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025, to appear in PMLR 267. Code is available
  online at https://github.com/httk/wyckoffdiff</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chemical knowledge-informed framework for privacy-aware retrosynthesis
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guikun Chen, Xu Zhang, Xiaolin Hu, Yong Liu, Yi Yang, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical reaction data is a pivotal asset, driving advances in competitive
fields such as pharmaceuticals, materials science, and industrial chemistry.
Its proprietary nature renders it sensitive, as it often includes confidential
insights and competitive advantages organizations strive to protect. However,
in contrast to this need for confidentiality, the current standard training
paradigm for machine learning-based retrosynthesis gathers reaction data from
multiple sources into one single edge to train prediction models. This paradigm
poses considerable privacy risks as it necessitates broad data availability
across organizational boundaries and frequent data transmission between
entities, potentially exposing proprietary information to unauthorized access
or interception during storage and transfer. In the present study, we introduce
the chemical knowledge-informed framework (CKIF), a privacy-preserving approach
for learning retrosynthesis models. CKIF enables distributed training across
multiple chemical organizations without compromising the confidentiality of
proprietary reaction data. Instead of gathering raw reaction data, CKIF learns
retrosynthesis models through iterative, chemical knowledge-informed
aggregation of model parameters. In particular, the chemical properties of
predicted reactants are leveraged to quantitatively assess the observable
behaviors of individual models, which in turn determines the adaptive weights
used for model aggregation. On a variety of reaction datasets, CKIF outperforms
several strong baselines by a clear margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal
  <span class="highlight-title">Large Language Model</span>s Preserving Language Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Xia, Yifeng Ding, Fengfa Li, Lei Ren, Wei Chen, Fangxiang Feng, Xiaojie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Explainable Reinforcement Learning: Concepts, Algorithms,
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06665v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06665v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Qing, Shunyu Liu, Jie Song, Huiqiong Wang, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and performance of agents while this
kind of method is often ignored in XRL field. Some challenges and opportunities
in XRL are discussed. This survey intends to provide a high-level summarization
of XRL and to motivate future research on more effective XRL solutions.
Corresponding open source codes are collected and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confucius3-Math: A Lightweight High-Performance Reasoning <span class="highlight-title">LLM</span> for
  Chinese K-12 Mathematics Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $C^3$-Bench: The Things Real Disturbing <span class="highlight-title">LLM</span> based Agent in <span class="highlight-title">Multi-Task</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents based on large language models leverage tools to modify environments,
revolutionizing how AI interacts with the physical world. Unlike traditional
NLP tasks that rely solely on historical dialogue for responses, these agents
must consider more complex factors, such as inter-tool relationships,
environmental feedback and previous decisions, when making choices. Current
research typically evaluates agents via multi-turn dialogues. However, it
overlooks the influence of these critical factors on agent behavior. To bridge
this gap, we present an open-source and high-quality benchmark $C^3$-Bench.
This benchmark integrates attack concepts and applies univariate analysis to
pinpoint key elements affecting agent robustness. In concrete, we design three
challenges: navigate complex tool relationships, handle critical hidden
information and manage dynamic decision paths. Complementing these challenges,
we introduce fine-grained metrics, innovative data collection algorithms and
reproducible evaluation methods. Extensive experiments are conducted on 49
mainstream agents, encompassing general fast-thinking, slow-thinking and
domain-specific models. We observe that agents have significant shortcomings in
handling tool dependencies, long context information dependencies and frequent
policy-type switching. In essence, $C^3$-Bench aims to expose model
vulnerabilities through these challenges and drive research into the
interpretability of agent performance. The benchmark is publicly available at
https://github.com/yupeijei1997/C3-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungho Baek, Taegeon Park, Jongchan Park, Seungjun Oh, Yusung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled
  <span class="highlight-title">Diffusion</span> Sequential Monte Carlo <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Ekström Kelvinius, Zheng Zhao, Fredrik Lindsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent line of research has exploited pre-trained generative diffusion
models as priors for solving Bayesian inverse problems. We contribute to this
research direction by designing a sequential Monte Carlo method for
linear-Gaussian inverse problems which builds on "decoupled diffusion", where
the generative process is designed such that larger updates to the sample are
possible. The method is asymptotically exact and we demonstrate the
effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC)
algorithm on both synthetic as well as protein and image data. Further, we
demonstrate how the approach can be extended to discrete data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025, to appear in PMLR 267. Code available at
  https://github.com/filipekstrm/ddsmc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Truthfulness and Informativeness with Uncertainty-Aware
  Instruction Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction fine-tuning (IFT) can increase the informativeness of large
language models (LLMs), but may reduce their truthfulness. This trade-off
arises because IFT steers LLMs to generate responses containing long-tail
knowledge that was not well covered during pre-training. As a result, models
become more informative but less accurate when generalizing to unseen tasks. In
this paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets
can negatively affect the truthfulness of LLMs, and we introduce two new IFT
paradigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$
identifies and removes unfamiliar knowledge from IFT datasets to mitigate its
impact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize
their uncertainty and explicitly indicate it at the end of their responses. Our
experiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,
while $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by
distinguishing between confident and uncertain statements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aurora: Are Android Malware Classifiers Reliable and Stable under
  Distribution Shift? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Herzog, Aliai Eusebi, Lorenzo Cavallaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance figures of modern drift-adaptive malware classifiers appear
promising, but does this translate to genuine operational reliability? The
standard evaluation paradigm primarily focuses on baseline performance metrics,
neglecting confidence-error alignment and operational stability. While
TESSERACT established the importance of temporal evaluation, we take a
complementary direction by investigating whether malware classifiers maintain
reliable and stable confidence estimates under distribution shifts and
exploring the tensions between scientific advancement and practical impacts
when they do not. We propose AURORA, a framework to evaluate malware
classifiers based on their confidence quality and operational resilience.
AURORA subjects the confidence profile of a given model to verification to
assess the reliability of its estimates. Unreliable confidence estimates erode
operational trust, waste valuable annotation budget on non-informative samples
for active learning, and leave error-prone instances undetected in selective
classification. AURORA is complemented by a set of metrics designed to go
beyond point-in-time performance, striving towards a more holistic assessment
of operational stability throughout temporal evaluation periods. The fragility
in SOTA frameworks across datasets of varying drift suggests the need for a
return to the whiteboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teacher Motion Priors: Enhancing Robot Locomotion over Challenging
  Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangcheng Jin, Yuqi Wang, Peixin Ma, Guodong Yang, Pan Zhao, En Li, Zhengtao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving robust locomotion on complex terrains remains a challenge due to
high dimensional control and environmental uncertainties. This paper introduces
a teacher prior framework based on the teacher student paradigm, integrating
imitation and auxiliary task learning to improve learning efficiency and
generalization. Unlike traditional paradigms that strongly rely on
encoder-based state embeddings, our framework decouples the network design,
simplifying the policy network and deployment. A high performance teacher
policy is first trained using privileged information to acquire generalizable
motion skills. The teacher's motion distribution is transferred to the student
policy, which relies only on noisy proprioceptive data, via a generative
adversarial mechanism to mitigate performance degradation caused by
distributional shifts. Additionally, auxiliary task learning enhances the
student policy's feature representation, speeding up convergence and improving
adaptability to varying terrains. The framework is validated on a humanoid
robot, showing a great improvement in locomotion stability on dynamic terrains
and significant reductions in development costs. This work provides a practical
solution for deploying robust locomotion strategies in humanoid robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 6 tables, IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and
  Real-World Wound Care <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Borst, Timo Dittus, Tassilo Dege, Astrid Schmieder, Samuel Kounev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic wounds affect a large population, particularly the elderly and
diabetic patients, who often exhibit limited mobility and co-existing health
conditions. Automated wound monitoring via mobile image capture can reduce
in-person physician visits by enabling remote tracking of wound size. Semantic
segmentation is key to this process, yet wound segmentation remains
underrepresented in medical imaging research. To address this, we benchmark
state-of-the-art deep learning models from general-purpose vision, medical
imaging, and top methods from public wound challenges. For a fair comparison,
we standardize training, data augmentation, and evaluation, conducting
cross-validation to minimize partitioning bias. We also assess real-world
deployment aspects, including generalization to an out-of-distribution wound
dataset, computational efficiency, and interpretability. Additionally, we
propose a reference object-based approach to convert AI-generated masks into
clinically relevant wound size estimates and evaluate this, along with mask
quality, for the five best architectures based on physician assessments.
Overall, the transformer-based TransNeXt showed the highest levels of
generalizability. Despite variations in inference times, all models processed
at least one image per second on the CPU, which is deemed adequate for the
intended application. Interpretability analysis typically revealed prominent
activations in wound regions, emphasizing focus on clinically relevant
features. Expert evaluation showed high mask approval for all analyzed models,
with VWFormer and ConvNeXtS backbone performing the best. Size retrieval
accuracy was similar across models, and predictions closely matched expert
annotations. Finally, we demonstrate how our AI-driven wound size estimation
framework, WoundAmbit, is integrated into a custom telehealth system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 18 pages; supplementary material: 15 pages; the paper has
  been accepted for publication at the Applied Data Science (ADS) track of the
  European Conference on Machine Learning and Principles and Practice of
  Knowledge Discovery in Databases (ECML PKDD 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Yu, Arthur Aubret, Marcel C. Raabe, Jane Yang, Chen Yu, Jochen Triesch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toddlers learn to recognize objects from different viewpoints with almost no
supervision. During this learning, they execute frequent eye and head movements
that shape their visual experience. It is presently unclear if and how these
behaviors contribute to toddlers' emerging object recognition abilities. To
answer this question, we here combine head-mounted eye tracking during dyadic
play with unsupervised machine learning. We approximate toddlers' central
visual field experience by cropping image regions from a head-mounted camera
centered on the current gaze location estimated via eye tracking. This visual
stream feeds an unsupervised computational model of toddlers' learning, which
constructs visual representations that slowly change over time. Our experiments
demonstrate that toddlers' gaze strategy supports the learning of invariant
object representations. Our analysis also shows that the limited size of the
central visual field where acuity is high is crucial for this. Overall, our
work reveals how toddlers' gaze behavior may support their development of
view-invariant object recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed satellite information networks: Architecture, enabling
  technologies, and trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyu Zhang, Liang Xu, Jianhao Huang, Tao Yang, Jian Jiao, Ye Wang, Yao Shi, Chiya Zhang, Xingjian Zhang, Ke Zhang, Yupeng Gong, Na Deng, Nan Zhao, Zhen Gao, Shujun Han, Xiaodong Xu, Li You, Dongming Wang, Shan Jiang, Dixian Zhao, Nan Zhang, Liujun Hu, Xiongwen He, Yonghui Li, Xiqi Gao, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the vision of ubiquitous connectivity and wireless intelligence,
the evolution of ultra-dense constellation-based satellite-integrated Internet
is underway, now taking preliminary shape. Nevertheless, the entrenched
institutional silos and limited, nonrenewable heterogeneous network resources
leave current satellite systems struggling to accommodate the escalating
demands of next-generation intelligent applications. In this context, the
distributed satellite information networks (DSIN), exemplified by the cohesive
clustered satellites system, have emerged as an innovative architecture,
bridging information gaps across diverse satellite systems, such as
communication, navigation, and remote sensing, and establishing a unified, open
information network paradigm to support resilient space information services.
This survey first provides a profound discussion about innovative network
architectures of DSIN, encompassing distributed regenerative satellite network
architecture, distributed satellite computing network architecture, and
reconfigurable satellite formation flying, to enable flexible and scalable
communication, computing and control. The DSIN faces challenges from network
heterogeneity, unpredictable channel dynamics, sparse resources, and
decentralized collaboration frameworks. To address these issues, a series of
enabling technologies is identified, including channel modeling and estimation,
cloud-native distributed MIMO cooperation, grant-free massive access, network
routing, and the proper combination of all these diversity techniques.
Furthermore, to heighten the overall resource efficiency, the cross-layer
optimization techniques are further developed to meet upper-layer
deterministic, adaptive and secure information services requirements. In
addition, emerging research directions and new opportunities are highlighted on
the way to achieving the DSIN vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds
  via Self-Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J Rosser, Jakob Nicolaus Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaffolding Large Language Models (LLMs) into multi-agent systems often
improves performance on complex tasks, but the safety impact of such scaffolds
has not been thoroughly explored. We introduce AgentBreeder, a framework for
multi-objective self-improving evolutionary search over scaffolds. We evaluate
discovered scaffolds on widely recognized reasoning, mathematics, and safety
benchmarks and compare them with popular baselines. In 'blue' mode, we see a
79.4% average uplift in safety benchmark performance while maintaining or
improving capability scores. In 'red' mode, we find adversarially weak
scaffolds emerging concurrently with capability optimization. Our work
demonstrates the risks of multi-agent scaffolding and provides a framework for
mitigating them. Code is available at
https://github.com/J-Rosser-UK/AgentBreeder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with
  Sparse and Dense Map Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D gaussian splatting has advanced simultaneous localization and mapping
(SLAM) technology by enabling real-time positioning and the construction of
high-fidelity maps. However, the uncertainty in gaussian position and
initialization parameters introduces challenges, often requiring extensive
iterative convergence and resulting in redundant or insufficient gaussian
representations. To address this, we introduce a novel adaptive densification
method based on Fourier frequency domain analysis to establish gaussian priors
for rapid convergence. Additionally, we propose constructing independent and
unified sparse and dense maps, where a sparse map supports efficient tracking
via Generalized Iterative Closest Point (GICP) and a dense map creates
high-fidelity visual representations. This is the first SLAM system leveraging
frequency domain analysis to achieve high-quality gaussian mapping in
real-time. Experimental results demonstrate an average frame rate of 36 FPS on
Replica and TUM RGB-D datasets, achieving competitive accuracy in both
localization and mapping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale
  Dynamic Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghan Li, Mingchen Li, Yipu Liao, Ruisheng Diao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IKDiffuser: A Generative Inverse Kinematics Solver for Multi-arm Robots
  via <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13087v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13087v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Zhang, Ziyuan Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has
primarily been successful with single serial manipulators. For multi-arm
robotic systems, IK remains challenging due to complex self-collisions, coupled
joints, and high-dimensional redundancy. These complexities make traditional IK
solvers slow, prone to failure, and lacking in solution diversity. In this
paper, we present IKDiffuser, a diffusion-based model designed for fast and
diverse IK solution generation for multi-arm robotic systems. IKDiffuser learns
the joint distribution over the configuration space, capturing complex
dependencies and enabling seamless generalization to multi-arm robotic systems
of different structures. In addition, IKDiffuser can incorporate additional
objectives during inference without retraining, offering versatility and
adaptability for task-specific requirements. In experiments on 6 different
multi-arm systems, the proposed IKDiffuser achieves superior solution accuracy,
precision, diversity, and computational efficiency compared to existing
solvers. The proposed IKDiffuser framework offers a scalable, unified approach
to solving multi-arm IK problems, facilitating the potential of multi-arm
robotic systems in real-time manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReconX: Reconstruct Any Scene from Sparse Views with Video <span class="highlight-title">Diffusion</span>
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16767v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16767v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://liuff19.github.io/ReconX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid AI for Responsive Multi-Turn Online Conversations with Novel
  Dynamic Routing and Feedback Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyaranjan Pattnayak, Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, Srikant Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 4th International Workshop on Knowledge Augmented
  Methods for Natural Language Processing in NAACL 2025, pages 215 to 229,
  Albuquerque, New Mexico, USA. Association for Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping the Evolution of Research Contributions using KnoVo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajratul Y. Rubaiat, Syed N. Sakib, Hasan M. Jamil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, Xinzhu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction
  and Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Chen, Jiyuan Zhang, Zecheng Hao, Yajing Zheng, Tiejun Huang, Zhaofei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spike cameras, as an innovative neuromorphic camera that captures scenes with
the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D
reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting
(3DGS). Previous spike-based 3D reconstruction approaches often employ a
casecased pipeline: starting with high-quality image reconstruction from spike
streams based on established spike-to-image reconstruction algorithms, then
progressing to camera pose estimation and 3D reconstruction. However, this
cascaded approach suffers from substantial cumulative errors, where quality
limitations of initial image reconstructions negatively impact pose estimation,
ultimately degrading the fidelity of the 3D reconstruction. To address these
issues, we propose a synergistic optimization framework, \textbf{USP-Gaussian},
that unifies spike-based image reconstruction, pose correction, and Gaussian
splatting into an end-to-end framework. Leveraging the multi-view consistency
afforded by 3DGS and the motion capture capability of the spike camera, our
framework enables a joint iterative optimization that seamlessly integrates
information between the spike-to-image network and 3DGS. Experiments on
synthetic datasets with accurate poses demonstrate that our method surpasses
previous approaches by effectively eliminating cascading errors. Moreover, we
integrate pose optimization to achieve robust 3D reconstruction in real-world
scenarios with inaccurate initial poses, outperforming alternative methods by
effectively reducing noise and preserving fine texture details. Our code, data
and trained models will be available at
https://github.com/chenkang455/USP-Gaussian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rewarding Graph Reasoning Process makes <span class="highlight-title">LLM</span>s more Generalized Reasoners <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Peng, Nuo Chen, Zongrui Suo, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Large Language Models (LLMs), developing
advanced reasoning capabilities in LLMs remains a key challenge. Process Reward
Models (PRMs) have demonstrated exceptional promise in enhancing reasoning by
providing step-wise feedback, particularly in the context of mathematical
reasoning. However, their application to broader reasoning domains remains
understudied, largely due to the high costs associated with manually creating
step-level supervision. In this work, we explore the potential of PRMs in graph
reasoning problems - a domain that demands sophisticated multi-step reasoning
and offers opportunities for automated step-level data generation using
established graph algorithms. We introduce GraphSILO, the largest dataset for
graph reasoning problems with fine-grained step-wise labels, built using
automated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to
generate detailed reasoning steps with step-wise labels. Building upon this
dataset, we train GraphPRM, the first PRM designed for graph reasoning
problems, and evaluate its effectiveness in two key settings: inference-time
scaling and reinforcement learning via Direct Preference Optimization (DPO).
Experimental results show that GraphPRM significantly improves LLM performance
across 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and
demonstrating transferability to new graph reasoning datasets and new reasoning
domains like mathematical problem-solving. Notably, GraphPRM enhances LLM
performance on GSM8K and Math500, underscoring the cross-domain applicability
of graph-based reasoning rewards. Our findings highlight the potential of PRMs
in advancing reasoning across diverse domains, paving the way for more
versatile and effective LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3S3: Complementary Competition and Contrastive Selection for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying He, Yitong Lin, Jiahe Chen, Honghui Xu, Jianwei Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
Outcome-Driven Contrastive Learning module dedicated to refining boundary
localization. Additionally, we incorporate a Dynamic Complementary Competition
module that leverages two high-performing sub-networks to generate
pseudo-labels, thereby further improving segmentation quality. The proposed
C3S3 undergoes rigorous validation on two publicly accessible datasets,
encompassing the practices of both MRI and CT scans. The results demonstrate
that our method achieves superior performance compared to previous cutting-edge
competitors. Especially, on the 95HD and ASD metrics, our approach achieves a
notable improvement of at least 6%, highlighting the significant advancements.
The code is available at https://github.com/Y-TARL/C3S3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnchorDP3: 3D Affordance Guided Sparse <span class="highlight-title">Diffusion</span> Policy for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Zhao, Ke Fan, He-Yang Xu, Ning Qiao, Bo Peng, Wenlong Gao, Dongjiang Li, Hui Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AnchorDP3, a diffusion policy framework for dual-arm robotic
manipulation that achieves state-of-the-art performance in highly randomized
environments. AnchorDP3 integrates three key innovations: (1)
Simulator-Supervised Semantic Segmentation, using rendered ground truth to
explicitly segment task-critical objects within the point cloud, which provides
strong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight
modules processing augmented point clouds per task, enabling efficient
multi-task learning through a shared diffusion-based action expert; (3)
Affordance-Anchored Keypose Diffusion with Full State Supervision, replacing
dense trajectory prediction with sparse, geometrically meaningful action
anchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to
affordances, drastically simplifying the prediction space; the action expert is
forced to predict both robot joint angles and end-effector poses
simultaneously, which exploits geometric consistency to accelerate convergence
and boost accuracy. Trained on large-scale, procedurally generated simulation
data, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark
across diverse tasks under extreme randomization of objects, clutter, table
height, lighting, and backgrounds. This framework, when integrated with the
RoboTwin real-to-sim pipeline, has the potential to enable fully autonomous
generation of deployable visuomotor policies from only scene and instruction,
totally eliminating human demonstrations from learning manipulation skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Wang, Siyuan Liang, Zhe Liu, Yi Yu, Yuliang Lu, Xiaochun Cao, Ee-Chien Chang, Xitong Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing integration of vision-language models (VLMs), mobile agents
are now widely used for tasks like UI automation and camera-based user
assistance. These agents are often fine-tuned on limited user-generated
datasets, leaving them vulnerable to covert threats during the training
process. In this work we present GHOST, the first clean-label backdoor attack
specifically designed for mobile agents built upon VLMs. Our method manipulates
only the visual inputs of a portion of the training samples - without altering
their corresponding labels or instructions - thereby injecting malicious
behaviors into the model. Once fine-tuned with this tampered data, the agent
will exhibit attacker-controlled responses when a specific visual trigger is
introduced at inference time. The core of our approach lies in aligning the
gradients of poisoned samples with those of a chosen target instance, embedding
backdoor-relevant features into the poisoned training data. To maintain stealth
and enhance robustness, we develop three realistic visual triggers: static
visual patches, dynamic motion cues, and subtle low-opacity overlays. We
evaluate our method across six real-world Android apps and three VLM
architectures adapted for mobile use. Results show that our attack achieves
high attack success rates (up to 94.67 percent) while maintaining high
clean-task performance (FSR up to 95.85 percent). Additionally, ablation
studies shed light on how various design choices affect the efficacy and
concealment of the attack. Overall, this work is the first to expose critical
security flaws in VLM-based mobile agents, highlighting their susceptibility to
clean-label backdoor attacks and the urgent need for effective defense
mechanisms in their training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Ekambaram, Subodh Kumar, Arindam Jati, Sumanta Mukherjee, Tomoya Sakai, Pankaj Dayama, Wesley M. Gifford, Jayant Kalagnanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of time-series pre-trained models has advanced temporal
representation learning, but current state-of-the-art models are often
large-scale, requiring substantial compute. We introduce TSPulse, ultra-compact
time-series pre-trained models with only 1M parameters, specialized to perform
strongly across classification, anomaly detection, imputation, and retrieval
tasks. TSPulse introduces innovations at both the architecture and task levels.
At the architecture level, it employs a dual-space masked reconstruction,
learning from both time and frequency domains to capture complementary signals.
This is further enhanced by a dual-embedding disentanglement, generating both
detailed embeddings for fine-grained analysis and high-level semantic
embeddings for broader task understanding. Notably, TSPulse's semantic
embeddings are robust to shifts in time, magnitude, and noise, which is
important for robust retrieval. At the task level, TSPulse incorporates TSLens,
a fine-tuning component enabling task-specific feature attention. It also
introduces a multi-head triangulation technique that correlates deviations from
multiple prediction heads, enhancing anomaly detection by fusing complementary
model outputs. Additionally, a hybrid mask pretraining is proposed to improves
zero-shot imputation by reducing pre-training bias. These architecture and task
innovations collectively contribute to TSPulse's significant performance gains:
5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly
detection leaderboard, +50% in zero-shot imputation, and +25% in time-series
retrieval. Remarkably, these results are achieved with just 1M parameters
(10-100X smaller than existing SOTA models) and allow GPU-free inference,
setting a new standard for efficient time-series pre-trained models. The models
can be accessed from
https://huggingface.co/ibm-granite/granite-timeseries-tspulse-r1
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Generalization and Representation Stability in Small LMs via
  Prompting, Fine-Tuning and Out-of-Distribution Prompts <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Raja, Arpita Vats
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings. Beyond accuracy, we analyze the internal
representations learned by each approach to assess the stability and
abstraction of task-specific features. Our findings highlight critical
differences in how small models internalize and generalize knowledge under
different adaptation strategies. This work offers practical guidance for model
selection in low-data regimes and contributes empirical insight into the
ongoing debate over prompting versus fine-tuning. Code for the experiments is
available at the following
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Multimodal Learning for Ophthalmic Disease Grading via
  Disentangled Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, Imran Razzak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses how ophthalmologists often rely on multimodal data to
improve diagnostic accuracy. However, complete multimodal data is rare in
real-world applications due to a lack of medical equipment and concerns about
data privacy. Traditional deep learning methods typically address these issues
by learning representations in latent space. However, the paper highlights two
key limitations of these approaches: (i) Task-irrelevant redundant information
(e.g., numerous slices) in complex modalities leads to significant redundancy
in latent space representations. (ii) Overlapping multimodal representations
make it difficult to extract unique features for each modality. To overcome
these challenges, the authors propose the Essence-Point and Disentangle
Representation Learning (EDRL) strategy, which integrates a self-distillation
mechanism into an end-to-end framework to enhance feature selection and
disentanglement for more robust multimodal learning. Specifically, the
Essence-Point Representation Learning module selects discriminative features
that improve disease grading performance. The Disentangled Representation
Learning module separates multimodal data into modality-common and
modality-unique representations, reducing feature entanglement and enhancing
both robustness and interpretability in ophthalmic disease diagnosis.
Experiments on multimodal ophthalmology datasets show that the proposed EDRL
strategy significantly outperforms current state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morse: Dual-Sampling for Lossless Acceleration of <span class="highlight-title">Diffusion</span> Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Li, Jiawei Fan, Anbang Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a prompt typo in Figure 18 of the Appendix. This work is
  accepted to ICML 2025. The project page:
  https://github.com/deep-optimization/Morse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PP-DocBee2: Improved Baselines with Efficient Data for Multimodal
  Document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kui Huang, Xinrong Chen, Wenyu Lv, Jincheng Liao, Guanzhong Wang, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Perturbation Guidance via Attention Head Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent guidance methods in diffusion models steer reverse sampling by
perturbing the model to construct an implicit weak model and guide generation
away from it. Among these approaches, attention perturbation has demonstrated
strong empirical performance in unconditional scenarios where classifier-free
guidance is not applicable. However, existing attention perturbation methods
lack principled approaches for determining where perturbations should be
applied, particularly in Diffusion Transformer (DiT) architectures where
quality-relevant computations are distributed across layers. In this paper, we
investigate the granularity of attention perturbations, ranging from the layer
level down to individual attention heads, and discover that specific heads
govern distinct visual concepts such as structure, style, and texture quality.
Building on this insight, we propose "HeadHunter", a systematic framework for
iteratively selecting attention heads that align with user-centric objectives,
enabling fine-grained control over generation quality and visual attributes. In
addition, we introduce SoftPAG, which linearly interpolates each selected
head's attention map toward an identity matrix, providing a continuous knob to
tune perturbation strength and suppress artifacts. Our approach not only
mitigates the oversmoothing issues of existing layer-level perturbation but
also enables targeted manipulation of specific visual styles through
compositional head selection. We validate our method on modern large-scale
DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,
demonstrating superior performance in both general quality enhancement and
style-specific guidance. Our work provides the first head-level analysis of
attention perturbation in diffusion models, uncovering interpretable
specialization within attention layers and enabling practical design of
effective perturbation strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cvlab-kaist.github.io/HeadHunter/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding World or Predicting Future? A Comprehensive <span class="highlight-title">Survey</span> of
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, Fengli Xu, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of world models has garnered significant attention due to
advancements in multimodal large language models such as GPT-4 and video
generation models such as Sora, which are central to the pursuit of artificial
general intelligence. This survey offers a comprehensive review of the
literature on world models. Generally, world models are regarded as tools for
either understanding the present state of the world or predicting its future
dynamics. This review presents a systematic categorization of world models,
emphasizing two primary functions: (1) constructing internal representations to
understand the mechanisms of the world, and (2) predicting future states to
simulate and guide decision-making. Initially, we examine the current progress
in these two categories. We then explore the application of world models in key
domains, including autonomous driving, robotics, and social simulacra, with a
focus on how each domain utilizes these aspects. Finally, we outline key
challenges and provide insights into potential future research directions. We
summarize the representative papers along with their code repositories in
https://github.com/tsinghua-fib-lab/World-Model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CSUR, 37 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From System 1 to System 2: A <span class="highlight-title">Survey</span> of Reasoning <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17419v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17419v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Xiao Liang, Zhijiang Guo, Le Song, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving human-level intelligence requires refining the transition from the
fast, intuitive System 1 to the slower, more deliberate System 2 reasoning.
While System 1 excels in quick, heuristic decisions, System 2 relies on logical
reasoning for more accurate judgments and reduced biases. Foundational Large
Language Models (LLMs) excel at fast decision-making but lack the depth for
complex reasoning, as they have not yet fully embraced the step-by-step
analysis characteristic of true System 2 thinking. Recently, reasoning LLMs
like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level
performance in fields such as mathematics and coding, closely mimicking the
deliberate reasoning of System 2 and showcasing human-like cognitive abilities.
This survey begins with a brief overview of the progress in foundational LLMs
and the early development of System 2 technologies, exploring how their
combination has paved the way for reasoning LLMs. Next, we discuss how to
construct reasoning LLMs, analyzing their features, the core methods enabling
advanced reasoning, and the evolution of various reasoning LLMs. Additionally,
we provide an overview of reasoning benchmarks, offering an in-depth comparison
of the performance of representative reasoning LLMs. Finally, we explore
promising directions for advancing reasoning LLMs and maintain a real-time
\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub
Repository} to track the latest developments. We hope this survey will serve as
a valuable resource to inspire innovation and drive progress in this rapidly
evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Slow-thinking, Large Language Models, Human-like Reasoning, Decision
  Making in AI, AGI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Quantum Machine Learning: A Future Outlook from Qubits to
  Enterprise Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24765v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24765v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srikanth Thudumu, Jason Fisher, Hung Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised Quantum Machine Learning (QML) represents an intersection of
quantum computing and classical machine learning, aiming to use quantum
resources to support model training and inference. This paper reviews recent
developments in supervised QML, focusing on methods such as variational quantum
circuits, quantum neural networks, and quantum kernel methods, along with
hybrid quantum-classical workflows. We examine recent experimental studies that
show partial indications of quantum advantage and describe current limitations
including noise, barren plateaus, scalability issues, and the lack of formal
proofs of performance improvement over classical methods. The main contribution
is a ten-year outlook (2025-2035) that outlines possible developments in
supervised QML, including a roadmap describing conditions under which QML may
be used in applied research and enterprise systems over the next decade.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Future outlook and roadmap of QML with 7 pages and 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Turing Test 2.0: The General Intelligence Threshold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19550v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19550v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Mappouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of artificial intelligence (A.I.) and large language models
like ChatGPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a system
(computer or any other) has reached or surpassed A.G.I. To achieve this, we
make two new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing test 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing test 2.0 framework
on modern A.I. models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIDRIN 2.0: A Framework to Assess Data Readiness for AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaveen Hiniduma, Dylan Ryan, Suren Byna, Jean Luca Bez, Ravi Madduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI Data Readiness Inspector (AIDRIN) is a framework to evaluate and improve
data preparedness for AI applications. It addresses critical data readiness
dimensions such as data quality, bias, fairness, and privacy. This paper
details enhancements to AIDRIN by focusing on user interface improvements and
integration with a privacy-preserving federated learning (PPFL) framework. By
refining the UI and enabling smooth integration with decentralized AI
pipelines, AIDRIN becomes more accessible and practical for users with varying
technical expertise. Integrating with an existing PPFL framework ensures that
data readiness and privacy are prioritized in federated learning environments.
A case study involving a real-world dataset demonstrates AIDRIN's practical
value in identifying data readiness issues that impact AI model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Fairness in <span class="highlight-title">LLM</span>s Beyond Tokens: A Semantic and Statistical
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum-Classical Hybrid Quantized Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxin Li, Chuan Wang, Hongdong Zhu, Qi Gao, Yin Ma, Hai Wei, Kai Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 figures, comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-light Pedestrian Detection in Visible and Infrared Image Feeds:
  Issues and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08557v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08557v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thangarajah Akilan, Hrishikesh Vachhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study reviews recent developments in low-light pedestrian
detection approaches. It systematically categorizes and analyses various
algorithms from region-based to non-region-based and graph-based learning
methodologies by highlighting their methodologies, implementation issues, and
challenges. It also outlines the key benchmark datasets that can be used for
research and development of advanced pedestrian detection algorithms,
particularly in low-light situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 4 tables, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computation Mechanism Behind <span class="highlight-title">LLM</span> Position Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13305v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13305v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Han, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most written natural languages are composed of sequences of words and
sentences. Similar to humans, large language models (LLMs) exhibit flexibility
in handling textual positions - a phenomenon we term position generalization.
They can understand texts with position perturbations and generalize to longer
texts than those encountered during training with the latest techniques. These
phenomena suggest that LLMs handle positions tolerantly, but how LLMs
computationally process positional relevance remains largely unexplored. This
work connects the linguistic phenomenon with LLMs' computational mechanisms. We
show how LLMs enforce certain computational mechanisms for the aforementioned
tolerance in position perturbations. Despite the complex design of the
self-attention mechanism, this work reveals that LLMs learn a counterintuitive
disentanglement of attention logits. Their values show a 0.959 linear
correlation with an approximation of the arithmetic sum of positional relevance
and semantic importance. Furthermore, we identify a prevalent pattern in
intermediate features, which we prove theoretically enables this effect. The
pattern, which is different from how randomly initialized parameters would
behave, suggests that it is a learned behavior rather than a natural result of
the model architecture. Based on these findings, we provide computational
explanations and criteria for LLMs' position flexibilities. This work takes a
pioneering step in linking position generalization with modern LLMs' internal
mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thought Anchors: Which <span class="highlight-title">LLM</span> Reasoning Steps Matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning large language models have recently achieved state-of-the-art
performance in many fields. However, their long-form chain-of-thought reasoning
creates interpretability challenges as each generated token depends on all
previous ones, making the computation harder to decompose. We argue that
analyzing reasoning traces at the sentence level is a promising approach to
understanding reasoning processes. We present three complementary attribution
methods: (1) a black-box method measuring each sentence's counterfactual
importance by comparing final answers across 100 rollouts conditioned on the
model generating that sentence or one with a different meaning; (2) a white-box
method of aggregating attention patterns between pairs of sentences, which
identified "broadcasting" sentences that receive disproportionate attention
from all future sentences via "receiver" attention heads; (3) a causal
attribution method measuring logical connections between sentences by
suppressing attention toward one sentence and measuring the effect on each
future sentence's tokens. Each method provides evidence for the existence of
thought anchors, reasoning steps that have outsized importance and that
disproportionately influence the subsequent reasoning process. These thought
anchors are typically planning or backtracking sentences. We provide an
open-source tool (www.thought-anchors.com) for visualizing the outputs of our
methods, and present a case study showing converging patterns across methods
that map how a model performs multi-step reasoning. The consistency across
methods demonstrates the potential of sentence-level analysis for a deeper
understanding of reasoning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paul C. Bogdan and Uzay Macar contributed equally to this work, and
  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy
  contributed equally to this work as senior authors, and their listed order
  was determined by coinflip</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Necessity of Output Distribution Reweighting for Effective Class
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yian Wang, Ali Ebrahimpour-Boroojeny, Hari Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omniwise: Predicting GPU Kernels Performance with <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex Model Transformations by Reinforcement Learning with Uncertain
  Human Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyanna Dagenais, Istvan David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ACM/IEEE MODELS'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Digital Agriculture: A Privacy-Preserving Framework for Data
  Sharing and Collaborative Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osama Zafar, Rosemarie Santa González, Mina Namazi, Alfonso Morales, Erman Ayday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2409.06069</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leaner Training, Lower Leakage: Revisiting Memorization in <span class="highlight-title">LLM</span>
  Fine-Tuning with LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Baochun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Reinforcement Learning for Cognitive Radar Resource
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Lu, Subodh Kalia, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Resource Management in Integrated Sensing and
  Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation
  Plasticity and Stress-Strain Response in FCC Alloys 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Luo, Yejun Gu, Yanfei Wang, Xiaolong Ma, Jaafar. A El-Awady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficacy of Temporal Fusion <span class="highlight-title">Transformer</span>s for Runoff Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sinan Rasiya Koya, Tirthankar Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying Distributed Training of Graph Neural Networks for Link
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Huang, Chul-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICDCS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal and Efficient Detection of Adversarial Data through Nonuniform
  Impact on Network Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Mumcu, Yasin Yilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2410.17442</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide, Specialize, and Route: A New Approach to Efficient Ensemble
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Piwko, Jędrzej Ruciński, Dawid Płudowski, Antoni Zajko, Patryzja Żak, Mateusz Zacharecki, Anna Kozak, Katarzyna Woźnica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated
  LSTMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashwat Khandelwal, Jakoba Petri-Koenig, Thomas B. Preußer, Michaela Blott, Shreejith Shanker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 5 tables, Accepted for publication in IEEE
  FPL-2025 (https://2025.fpl.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU Kernel Scientist: An <span class="highlight-title">LLM</span>-Driven Framework for Iterative Kernel
  Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Andrews, Sam Witteveen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 page paper plus Appendices. Accepted to the ES-FoMo "Efficient
  Systems for Foundation Models" workshop at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Ideation-Execution Gap: Execution Outcomes of <span class="highlight-title">LLM</span>-Generated versus
  Human Research Ideas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglei Si, Tatsunori Hashimoto, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper is 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural System Identification via Validation and Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian López, Keegan J. Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Parameter Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius Bushnaq, Dan Braun, Lee Sharkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A
  Theoretical Framework for Energy-Efficient Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Bara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, patent pending</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Minima of ReLU Neural Networks Suffer from the Curse of
  Dimensionality: The Neural Shattering Phenomenon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtong Liang, Dan Qiao, Yu-Xiang Wang, Rahul Parhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments Welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic and Non-local Closure Modeling for Nonlinear Dynamical
  Systems via Latent Score-based Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Dong, Huchen Yang, Jin-Long Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next-token prediction capacity: general upper bounds and a lower bound
  for <span class="highlight-title">transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Madden, Curtis Fox, Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a sequence of tokens, such as words, the task of next-token prediction
is to predict the next-token conditional probability distribution. Decoder-only
transformers have become effective models for this task, but their properties
are still not fully understood. In particular, the largest number of distinct
context sequences that a decoder-only transformer can interpolate next-token
distributions for has not been established. To fill this gap, we prove upper
and lower bounds on this number, which are equal up to a multiplicative
constant. We prove these bounds in the general setting where next-token
distributions can be arbitrary as well as the empirical setting where they are
calculated from a finite number of document sequences. Our lower bounds are for
one-layer multi-head decoder-only transformers and our proofs highlight an
important injectivity property satisfied by self-attention. Furthermore, we
provide numerical evidence that the minimal number of parameters for
memorization is sufficient for being able to train the model to the entropy
lower bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V3: added two examples, a remark, and a second experiment where only
  the FNN layers are trained</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperINF: Unleashing the HyperPower of the Schulz's Method for Data
  Influence Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhou, Simin Fan, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide a principled method to assess the contribution of
individual training samples to a specific target. Yet, their high computational
costs limit their applications on large-scale models and datasets. Existing
methods proposed for influence function approximation have significantly
reduced the computational overheads. However, they mostly suffer from
inaccurate estimation due to the lack of strong convergence guarantees from the
algorithm. The family of hyperpower methods are well-known for their rigorous
convergence guarantees on matrix inverse approximation, while the matrix
multiplication operation can involve intractable memory and computation costs
on large-scale models. We propose HyperINF, an efficient and accurate influence
function approximation method which leverages the hyperpower method,
specifically Schulz's iterative algorithm. To deal with the
computation-intensive matrix multiplication, we incorporate the generalized
fisher information (GFIM) as a low-rank approximation of the Hessian matrix,
which reduces the memory and computation overheads to constant costs
independent of ranks on LoRA-tuned models. We first demonstrate the superior
accuracy and stability of HyperINF compared to other baselines through a
synthetic convergence simulation for matrix inversion. We further validate the
efficacy of HyperINF through extensive real-world data attribution tasks,
including mislabeled data detection and data selection for LLM and VLM
fine-tuning. On LoRA-tuned models, HyperINF achieves superior downstream
performance with minimal memory and computational overhead, while other
baselines suffer from significant degradation. Our codebase is available at
https://github.com/Blackzxy/HyperINF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairly Accurate: Fairness-aware Multi-group Target Detection in Online
  Discussion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyajit Gupta, Maria De-Arteaga, Matthew Lease
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target-group detection is the task of detecting which group(s) a social media
post is ``directed at or about'', with various applications, such as
targeted-marketing. In this work, we focus on the fairness implications of
target-group detection in the context of toxicity detection, where the
perceived harm of a post often depends on which group(s) it targets. Because
toxicity is highly contextual, language that appears benign in general may be
harmful when targeting specific demographic groups. It is thus important to
first detect which group(s) are being {\em targeted} by a post as a precursor
to the subsequent task of determining whether the post is toxic given the
group(s). Target-group detection is also challenging: a single post may
simultaneously target one to many groups, and we must detect groups fairly in
order to promote equitable treatment. We show that our proposed approach to
{\em fairness-aware multi target-group detection} not only reduces bias across
groups, but also achieves competitive predictive performance, outperforming
existing fairness-aware baselines. To spur future research on fairness-aware
target-group detection and support competitive benchmarking, we also share our
code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Always Skip Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Peyman Moghadam, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We highlight a curious empirical result within modern Vision Transformers
(ViTs). Specifically, self-attention catastrophically fails to train unless it
is used in conjunction with a skip connection. This is in contrast to other
elements of a ViT that continue to exhibit good performance (albeit suboptimal)
when skip connections are removed. Further, we show that this critical
dependence on skip connections is a relatively new phenomenon, with previous
deep architectures (\eg, CNNs) exhibiting good performance in their absence. In
this paper, we theoretically characterize that the self-attention mechanism is
fundamentally ill-conditioned and is, therefore, uniquely dependent on skip
connections for regularization. Additionally, we propose Token Graying -- a
simple yet effective complement (to skip connections) that further improves the
condition of input tokens. We validate our approach in both supervised and
self-supervised training methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has just been accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A3 : an Analytical Low-Rank Approximation Framework for Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like pruning and quantization, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, KV cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and
mixed-rank assignments for enhanced performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-dimensional Contextual Bandit Problem without Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpei Komiyama, Masaaki Imaizumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we investigate the high-dimensional linear contextual
bandit problem where the number of features $p$ is greater than the budget $T$,
or it may even be infinite. Differing from the majority of previous works in
this field, we do not impose sparsity on the regression coefficients. Instead,
we rely on recent findings on overparameterized models, which enables us to
analyze the performance of the minimum-norm interpolating estimator when data
distributions have small effective ranks. We propose an explore-then-commit
(EtC) algorithm to address this problem and examine its performance. Through
our analysis, we derive the optimal rate of the ETC algorithm in terms of $T$
and show that this rate can be achieved by balancing exploration and
exploitation. Moreover, we introduce an adaptive explore-then-commit (AEtC)
algorithm that adaptively finds the optimal balance. We assess the performance
of the proposed algorithms through a series of simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subspace-Distance-Enabled Active Learning for Efficient Data-Driven
  Model Reduction of Parametric Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshit Kapadia, Peter Benner, Lihong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In situations where the solution of a high-fidelity dynamical system needs to
be evaluated repeatedly, over a vast pool of parametric configurations and in
absence of access to the underlying governing equations, data-driven model
reduction techniques are preferable. We propose a novel active learning
approach to build a parametric data-driven reduced-order model (ROM) by
greedily picking the most important parameter samples from the parameter
domain. As a result, during the ROM construction phase, the number of
high-fidelity solutions dynamically grow in a principled fashion. The
high-fidelity solution snapshots are expressed in several parameter-specific
linear subspaces, with the help of proper orthogonal decomposition (POD), and
the relative distance between these subspaces is used as a guiding mechanism to
perform active learning. For successfully achieving this, we provide a distance
measure to evaluate the similarity between pairs of linear subspaces with
different dimensions, and also show that this distance measure is a metric. The
usability of the proposed subspace-distance-enabled active learning (SDE-AL)
framework is demonstrated by augmenting two existing non-intrusive
reduced-order modeling approaches, and providing their active-learning-driven
(ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN.
Furthermore, we report positive results for two parametric physical models,
highlighting the efficiency of the proposed SDE-AL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 10 figures, 4 tables; v2: minor improvements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterFormer: Effective Heterogeneous Interaction Learning for
  Click-Through Rate Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09852v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09852v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Zeng, Xiaolong Liu, Mengyue Hang, Xiaoyi Liu, Qinghai Zhou, Chaofei Yang, Yiqun Liu, Yichen Ruan, Laming Chen, Yuxin Chen, Yujia Hao, Jiaqi Xu, Jade Nie, Xi Liu, Buyun Zhang, Wei Wen, Siyang Yuan, Hang Yin, Xin Zhang, Kai Wang, Wen-Yen Chen, Yiping Han, Huayu Li, Chunzhi Yang, Bo Long, Philip S. Yu, Hanghang Tong, Jiyan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction, which predicts the probability of a user
clicking an ad, is a fundamental task in recommender systems. The emergence of
heterogeneous information, such as user profile and behavior sequences, depicts
user interests from different aspects. A mutually beneficial integration of
heterogeneous information is the cornerstone towards the success of CTR
prediction. However, most of the existing methods suffer from two fundamental
limitations, including (1) insufficient inter-mode interaction due to the
unidirectional information flow between modes, and (2) aggressive information
aggregation caused by early summarization, resulting in excessive information
loss. To address the above limitations, we propose a novel module named
InterFormer to learn heterogeneous information interaction in an interleaving
style. To achieve better interaction learning, InterFormer enables
bidirectional information flow for mutually beneficial learning across
different modes. To avoid aggressive information aggregation, we retain
complete information in each data mode and use a separate bridging arch for
effective information selection and summarization. Our proposed InterFormer
achieves state-of-the-art performance on three public datasets and a
large-scale industrial dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tiny Machine Learning to Tiny Deep Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shriyank Somvanshi, Md Monzurul Islam, Gaurab Chhetri, Rohit Chakraborty, Mahmuda Sultana Mimi, Sawgat Ahmed Shuvo, Kazi Sifatul Islam, Syed Aaqib Javed, Sharif Ahmed Rafat, Anandi Dutta, Subasish Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of edge devices has driven the demand for deploying
artificial intelligence (AI) at the edge, giving rise to Tiny Machine Learning
(TinyML) and its evolving counterpart, Tiny Deep Learning (TinyDL). While
TinyML initially focused on enabling simple inference tasks on
microcontrollers, the emergence of TinyDL marks a paradigm shift toward
deploying deep learning models on severely resource-constrained hardware. This
survey presents a comprehensive overview of the transition from TinyML to
TinyDL, encompassing architectural innovations, hardware platforms, model
optimization techniques, and software toolchains. We analyze state-of-the-art
methods in quantization, pruning, and neural architecture search (NAS), and
examine hardware trends from MCUs to dedicated neural accelerators.
Furthermore, we categorize software deployment frameworks, compilers, and
AutoML tools enabling practical on-device learning. Applications across domains
such as computer vision, audio recognition, healthcare, and industrial
monitoring are reviewed to illustrate the real-world impact of TinyDL. Finally,
we identify emerging directions including neuromorphic computing, federated
TinyDL, edge-native foundation models, and domain-specific co-design
approaches. This survey aims to serve as a foundational resource for
researchers and practitioners, offering a holistic view of the ecosystem and
laying the groundwork for future advancements in edge AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Biases in Record Matching Through Scores Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hossein Moslemi, Mostafa Milani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Record matching is the task of identifying records that refer to the same
real-world entity across datasets. While most existing models optimize for
accuracy, fairness has become an important concern due to the potential for
unequal outcomes across demographic groups. Prior work typically focuses on
binary outcomes evaluated at fixed decision thresholds. However, such
evaluations can miss biases in matching scores--biases that persist across
thresholds and affect downstream tasks. We propose a threshold-independent
framework for measuring and reducing score bias, defined as disparities in the
distribution of matching scores across groups. We show that several
state-of-the-art matching methods exhibit substantial score bias, even when
appearing fair under standard threshold-based metrics. To address this, we
introduce two post-processing score calibration algorithms. The first, calib,
aligns group-wise score distributions using the Wasserstein barycenter,
targeting demographic parity. The second, ccalib, conditions on predicted
labels to further reduce label-dependent biases, such as equal opportunity.
Both methods are model-agnostic and require no access to model training data.
calib also offers theoretical guarantees, ensuring reduced bias with minimal
deviation from original scores. Experiments across real-world datasets and
matching models confirm that calib and ccalib substantially reduce score bias
while minimally impacting model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Global False Negatives On the Fly for Self-supervised
  Contrastive Learning <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicente Balmaseda, Bokun Wang, Ching-Long Lin, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In self-supervised contrastive learning, negative pairs are typically
constructed using an anchor image and a sample drawn from the entire dataset,
excluding the anchor. However, this approach can result in the creation of
negative pairs with similar semantics, referred to as "false negatives",
leading to their embeddings being falsely pushed apart. To address this issue,
we introduce GloFND, an optimization-based approach that automatically learns
on the fly the threshold for each anchor data to identify its false negatives
during training. In contrast to previous methods for false negative discovery,
our approach globally detects false negatives across the entire dataset rather
than locally within the mini-batch. Moreover, its per-iteration computation
cost remains independent of the dataset size. Experimental results on image and
image-text data demonstrate the effectiveness of the proposed method. Our
implementation is available at https://github.com/vibalcam/GloFND.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Universal Geometry of Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishi Jha, Collin Zhang, Vitaly Shmatikov, John X. Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the first method for translating text embeddings from one vector
space to another without any paired data, encoders, or predefined sets of
matches. Our unsupervised approach translates any embedding to and from a
universal latent representation (i.e., a universal semantic structure
conjectured by the Platonic Representation Hypothesis). Our translations
achieve high cosine similarity across model pairs with different architectures,
parameter counts, and training datasets.
  The ability to translate unknown embeddings into a different space while
preserving their geometry has serious implications for the security of vector
databases. An adversary with access only to embedding vectors can extract
sensitive information about the underlying documents, sufficient for
classification and attribute inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxa<span class="highlight-title">Diffusion</span>: Progressively Trained <span class="highlight-title">Diffusion</span> Model for Fine-Grained
  Species Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Karimi Monsefi, Mridul Khurana, Rajiv Ramnath, Anuj Karpatne, Wei-Lun Chao, Cheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose TaxaDiffusion, a taxonomy-informed training framework for
diffusion models to generate fine-grained animal images with high morphological
and identity accuracy. Unlike standard approaches that treat each species as an
independent category, TaxaDiffusion incorporates domain knowledge that many
species exhibit strong visual similarities, with distinctions often residing in
subtle variations of shape, pattern, and color. To exploit these relationships,
TaxaDiffusion progressively trains conditioned diffusion models across
different taxonomic levels -- starting from broad classifications such as Class
and Order, refining through Family and Genus, and ultimately distinguishing at
the Species level. This hierarchical learning strategy first captures
coarse-grained morphological traits shared by species with common ancestors,
facilitating knowledge transfer before refining fine-grained differences for
species-level distinction. As a result, TaxaDiffusion enables accurate
generation even with limited training samples per species. Extensive
experiments on three fine-grained animal datasets demonstrate that outperforms
existing approaches, achieving superior fidelity in fine-grained animal image
generation. Project page: https://amink8.github.io/TaxaDiffusion/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced computer vision for extracting georeferenced vehicle
  trajectories from drone imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02136v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02136v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Fonod, Haechan Cho, Hwasoo Yeo, Nikolas Geroliminis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework for extracting georeferenced vehicle
trajectories from high-altitude drone imagery, addressing key challenges in
urban traffic monitoring and the limitations of traditional ground-based
systems. Our approach integrates several novel contributions, including a
tailored object detector optimized for high-altitude bird's-eye view
perspectives, a unique track stabilization method that uses detected vehicle
bounding boxes as exclusion masks during image registration, and an orthophoto
and master frame-based georeferencing strategy that enhances consistent
alignment across multiple drone viewpoints. Additionally, our framework
features robust vehicle dimension estimation and detailed road segmentation,
enabling comprehensive traffic analysis. Conducted in the Songdo International
Business District, South Korea, the study utilized a multi-drone experiment
covering 20 intersections, capturing approximately 12TB of 4K video data over
four days. The framework produced two high-quality datasets: the Songdo Traffic
dataset, comprising approximately 700,000 unique vehicle trajectories, and the
Songdo Vision dataset, containing over 5,000 human-annotated images with about
300,000 vehicle instances in four classes. Comparisons with high-precision
sensor data from an instrumented probe vehicle highlight the accuracy and
consistency of our extraction pipeline in dense urban environments. The public
release of Songdo Traffic and Songdo Vision, and the complete source code for
the extraction pipeline, establishes new benchmarks in data quality,
reproducibility, and scalability in traffic research. Results demonstrate the
potential of integrating drone technology with advanced computer vision for
precise and cost-effective urban traffic monitoring, providing valuable
resources for developing intelligent transportation systems and enhancing
traffic management strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Your <span class="highlight-title">Diffusion</span> Policy with Latent Space Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Wagenmaker, Mitsuhiko Nakamoto, Yunchu Zhang, Seohong Park, Waleed Yagoub, Anusha Nagabandi, Abhishek Gupta, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing higher-order neural representations of uncertainty with the
  Noise Estimation through Reinforcement-based <span class="highlight-title">Diffusion</span> (NERD) model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojjat Azimi Asrari, Megan A. K. Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies often aim to reveal ``first-order" representations (FORs), which
encode aspects of an observer's environment, such as contents or structure. A
less-common target is ``higher-order" representations (HORs), which are
``about" FORs -- e.g., their strength or uncertainty -- and which may
contribute to learning. HORs about uncertainty are unlikely to be direct
``read-outs" of FOR characteristics, instead reflecting noisy estimation
processes incorporating prior expectations about uncertainty, but how the brain
represents such expected uncertainty distributions remains largely unexplored.
Here, we study ``noise expectation" HORs using neural data from a task which
may require the brain to learn about its own noise: decoded neurofeedback,
wherein human subjects learn to volitionally produce target neural patterns. We
develop and apply a Noise Estimation through Reinforcement-based Diffusion
(NERD) model to characterize how brains may undertake this process, and show
that NERD offers high explanatory power for human behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 7 figures, 12 equations</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-24T00:00:00Z">2025-06-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Retrieval-augmented Context Evaluation for Long-form RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Huei Ju, Suzan Verberne, Maarten de Rijke, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances large language models by
incorporating context retrieved from external knowledge sources. While the
effectiveness of the retrieval module is typically evaluated with
relevance-based ranking metrics, such metrics may be insufficient to reflect
the retrieval's impact on the final RAG result, especially in long-form
generation scenarios. We argue that providing a comprehensive
retrieval-augmented context is important for long-form RAG tasks like report
generation and propose metrics for assessing the context independent of
generation. We introduce CRUX, a \textbf{C}ontrolled
\textbf{R}etrieval-a\textbf{U}gmented conte\textbf{X}t evaluation framework
designed to directly assess retrieval-augmented contexts. This framework uses
human-written summaries to control the information scope of knowledge, enabling
us to measure how well the context covers information essential for long-form
generation. CRUX uses question-based evaluation to assess RAG's retrieval in a
fine-grained manner. Empirical results show that CRUX offers more reflective
and diagnostic evaluation. Our findings also reveal substantial room for
improvement in current retrieval methods, pointing to promising directions for
advancing RAG's retrieval. Our data and code are publicly available to support
and advance future research on retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for
  Evolving Multi-Class Imbalanced Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soheil Abadifard, Fazli Can
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoVE: Compressed Vocabulary Expansion Makes Better <span class="highlight-title">LLM</span>-based Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Fan Guo, Albert Merono Penuela, Sergio Maffeis, Fabio Pierazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive research on Machine Learning-based Network Intrusion
Detection Systems (ML-NIDS), their capability to detect diverse attack variants
remains uncertain. Prior studies have largely relied on homogeneous datasets,
which artificially inflate performance scores and offer a false sense of
security. Designing systems that can effectively detect a wide range of attack
variants remains a significant challenge. The progress of ML-NIDS continues to
depend heavily on human expertise, which can embed subjective judgments of
system designers into the model, potentially hindering its ability to
generalize across diverse attack types.
  To address this gap, we propose KnowML, a framework for knowledge-guided
machine learning that integrates attack knowledge into ML-NIDS. KnowML
systematically explores the threat landscape by leveraging Large Language
Models (LLMs) to perform automated analysis of attack implementations. It
constructs a unified Knowledge Graph (KG) of attack strategies, on which it
applies symbolic reasoning to generate KG-Augmented Input, embedding domain
knowledge directly into the design process of ML-NIDS.
  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly
collected for this study. Our findings reveal that baseline ML-NIDS models fail
to detect several variants entirely, achieving F1 scores as low as 0 %. In
contrast, our knowledge-guided approach achieves up to 99 % F1 score while
maintaining a False Positive Rate below 0.1 %.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Do Open-Source <span class="highlight-title">LLM</span>s Struggle with Data Analysis? A Systematic
  Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold promise in automating data analysis tasks,
yet open-source models face significant limitations in these kinds of
reasoning-intensive scenarios. In this work, we investigate strategies to
enhance the data analysis capabilities of open-source LLMs. By curating a seed
dataset of diverse, realistic scenarios, we evaluate models across three
dimensions: data understanding, code generation, and strategic planning. Our
analysis reveals three key findings: (1) Strategic planning quality serves as
the primary determinant of model performance; (2) Interaction design and task
complexity significantly influence reasoning capabilities; (3) Data quality
demonstrates a greater impact than diversity in achieving optimal performance.
We leverage these insights to develop a data synthesis methodology,
demonstrating significant improvements in open-source LLMs' analytical
reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alleviating User-Sensitive bias with Fair Generative Sequential
  <span class="highlight-title">Recommendation</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Feng Wu, Xuefang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation fairness has recently attracted much attention. In the real
world, recommendation systems are driven by user behavior, and since users with
the same sensitive feature (e.g., gender and age) tend to have the same
patterns, recommendation models can easily capture the strong correlation
preference of sensitive features and thus cause recommendation unfairness.
Diffusion model (DM) as a new generative model paradigm has achieved great
success in recommendation systems. DM's ability to model uncertainty and
represent diversity, and its modeling mechanism has a high degree of
adaptability with the real-world recommendation process with bias. Therefore,
we use DM to effectively model the fairness of recommendation and enhance the
diversity. This paper proposes a FairGENerative sequential Recommendation model
based on DM, FairGENRec. In the training phase, we inject random noise into the
original distribution under the guidance of the sensitive feature recognition
model, and a sequential denoise model is designed for the reverse
reconstruction of items. Simultaneously, recommendation fairness modeling is
completed by injecting multi-interests representational information that
eliminates the bias of sensitive user features into the generated results. In
the inference phase, the model obtains the noise in the form of noise addition
by using the history interactions which is followed by reverse iteration to
reconstruct the target item representation. Finally, our extensive experiments
on three datasets demonstrate the dual enhancement effect of FairGENRec on
accuracy and fairness, while the statistical analysis of the cases visualizes
the degree of improvement on the fairness of the recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and
  Ranking <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenbin Qian, Diptesh Kanojia, Samarth Agrawal, Hadeel Saadany, Swapnil Bhosale, Constantin Orasan, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce information retrieval (IR) systems struggle to simultaneously
achieve high accuracy in interpreting complex user queries and maintain
efficient processing of vast product catalogs. The dual challenge lies in
precisely matching user intent with relevant products while managing the
computational demands of real-time search across massive inventories. In this
paper, we propose a Nested Embedding Approach to product Retrieval and Ranking,
called NEAR$^2$, which can achieve up to $12$ times efficiency in embedding
size at inference time while introducing no extra cost in training and
improving performance in accuracy for various encoder-based Transformer models.
We validate our approach using different loss functions for the retrieval and
ranking task, including multiple negative ranking loss and online contrastive
loss, on four different test sets with various IR challenges such as short and
implicit queries. Our approach achieves an improved performance over a smaller
embedding dimension, compared to any existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to the 2025 SIGIR Workshop on eCommerce</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Higher-Order Graph Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Shriram Chandran, Jakub Cudak, Patrick Iff, Marcin Copik, Robert Gerstenberger, Tomasz Szydlo, Jürgen Müller, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in graph databases (GDBs) have been driving interest in
large-scale analytics, yet current systems fail to support higher-order (HO)
interactions beyond first-order (one-hop) relations, which are crucial for
tasks such as subgraph counting, polyadic modeling, and HO graph learning. We
address this by introducing a new class of systems, higher-order graph
databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly
extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and
OLAP queries, ensuring correctness, scalability, and ACID compliance. We
implement a lightweight, modular, and parallelizable HO-GDB prototype that
offers native support for hypergraphs, node-tuples, subgraphs, and other HO
structures under a unified API. The prototype scales to large HO OLTP & OLAP
workloads and shows how HO improves analytical tasks, for example enhancing
accuracy of graph neural networks within a GDB by 44%. Our work ensures low
latency and high query throughput, and generalizes both ACID-compliant and
eventually consistent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devesh Pant, Rishi Raj Grandhe, Vipin Samaria, Mukul Paul, Sudhir Kumar, Saransh Khanna, Jatin Agrawal, Jushaan Singh Kalra, Akhil VSSG, Satish V Khalikar, Vipin Garg, Himanshu Chauhan, Pranay Verma, Neha Khandelwal, Soma S Dhavala, Minesh Mathew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of disease outbreaks is crucial to ensure timely intervention
by the health authorities. Due to the challenges associated with traditional
indicator-based surveillance, monitoring informal sources such as online media
has become increasingly popular. However, owing to the number of online
articles getting published everyday, manual screening of the articles is
impractical. To address this, we propose Health Sentinel. It is a multi-stage
information extraction pipeline that uses a combination of ML and non-ML
methods to extract events-structured information concerning disease outbreaks
or other unusual health events-from online articles. The extracted events are
made available to the Media Scanning and Verification Cell (MSVC) at the
National Centre for Disease Control (NCDC), Delhi for analysis, interpretation
and further dissemination to local agencies for timely intervention. From April
2022 till date, Health Sentinel has processed over 300 million news articles
and identified over 95,000 unique health events across India of which over
3,500 events were shortlisted by the public health experts at NCDC as potential
outbreaks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aug2Search: Enhancing Facebook Marketplace Search with <span class="highlight-title">LLM</span>-Generated
  Synthetic Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Xi, He Ba, Hao Yuan, Rishu Agrawal, Yuxin Tian, Ruoyan Kong, Arul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., "Click" and "Listing
Interactions")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Answering Multimodal Exclusion Queries with Lightweight Sparse
  Disentangled Representations <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03184v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03184v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prachi J, Sumit Bhatia, Srikanta Bedathur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representations that enable cross-modal retrieval are widely used.
However, these often lack interpretability making it difficult to explain the
retrieved results. Solutions such as learning sparse disentangled
representations are typically guided by the text tokens in the data, making the
dimensionality of the resulting embeddings very high. We propose an approach
that generates smaller dimensionality fixed-size embeddings that are not only
disentangled but also offer better control for retrieval tasks. We demonstrate
their utility using challenging exclusion queries over MSCOCO and Conceptual
Captions benchmarks. Our experiments show that our approach is superior to
traditional dense models such as CLIP, BLIP and VISTA (gains up to 11% in
AP@10), as well as sparse disentangled models like VDR (gains up to 21% in
AP@10). We also present qualitative results to further underline the
interpretability of disentangled representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 2025 International ACM SIGIR Conference on
  Innovative Concepts and Theories in Information Retrieval (ICTIR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy and type-token ratio in gigaword corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are different ways of measuring diversity in complex systems. In
particular, in language, lexical diversity is characterized in terms of the
type-token ratio and the word entropy. We here investigate both diversity
metrics in six massive linguistic datasets in English, Spanish, and Turkish,
consisting of books, news articles, and tweets. These gigaword corpora
correspond to languages with distinct morphological features and differ in
registers and genres, thus constituting a varied testbed for a quantitative
approach to lexical diversity. We unveil an empirical functional relation
between entropy and type-token ratio of texts of a given corpus and language,
which is a consequence of the statistical laws observed in natural language.
Further, in the limit of large text lengths we find an analytical expression
for this relation relying on both Zipf and Heaps laws that agrees with our
empirical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Bo Wang, Sedigheh Eslami, Scott Martens, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-document retrieval, semantic text similarity, and code search.
Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves
state-of-the-art performance on both single-modal and cross-modal retrieval
tasks, with particular strength in processing visually rich content such as
tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of
this capability, we also introduce Jina-VDR, a novel benchmark specifically
designed for visually rich image retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 1-10 main, 14-22 experimental results, benchmark tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking to GDELT Through Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07584v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07584v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Audun Myers, Max Vargas, Sinan G. Aksoy, Cliff Joslyn, Benjamin Wilson, Lee Burke, Tom Grimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we study various Retrieval Augmented Regeneration (RAG)
approaches to gain an understanding of the strengths and weaknesses of each
approach in a question-answering analysis. To gain this understanding we use a
case-study subset of the Global Database of Events, Language, and Tone (GDELT)
dataset as well as a corpus of raw text scraped from the online news articles.
To retrieve information from the text corpus we implement a traditional vector
store RAG as well as state-of-the-art large language model (LLM) based
approaches for automatically constructing KGs and retrieving the relevant
subgraphs. In addition to these corpus approaches, we develop a novel
ontology-based framework for constructing knowledge graphs (KGs) from GDELT
directly which leverages the underlying schema of GDELT to create structured
representations of global events. For retrieving relevant information from the
ontology-based KGs we implement both direct graph queries and state-of-the-art
graph retrieval approaches. We compare the performance of each method in a
question-answering task. We find that while our ontology-based KGs are valuable
for question-answering, automated extraction of the relevant subgraphs is
challenging. Conversely, LLM-generated KGs, while capturing event summaries,
often lack consistency and interpretability. Our findings suggest benefits of a
synergistic approach between ontology and LLM-based KG construction, with
proposed avenues toward that end.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIM-SUM: Dynamic IMputation for Smart Utility Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Hildebrant, Rahul Bhope, Sharad Mehrotra, Christopher Tull, Nalini Venkatasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near Data Processing in Taurus Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Lin, Arunprasad P. Marathe, Per-Ȧke Larson, Chong Chen, Calvin Sun, Paul Lee, Weidong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Huawei's cloud-native database system GaussDB for MySQL (also known as
Taurus) stores data in a separate storage layer consisting of a pool of storage
servers. Each server has considerable compute power making it possible to push
data reduction operations (selection, projection, and aggregation) close to
storage. This paper describes the design and implementation of near data
processing (NDP) in Taurus. NDP has several benefits: it reduces the amount of
data shipped over the network; frees up CPU capacity in the compute layer; and
reduces query run time, thereby enabling higher system throughput. Experiments
with the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited
from NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.
On Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU
time by 91 percent; and run time by 80 percent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Higher-Order Graph Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Shriram Chandran, Jakub Cudak, Patrick Iff, Marcin Copik, Robert Gerstenberger, Tomasz Szydlo, Jürgen Müller, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in graph databases (GDBs) have been driving interest in
large-scale analytics, yet current systems fail to support higher-order (HO)
interactions beyond first-order (one-hop) relations, which are crucial for
tasks such as subgraph counting, polyadic modeling, and HO graph learning. We
address this by introducing a new class of systems, higher-order graph
databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly
extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and
OLAP queries, ensuring correctness, scalability, and ACID compliance. We
implement a lightweight, modular, and parallelizable HO-GDB prototype that
offers native support for hypergraphs, node-tuples, subgraphs, and other HO
structures under a unified API. The prototype scales to large HO OLTP & OLAP
workloads and shows how HO improves analytical tasks, for example enhancing
accuracy of graph neural networks within a GDB by 44%. Our work ensures low
latency and high query throughput, and generalizes both ACID-compliant and
eventually consistent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SiriusBI: A Comprehensive <span class="highlight-title">LLM</span>-Powered Solution for Data Analytics in
  Business Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Jiang, Haining Xie,  Siqishen, Yu Shen, Zihan Zhang, Meng Lei, Yifeng Zheng, Yang Li, Chunyou Li, Danqing Huang, Yinjun Wu, Wentao Zhang, Xiaofeng Yang, Bin Cui, Peng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of Large Language Models (LLMs) in Business
Intelligence (BI), existing solutions face critical challenges in industrial
deployments: functionality deficiencies from legacy systems failing to meet
evolving LLM-era user demands, interaction limitations from single-round SQL
generation paradigms inadequate for multi-round clarification, and cost for
domain adaptation arising from cross-domain methods migration.
  We present SiriusBI, a practical LLM-powered BI system addressing the
challenges of industrial deployments through three key innovations: (a) An
end-to-end architecture integrating multi-module coordination to overcome
functionality gaps in legacy systems; (b) A multi-round dialogue with querying
mechanism, consisting of semantic completion, knowledge-guided clarification,
and proactive querying processes, to resolve interaction constraints in SQL
generation; (c) A data-conditioned SQL generation method selection strategy
that supports both an efficient one-step Fine-Tuning approach and a two-step
method leveraging Semantic Intermediate Representation for low-cost
cross-domain applications. Experiments on both real-world datasets and public
benchmarks demonstrate the effectiveness of SiriusBI. User studies further
confirm that SiriusBI enhances both productivity and user experience.
  As an independent service on Tencent's data platform, SiriusBI is deployed
across finance, advertising, and cloud sectors, serving dozens of enterprise
clients. It achieves over 93% accuracy in SQL generation and reduces data
analysts' query time from minutes to seconds in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Autocomplete: Designing CopilotLens Towards Transparent and
  Explainable AI Coding Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runlong Ye, Zeling Zhang, Boushra Almazroua, Michael Liut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-powered code assistants are widely used to generate code completions,
significantly boosting developer productivity. However, these tools typically
present suggestions without explaining their rationale, leaving their
decision-making process inscrutable. This opacity hinders developers' ability
to critically evaluate the output, form accurate mental models, and build
calibrated trust in the system. To address this, we introduce CopilotLens, a
novel interactive framework that reframes code completion from a simple
suggestion into a transparent, explainable event. CopilotLens operates as an
explanation layer that reveals the AI agent's "thought process" through a
dynamic two-level interface, surfacing everything from its reconstructed
high-level plans to the specific codebase context influencing the code. This
paper presents the design and rationale of CopilotLens, offering a concrete
framework for building future agentic code assistants that prioritize clarity
of reasoning over speed of suggestion, thereby fostering deeper comprehension
and more robust human-AI collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dia<span class="highlight-title">LLM</span>s: EHR Enhanced Clinical Conversational System for Clinical Test
  <span class="highlight-title">Recommendation</span> and Diagnosis Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijieying Ren, Tianxiang Zhao, Lei Wang, Tianchun Wang, Vasant Honavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Robotic Exploration and Mapping Using Generative Occupancy Map
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorin Achey, Alec Reed, Brendan Crowe, Bradley Hayes, Christoffer Heckman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach for enhancing robotic exploration by using
generative occupancy mapping. We introduce SceneSense, a diffusion model
designed and trained for predicting 3D occupancy maps given partial
observations. Our proposed approach probabilistically fuses these predictions
into a running occupancy map in real-time, resulting in significant
improvements in map quality and traversability. We implement SceneSense onboard
a quadruped robot and validate its performance with real-world experiments to
demonstrate the effectiveness of the model. In these experiments, we show that
occupancy maps enhanced with SceneSense predictions better represent our fully
observed ground truth data (24.44% FID improvement around the robot and 75.59%
improvement at range). We additionally show that integrating
SceneSense-enhanced maps into our robotic exploration stack as a "drop-in" map
improvement, utilizing an existing off-the-shelf planner, results in
improvements in robustness and traversability time. Finally we show results of
full exploration evaluations with our proposed system in two dissimilar
environments and find that locally enhanced maps provide more consistent
exploration results than maps constructed only from direct sensor measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2409.10681</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GNN's Uncertainty Quantification using Self-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirad Daneshvar, Reza Samavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted in the International Conference on AI in
  Healthcare (AIiH) 2025 and will appear in the conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for
  Evolving Multi-Class Imbalanced Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soheil Abadifard, Fazli Can
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Cross</span>-Layer Discrete Concept Discovery for Interpreting Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Garg, Xuemin Yu, Hassan Sajjad, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Bilateral Team Formation in Cooperative Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koorosh Moslemi, Chi-Guhn Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2nd Coordination and Cooperation in Multi-Agent
  Reinforcement Learning (CoCoMARL) Workshop at RLC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Reinforcement Learning and Value Optimization for
  Challenging Quadruped Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremiah Coholich, Muhammad Ali Murtaza, Seth Hutchinson, Zsolt Kira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Generation of Diverse Courses of Actions for Multi-Agent
  Operations using Binary Optimization and Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithvi Poddar, Ehsan Tarkesh Esfahani, Karthik Dantu, Souma Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elucidated Rolling <span class="highlight-title">Diffusion</span> Models for Probabilistic Weather
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salva Rühling Cachay, Miika Aittala, Karsten Kreis, Noah Brenowitz, Arash Vahdat, Morteza Mardani, Rose Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persona-Assigned <span class="highlight-title">Large Language Model</span>s Exhibit Human-Like Motivated
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saloni Dash, Amélie Reymond, Emma S. Spiro, Aylin Caliskan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Trustworthy Real-Time Decision Support Systems with
  Low-Latency Interpretable AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Deng, Ziwei Liu, Ziqian Bi, Junhao Song, Chia Xin Liang, Joe Yeong, Junfeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Insights on Unfolding and Fine-tuning Quantum Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanika Iroshi Nanayakkara, Shiva Raj Pokhrel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 7 Tables, Submitted to IEEE/ACM journal 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Energy Efficient: Local Retrieval-Augmented Generation
  Models Outperform Commercial <span class="highlight-title">Large Language Model</span>s in Medical Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Vrettos, Michail E. Klontzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QHackBench: Benchmarking <span class="highlight-title">Large Language Model</span>s for Quantum Code
  Generation Using PennyLane Hackathon Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Basit, Minghao Shao, Haider Asif, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables, submitted to QAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRACED: Transition-aware Regret Approximation with Co-learnability for
  Environment Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonwoo Cho, Jaegyun Im, Jihwan Lee, Hojun Yi, Sejin Kim, Sundong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HERCULES: Hierarchical Embedding-based Recursive Clustering Using <span class="highlight-title">LLM</span>s
  for Efficient Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabor Petnehazi, Bernadett Aradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Attribution with Multi-Armed Bandit Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deng Pan, Keerthiram Murugesan, Nuno Moniz, Nitesh Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in
  Deformable Abdominal CT Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhang, Yuxi Zhang, Jiazheng Wang, Xiang Chen, Renjiu Hu, Xin Tian, Gaolei Li, Min Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Neural Networks for Propensity Score Estimation and Survival
  Analysis in Observational Biomedical Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtěch Novák, Ivan Zelinka, Lenka Přibylová, Lubomír Martínek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the application of quantum neural networks (QNNs) for
propensity score estimation to address selection bias in comparing survival
outcomes between laparoscopic and open surgical techniques in a cohort of 1177
colorectal carcinoma patients treated at University Hospital Ostrava
(2001-2009). Using a dataset with 77 variables, including patient demographics
and tumor characteristics, we developed QNN-based propensity score models
focusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture
employed a linear ZFeatureMap for data encoding, a SummedPaulis operator for
predictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
for robust, gradient-free optimization in noisy quantum environments. Variance
regularization was integrated to mitigate quantum measurement noise, with
simulations conducted under exact, sampling (1024 shots), and noisy hardware
(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,
outperformed classical logistic regression and gradient boosted machines in
small samples (AUC up to 0.750 for n=100), with noise modeling enhancing
predictive stability. Propensity score matching and weighting, optimized via
genetic matching and matching weights, achieved covariate balance with
standardized mean differences of 0.0849 and 0.0869, respectively. Survival
analyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen
additive regression revealed no significant survival differences
post-adjustment (p-values 0.287-0.851), indicating confounding bias in
unadjusted outcomes. These results highlight QNNs' potential, enhanced by
CMA-ES and noise-aware strategies, to improve causal inference in biomedical
research, particularly for small-sample, high-dimensional datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference Scaled GraphRAG: Improving Multi Hop Question Answering on
  Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Travis Thompson, Seung-Hwan Lim, Paul Liu, Ruoying He, Dongkuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An ab initio foundation model of wavefunctions that accurately describes
  chemical bond breaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Foster, Zeno Schätzle, P. Bernát Szabó, Lixue Cheng, Jonas Köhler, Gino Cassella, Nicholas Gao, Jiawei Li, Frank Noé, Jan Hermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable description of bond breaking remains a major challenge for quantum
chemistry due to the multireferential character of the electronic structure in
dissociating species. Multireferential methods in particular suffer from large
computational cost, which under the normal paradigm has to be paid anew for
each system at a full price, ignoring commonalities in electronic structure
across molecules. Quantum Monte Carlo with deep neural networks (deep QMC)
uniquely offers to exploit such commonalities by pretraining transferable
wavefunction models, but all such attempts were so far limited in scope. Here,
we bring this new paradigm to fruition with Orbformer, a novel transferable
wavefunction model pretrained on 22,000 equilibrium and dissociating structures
that can be fine-tuned on unseen molecules reaching an accuracy-cost ratio
rivalling classical multireferential methods. On established benchmarks as well
as more challenging bond dissociations and Diels-Alder reactions, Orbformer is
the only method that consistently converges to chemical accuracy (1 kcal/mol).
This work turns the idea of amortizing the cost of solving the Schr\"odinger
equation over many molecules into a practical approach in quantum chemistry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CycleDistill: Bootstrapping Machine Translation using <span class="highlight-title">LLM</span>s with Cyclical
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepon Halder, Thanmay Jayakumar, Raj Dabre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Size-Adaptive Federated Learning: A Comprehensive Framework
  for Heterogeneous <span class="highlight-title">Multi-Modal</span> Data Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajid Hussain, Muhammad Sohail, Nauman Ali Khan, Naima Iltaf, Ihtesham ul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prover Agent: An Agent-based Framework for Formal Mathematical Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaito Baba, Chaoran Liu, Shuhei Kurita, Akiyoshi Sannai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for
  Long Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have enabled high-quality video
generation, but the additional temporal dimension significantly increases
computational costs, making training and inference on long videos prohibitively
expensive. In this paper, we identify a phenomenon we term Spatiotemporal
Energy Decay in video diffusion models: post-softmax attention scores diminish
as spatial and temporal distance between tokens increase, akin to the physical
decay of signal or waves over space and time in nature. Motivated by this, we
propose Radial Attention, a scalable sparse attention mechanism with $O(n \log
n)$ complexity that translates energy decay into exponentially decaying compute
density, which is significantly more efficient than standard $O(n^2)$ dense
attention and more expressive than linear attention. Specifically, Radial
Attention employs a simple, static attention mask where each token attends to
spatially nearby tokens, with the attention window size shrinking with temporal
distance. Moreover, it allows pre-trained video diffusion models to extend
their generation length with efficient LoRA-based fine-tuning. Extensive
experiments show that Radial Attention maintains video quality across
Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup
over the original dense attention. With minimal tuning, it enables video
generation up to 4$\times$ longer while reducing training costs by up to
4.4$\times$ compared to direct fine-tuning and accelerating inference by up to
3.7$\times$ compared to dense attention inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/mit-han-lab/radial-attention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orthogonal Finetuning Made Scalable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Adrian Weller, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation
while preventing catastrophic forgetting, but its high runtime and memory
demands limit practical deployment. We identify the core computational
bottleneck in OFT as its weight-centric implementation, which relies on costly
matrix-matrix multiplications with cubic complexity. To overcome this, we
propose OFTv2, an input-centric reformulation that instead uses matrix-vector
multiplications (i.e., matrix-free computation), reducing the computational
cost to quadratic. We further introduce the Cayley-Neumann parameterization, an
efficient orthogonal parameterization that approximates the matrix inversion in
Cayley transform via a truncated Neumann series. These modifications allow
OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage
without compromising performance. In addition, we extend OFTv2 to support
finetuning quantized foundation models and show that it outperforms the popular
QLoRA in training stability, efficiency, and memory usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report (17 pages, 7 figures, project page:
  https://spherelab.ai/oftv2/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-<span class="highlight-title">LLM</span> Agents
  with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ai Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm
for increasingly complex tasks. However, joint evolution across heterogeneous
agents remains challenging due to cooperative inefficiency and training
instability. In this paper, we propose the joint evolution dynamics for MARL
called JoyAgents-R1, which first applies Group Relative Policy Optimization
(GRPO) to the joint training of heterogeneous multi-agents. By iteratively
refining agents' large language models (LLMs) and memories, the method achieves
holistic equilibrium with optimal decision-making and memory capabilities.
Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on
the behavior of each agent across entire reasoning trajectories to enhance GRPO
sampling efficiency while maintaining policy diversity. Then, our marginal
benefit-driven selection strategy identifies top-$K$ sampling groups with
maximal reward fluctuations, enabling targeted agent model updates that improve
training stability and maximize joint benefits through cost-effective parameter
adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution
mechanism that repurposes GRPO rewards as cost-free supervisory signals to
eliminate repetitive reasoning and accelerate convergence. Experiments across
general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves
performance comparable to that of larger LLMs while built on smaller
open-source models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 7 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guo Li, Zixiang Xu, Wei Zhang, Yikuan Hu, Xinyu Yang, Nikolay Aristov, Mingjie Tang, Elenna R Dugundji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting port congestion is crucial for maintaining reliable global supply
chains. Accurate forecasts enableimprovedshipment planning, reducedelaysand
costs, and optimizeinventoryanddistributionstrategies, thereby ensuring timely
deliveries and enhancing supply chain resilience. To achieve accurate
predictions, analyzing vessel behavior and their stay times at specific port
terminals is essential, focusing particularly on berth scheduling under various
conditions. Crucially, the model must capture and learn the underlying
priorities and patterns of berth scheduling. Berth scheduling and planning are
influenced by a range of factors, including incoming vessel size, waiting
times, and the status of vessels within the port terminal. By observing
historical Automatic Identification System (AIS) positions of vessels, we
reconstruct berth schedules, which are subsequently utilized to determine the
reward function via Inverse Reinforcement Learning (IRL). For this purpose, we
modeled a specific terminal at the Port of New York/New Jersey and developed
Temporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel
sequencing at the terminal and estimate vessel port stay, encompassing both
waiting and berthing times, to forecast port congestion. Utilizing data from
Maher Terminal spanning January 2015 to September 2023, we trained and tested
the model, achieving demonstrably excellent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TRB2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical
  Gaussian World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengbo Yu, Guanxing Lu, Zaijia Yang, Haoyuan Deng, Season Si Chen, Jiwen Lu, Wenbo Ding, Guoqiang Hu, Yansong Tang, Ziwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task robotic bimanual manipulation is becoming increasingly popular as
it enables sophisticated tasks that require diverse dual-arm collaboration
patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to
understanding the multi-body spatiotemporal dynamics. An existing method
ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual
representation via Gaussian world model for single-arm settings, which ignores
the interaction of multiple embodiments for dual-arm systems with significant
performance drop. In this paper, we propose ManiGaussian++, an extension of
ManiGaussian framework that improves multi-task bimanual manipulation by
digesting multi-body scene dynamics through a hierarchical Gaussian world
model. To be specific, we first generate task-oriented Gaussian Splatting from
intermediate visual features, which aims to differentiate acting and
stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build
a hierarchical Gaussian world model with the leader-follower architecture,
where the multi-body spatiotemporal dynamics is mined for intermediate visual
representation via future scene prediction. The leader predicts Gaussian
Splatting deformation caused by motions of the stabilizing arm, through which
the follower generates the physical consequences resulted from the movement of
the acting arm. As a result, our method significantly outperforms the current
state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in
10 simulated tasks, and achieves 60% success rate on average in 9 challenging
real-world tasks. Our code is available at
https://github.com/April-Yz/ManiGaussian_Bimanual.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Progressive Generation with Decomposable Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Ivan Skorokhodov, Arpit Sahni, Sergey Tulyakov, Vicente Ordonez, Aliaksandr Siarohin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-dimensional visual modalities is a computationally intensive
task. A common solution is progressive generation, where the outputs are
synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion
models benefit from the coarse-to-fine nature of denoising, explicit
multi-stage architectures are rarely adopted. These architectures have
increased the complexity of the overall approach, introducing the need for a
custom diffusion formulation, decomposition-dependent stage transitions,
add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow
Matching (DFM), is a simple and effective framework for the progressive
generation of visual media. DFM applies Flow Matching independently at each
level of a user-defined multi-scale representation (such as Laplacian pyramid).
As shown by our experiments, our approach improves visual quality for both
images and videos, featuring superior results compared to prior multistage
frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores
over the base architecture and 26.4% over the best-performing baseline, under
the same training compute. When applied to finetuning of large models, such as
FLUX, DFM shows faster convergence speed to the training distribution.
Crucially, all these advantages are achieved with a single model, architectural
simplicity, and minimal modifications to existing training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://snap-research.github.io/dfm/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A standard <span class="highlight-title">transformer</span> and attention with linear biases for molecular
  conformer generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viatcheslav Gurev, Timothy Rumbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling low-energy molecular conformations, spatial arrangements of atoms in
a molecule, is a critical task for many different calculations performed in the
drug discovery and optimization process. Numerous specialized equivariant
networks have been designed to generate molecular conformations from 2D
molecular graphs. Recently, non-equivariant transformer models have emerged as
a viable alternative due to their capability to scale to improve
generalization. However, the concern has been that non-equivariant models
require a large model size to compensate the lack of equivariant bias. In this
paper, we demonstrate that a well-chosen positional encoding effectively
addresses these size limitations. A standard transformer model incorporating
relative positional encoding for molecular graphs when scaled to 25 million
parameters surpasses the current state-of-the-art non-equivariant base model
with 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative
positional encoding as a negative attention bias that linearly increases with
the shortest path distances between graph nodes at varying slopes for different
attention heads, similar to ALiBi, a widely adopted relative positional
encoding technique in the NLP domain. This architecture has the potential to
serve as a foundation for a novel class of generative models for molecular
conformations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revision of paper at OpenReview:
  https://openreview.net/forum?id=BjjerMYL3F</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Compliance with Visualization Guidelines in Diagrams for
  Scientific Publications Using Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Rückert, Louise Bloch, Christoph M. Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagrams are widely used to visualize data in publications. The research
field of data visualization deals with defining principles and guidelines for
the creation and use of these diagrams, which are often not known or adhered to
by researchers, leading to misinformation caused by providing inaccurate or
incomplete information.
  In this work, large Vision Language Models (VLMs) are used to analyze
diagrams in order to identify potential problems in regards to selected data
visualization principles and guidelines. To determine the suitability of VLMs
for these tasks, five open source VLMs and five prompting strategies are
compared using a set of questions derived from selected data visualization
guidelines.
  The results show that the employed VLMs work well to accurately analyze
diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels
(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score
96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the
image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among
the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting
strategy performs best for most of the experimental questions.
  It is shown that VLMs can be used to automatically identify a number of
potential issues in diagrams, such as missing axes labels, missing legends, and
unnecessary 3D effects. The approach laid out in this work can be extended for
further aspects of data visualization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICDAR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persona Features Control Emergent Misalignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Johannes Heidecke, Tejal Patwardhan, Dan Mossing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how language models generalize behaviors from their training to
a broader deployment distribution is an important problem in AI safety. Betley
et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes
"emergent misalignment," where models give stereotypically malicious responses
to unrelated prompts. We extend this work, demonstrating emergent misalignment
across diverse conditions, including reinforcement learning on reasoning
models, fine-tuning on various synthetic datasets, and in models without safety
training. To investigate the mechanisms behind this generalized misalignment,
we apply a "model diffing" approach using sparse autoencoders to compare
internal model representations before and after fine-tuning. This approach
reveals several "misaligned persona" features in activation space, including a
toxic persona feature which most strongly controls emergent misalignment and
can be used to predict whether a model will exhibit such behavior.
Additionally, we investigate mitigation strategies, discovering that
fine-tuning an emergently misaligned model on just a few hundred benign samples
efficiently restores alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global and Local Contrastive Learning for Joint Representations from
  Cardiac MRI and ECG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Selivanov, Philip Müller, Özgün Turgut, Nil Stolt-Ansó, Daniel Rückert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to MICCAI 2025 (Springer LNCS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Do Open-Source <span class="highlight-title">LLM</span>s Struggle with Data Analysis? A Systematic
  Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold promise in automating data analysis tasks,
yet open-source models face significant limitations in these kinds of
reasoning-intensive scenarios. In this work, we investigate strategies to
enhance the data analysis capabilities of open-source LLMs. By curating a seed
dataset of diverse, realistic scenarios, we evaluate models across three
dimensions: data understanding, code generation, and strategic planning. Our
analysis reveals three key findings: (1) Strategic planning quality serves as
the primary determinant of model performance; (2) Interaction design and task
complexity significantly influence reasoning capabilities; (3) Data quality
demonstrates a greater impact than diversity in achieving optimal performance.
We leverage these insights to develop a data synthesis methodology,
demonstrating significant improvements in open-source LLMs' analytical
reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Task Belief Similarity with Latent Dynamics for
  Meta-Reinforcement Learning <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menglong Zhang, Fuyuan Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-reinforcement learning requires utilizing prior task distribution
information obtained during exploration to rapidly adapt to unknown tasks. The
efficiency of an agent's exploration hinges on accurately identifying the
current task. Recent Bayes-Adaptive Deep RL approaches often rely on
reconstructing the environment's reward signal, which is challenging in sparse
reward settings, leading to suboptimal exploitation. Inspired by bisimulation
metrics, which robustly extracts behavioral similarity in continuous MDPs, we
propose SimBelief-a novel meta-RL framework via measuring similarity of task
belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common
features of similar task distributions, enabling efficient task identification
and exploration in sparse reward environments. We introduce latent task belief
metric to learn the common structure of similar tasks and incorporate it into
the specific task belief. By learning the latent dynamics across task
distributions, we connect shared latent task belief features with specific task
features, facilitating rapid task identification and adaptation. Our method
outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2025 https://openreview.net/forum?id=5YbuOTUFQ4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAGE: Strategy-Adaptive Generation Engine for Query Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Wang, Hailei Gong, Changwang Zhang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewriting is pivotal for enhancing dense retrieval, yet current methods
demand large-scale supervised data or suffer from inefficient reinforcement
learning (RL) exploration. In this work, we first establish that guiding Large
Language Models (LLMs) with a concise set of expert-crafted strategies, such as
semantic expansion and entity disambiguation, substantially improves retrieval
effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,
and SciFact. Building on this insight, we introduce the Strategy-Adaptive
Generation Engine (SAGE), which operationalizes these strategies in an RL
framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit
Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative
learning signals. This strategy-guided approach not only achieves new
state-of-the-art NDCG@10 results, but also uncovers a compelling emergent
behavior: the agent learns to select optimal strategies, reduces unnecessary
exploration, and generates concise rewrites, lowering inference cost without
sacrificing performance. Our findings demonstrate that strategy-guided RL,
enhanced with nuanced reward shaping, offers a scalable, efficient, and more
interpretable paradigm for developing the next generation of robust information
retrieval systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alleviating User-Sensitive bias with Fair Generative Sequential
  <span class="highlight-title">Recommendation</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Feng Wu, Xuefang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation fairness has recently attracted much attention. In the real
world, recommendation systems are driven by user behavior, and since users with
the same sensitive feature (e.g., gender and age) tend to have the same
patterns, recommendation models can easily capture the strong correlation
preference of sensitive features and thus cause recommendation unfairness.
Diffusion model (DM) as a new generative model paradigm has achieved great
success in recommendation systems. DM's ability to model uncertainty and
represent diversity, and its modeling mechanism has a high degree of
adaptability with the real-world recommendation process with bias. Therefore,
we use DM to effectively model the fairness of recommendation and enhance the
diversity. This paper proposes a FairGENerative sequential Recommendation model
based on DM, FairGENRec. In the training phase, we inject random noise into the
original distribution under the guidance of the sensitive feature recognition
model, and a sequential denoise model is designed for the reverse
reconstruction of items. Simultaneously, recommendation fairness modeling is
completed by injecting multi-interests representational information that
eliminates the bias of sensitive user features into the generated results. In
the inference phase, the model obtains the noise in the form of noise addition
by using the history interactions which is followed by reverse iteration to
reconstruct the target item representation. Finally, our extensive experiments
on three datasets demonstrate the dual enhancement effect of FairGENRec on
accuracy and fairness, while the statistical analysis of the cases visualizes
the degree of improvement on the fairness of the recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kling-Foley: Multimodal <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span> for High-Quality
  Video-to-Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Xijuan Zeng, Chunyu Qiang, Ruilong Chen, Shiyao Wang, Le Wang, Wangjing Zhou, Pengfei Cai, Jiahui Zhao, Nan Li, Zihan Li, Yuzhe Liang, Xiaopeng Wang, Haorui Zheng, Ming Wen, Kang Yin, Yiran Wang, Nan Li, Feng Deng, Liang Dong, Chen Zhang, Di Zhang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation
model that synthesizes high-quality audio synchronized with video content. In
Kling-Foley, we introduce multimodal diffusion transformers to model the
interactions between video, audio, and text modalities, and combine it with a
visual semantic representation module and an audio-visual synchronization
module to enhance alignment capabilities. Specifically, these modules align
video conditions with latent audio elements at the frame level, thereby
improving semantic alignment and audio-visual synchronization. Together with
text conditions, this integrated approach enables precise generation of
video-matching sound effects. In addition, we propose a universal latent audio
codec that can achieve high-quality modeling in various scenarios such as sound
effects, speech, singing, and music. We employ a stereo rendering method that
imbues synthesized audio with a spatial presence. At the same time, in order to
make up for the incomplete types and annotations of the open-source benchmark,
we also open-source an industrial-level benchmark Kling-Audio-Eval. Our
experiments show that Kling-Foley trained with the flow matching objective
achieves new audio-visual SOTA performance among public models in terms of
distribution matching, semantic alignment, temporal alignment and audio
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Prompt Optimization for Knowledge Graph Construction: Insights
  from an Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandana Mihindukulasooriya, Niharika S. D'Souza, Faisal Chowdhury, Horst Samulowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A KG represents a network of entities and illustrates relationships between
them. KGs are used for various applications, including semantic search and
discovery, reasoning, decision-making, natural language processing, machine
learning, and recommendation systems. Triple (subject-relation-object)
extraction from text is the fundamental building block of KG construction and
has been widely studied, for example, in early benchmarks such as ACE 2002 to
more recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs
is explored for KG construction, handcrafting reasonable task-specific prompts
for LLMs is a labour-intensive exercise and can be brittle due to subtle
changes in the LLM models employed. Recent work in NLP tasks (e.g. autonomy
generation) uses automatic prompt optimization/engineering to address this
challenge by generating optimal or near-optimal task-specific prompts given
input-output examples.
  This empirical study explores the application of automatic prompt
optimization for the triple extraction task using experimental benchmarking. We
evaluate different settings by changing (a) the prompting strategy, (b) the LLM
being used for prompt optimization and task execution, (c) the number of
canonical relations in the schema (schema complexity), (d) the length and
diversity of input text, (e) the metric used to drive the prompt optimization,
and (f) the dataset being used for training and testing. We evaluate three
different automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use
two different triple extraction datasets, SynthIE and REBEL. Through rigorous
empirical evaluation, our main contribution highlights that automatic prompt
optimization techniques can generate reasonable prompts similar to humans for
triple extraction. In turn, these optimized prompts achieve improved results,
particularly with increasing schema complexity and text size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Multi-sensor Fusion Perception for Embodied AI: Background,
  Methods, Challenges and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulan Ruan, Rongwei Wang, Xuchen Shen, Huijie Liu, Baihui Xiao, Jun Shi, Kun Zhang, Zhenya Huang, Yu Liu, Enhong Chen, You He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,
which can serve a variety of downstream tasks (e.g., 3D object detection and
semantic segmentation) and application scenarios (e.g., autonomous driving and
swarm robotics). Recently, impressive achievements on AI-based MSFP methods
have been reviewed in relevant surveys. However, we observe that the existing
surveys have some limitations after a rigorous and detailed investigation. For
one thing, most surveys are oriented to a single task or research field, such
as 3D object detection or autonomous driving. Therefore, researchers in other
related tasks often find it difficult to benefit directly. For another, most
surveys only introduce MSFP from a single perspective of multi-modal fusion,
while lacking consideration of the diversity of MSFP methods, such as
multi-view fusion and time-series fusion. To this end, in this paper, we hope
to organize MSFP research from a task-agnostic perspective, where methods are
reported from various technical views. Specifically, we first introduce the
background of MSFP. Next, we review multi-modal and multi-agent fusion methods.
A step further, time-series fusion methods are analyzed. In the era of LLM, we
also investigate multimodal LLM fusion methods. Finally, we discuss open
challenges and future directions for MSFP. We hope this survey can help
researchers understand the important progress in MSFP and provide possible
insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRFT: A Single-Stage Method with Supervised and Reinforcement
  Fine-Tuning for Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Cross</span>-regularization: Adaptive Model Complexity through Validation
  Gradients <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Stein Brito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model regularization requires extensive manual tuning to balance complexity
against overfitting. Cross-regularization resolves this tradeoff by directly
adapting regularization parameters through validation gradients during
training. The method splits parameter optimization - training data guides
feature learning while validation data shapes complexity controls - converging
provably to cross-validation optima. When implemented through noise injection
in neural networks, this approach reveals striking patterns: unexpectedly high
noise tolerance and architecture-specific regularization that emerges
organically during training. Beyond complexity control, the framework
integrates seamlessly with data augmentation, uncertainty calibration and
growing datasets while maintaining single-run efficiency through a simple
gradient-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 13 figures. Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arabic Dialect Classification using RNNs, <span class="highlight-title">Transformer</span>s, and Large
  Language Models: A Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar A. Essameldin, Ali O. Elbeih, Wael H. Gomaa, Wael F. Elsersy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Arabic language is among the most popular languages in the world with a
huge variety of dialects spoken in 22 countries. In this study, we address the
problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.
RNN models, Transformer models, and large language models (LLMs) via prompt
engineering are created and tested. Among these, MARBERTv2 performed best with
65% accuracy and 64% F1-score. Through the use of state-of-the-art
preprocessing techniques and the latest NLP models, this paper identifies the
most significant linguistic issues in Arabic dialect identification. The
results corroborate applications like personalized chatbots that respond in
users' dialects, social media monitoring, and greater accessibility for Arabic
communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-based CBCT Reconstruction needs Normalization and Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowei Xu, Han Li, Dai Sun, Zhicheng Li, Yujia Li, Qingpeng Kong, Zhiwei Cheng, Nassir Navab, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cone Beam Computed Tomography (CBCT) is widely used in medical imaging.
However, the limited number and intensity of X-ray projections make
reconstruction an ill-posed problem with severe artifacts. NeRF-based methods
have achieved great success in this task. However, they suffer from a
local-global training mismatch between their two key components: the hash
encoder and the neural network. Specifically, in each training step, only a
subset of the hash encoder's parameters is used (local sparse), whereas all
parameters in the neural network participate (global dense). Consequently, hash
features generated in each step are highly misaligned, as they come from
different subsets of the hash encoder. These misalignments from different
training steps are then fed into the neural network, causing repeated
inconsistent global updates in training, which leads to unstable training,
slower convergence, and degraded reconstruction quality. Aiming to alleviate
the impact of this local-global optimization mismatch, we introduce a
Normalized Hash Encoder, which enhances feature consistency and mitigates the
mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI)
strategy that initializes the neural network before training by leveraging the
global mapping property from a well-trained model. The initialized neural
network exhibits improved stability during early training, enabling faster
convergence and enhanced reconstruction performance. Our method is simple yet
effective, requiring only a few lines of code while substantially improving
training efficiency on 128 CT cases collected from 4 different datasets,
covering 7 distinct anatomical regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who Does What in Deep Learning? Multidimensional Game-Theoretic
  Attribution of Function of Neural Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrey Dixit, Kayson Fakhar, Fatemeh Hadaeghi, Patrick Mineault, Konrad P. Kording, Claus C. Hilgetag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks now generate text, images, and speech with billions of
parameters, producing a need to know how each neural unit contributes to these
high-dimensional outputs. Existing explainable-AI methods, such as SHAP,
attribute importance to inputs, but cannot quantify the contributions of neural
units across thousands of output pixels, tokens, or logits. Here we close that
gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic
game-theoretic framework. By systematically lesioning combinations of units,
MSA yields Shapley Modes, unit-wise contribution maps that share the exact
dimensionality of the model's output. We apply MSA across scales, from
multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative
Adversarial Networks (GAN). The approach demonstrates how regularisation
concentrates computation in a few hubs, exposes language-specific experts
inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs.
Together, these results showcase MSA as a powerful approach for interpreting,
editing, and compressing deep neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric-Aware Variational Inference: Robust and Adaptive
  Regularization with Directional Weight Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Stein Brito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks require principled uncertainty quantification, yet
existing variational inference methods often employ isotropic Gaussian
approximations in weight space that poorly match the network's inherent
geometry. We address this mismatch by introducing Concentration-Adapted
Perturbations (CAP), a variational framework that models weight uncertainties
directly on the unit hypersphere using von Mises-Fisher distributions. Building
on recent work in radial-directional posterior decompositions and spherical
weight constraints, CAP provides the first complete theoretical framework
connecting directional statistics to practical noise regularization in neural
networks. Our key contribution is an analytical derivation linking vMF
concentration parameters to activation noise variance, enabling each layer to
learn its optimal uncertainty level through a novel closed-form KL divergence
regularizer. In experiments on CIFAR-10, CAP significantly improves model
calibration - reducing Expected Calibration Error by 5.6x - while providing
interpretable layer-wise uncertainty profiles. CAP requires minimal
computational overhead and integrates seamlessly into standard architectures,
offering a theoretically grounded yet practical approach to uncertainty
quantification in deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Reproduction to Replication: Evaluating Research Agents with
  Progressive Code Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongwon James Kim, Alex Wilf, Louis-Philippe Morency, Daniel Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in autonomous code generation has fueled excitement around AI
agents capable of accelerating scientific discovery by running experiments.
However, there is currently no benchmark that evaluates whether such agents can
implement scientific ideas when given varied amounts of code as a starting
point, interpolating between reproduction (running code) and from-scratch
replication (fully re-implementing and running code). We introduce
AutoExperiment, a benchmark that evaluates AI agents' ability to implement and
run machine learning experiments based on natural language descriptions in
research papers. In each task, agents are given a research paper, a codebase
with key functions masked out, and a command to run the experiment. The goal is
to generate the missing code, execute the experiment in a sandboxed
environment, and reproduce the results. AutoExperiment scales in difficulty by
varying the number of missing functions $n$, ranging from partial reproduction
to full replication. We evaluate state-of-the-art agents and find that
performance degrades rapidly as $n$ increases. Agents that can dynamically
interact with the environment (e.g. to debug their code) can outperform agents
in fixed "agentless" harnesses, and there exists a significant gap between
single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating
verifier approaches to our benchmark. Our findings highlight critical
challenges in long-horizon code generation, context retrieval, and autonomous
experiment execution, establishing AutoExperiment as a new benchmark for
evaluating progress in AI-driven scientific experimentation. Our data and code
are open-sourced at https://github.com/j1mk1m/AutoExperiment .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Conceptual Blindspots in Generative Image Models Using Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matyas Bohacek, Thomas Fel, Maneesh Agrawala, Ekdeep Singh Lubana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive performance, generative image models trained on
large-scale datasets frequently fail to produce images with seemingly simple
concepts -- e.g., human hands or objects appearing in groups of four -- that
are reasonably expected to appear in the training data. These failure modes
have largely been documented anecdotally, leaving open the question of whether
they reflect idiosyncratic anomalies or more structural limitations of these
models. To address this, we introduce a systematic approach for identifying and
characterizing "conceptual blindspots" -- concepts present in the training data
but absent or misrepresented in a model's generations. Our method leverages
sparse autoencoders (SAEs) to extract interpretable concept embeddings,
enabling a quantitative comparison of concept prevalence between real and
generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with
32,000 concepts -- the largest such SAE to date -- enabling fine-grained
analysis of conceptual disparities. Applied to four popular generative models
(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals
specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces
on documents) and exaggerated blindspots (e.g., wood background texture and
palm trees). At the individual datapoint level, we further isolate memorization
artifacts -- instances where models reproduce highly specific visual templates
seen during training. Overall, we propose a theoretically grounded framework
for systematically identifying conceptual blindspots in generative models by
assessing their conceptual fidelity with respect to the underlying
data-generating process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Driven Medical Document Analysis: Enhancing Trustworthy Pathology
  and Differential Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Kang, Xuanshuo Fu, Oriol Ramos Terrades, Javier Vazquez-Corral, Ernest Valveny, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical document analysis plays a crucial role in extracting essential
clinical insights from unstructured healthcare records, supporting critical
tasks such as differential diagnosis. Determining the most probable condition
among overlapping symptoms requires precise evaluation and deep medical
expertise. While recent advancements in large language models (LLMs) have
significantly enhanced performance in medical document analysis, privacy
concerns related to sensitive patient data limit the use of online LLMs
services in clinical settings. To address these challenges, we propose a
trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using
low-rank adaptation, specifically optimized for differential diagnosis tasks.
Our approach utilizes DDXPlus, the largest benchmark dataset for differential
diagnosis, and demonstrates superior performance in pathology prediction and
variable-length differential diagnosis compared to existing methods. The
developed web-based platform allows users to submit their own unstructured
medical documents and receive accurate, explainable diagnostic results. By
incorporating advanced explainability techniques, the system ensures
transparent and reliable predictions, fostering user trust and confidence.
Extensive evaluations confirm that the proposed method surpasses current
state-of-the-art models in predictive accuracy while offering practical utility
in clinical settings. This work addresses the urgent need for reliable,
explainable, and privacy-preserving artificial intelligence solutions,
representing a significant advancement in intelligent medical document analysis
for real-world healthcare applications. The code can be found at
\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICDAR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize
  Framework for Predictive Maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuojun Xie, Adam Abdin, Yiping Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research increasingly integrates machine learning (ML) into predictive
maintenance (PdM) to reduce operational and maintenance costs in data-rich
operational settings. However, uncertainty due to model misspecification
continues to limit widespread industrial adoption. This paper proposes a PdM
framework in which sensor-driven prognostics inform decision-making under
economic trade-offs within a finite decision space. We investigate two key
questions: (1) Does higher predictive accuracy necessarily lead to better
maintenance decisions? (2) If not, how can the impact of prediction errors on
downstream maintenance decisions be mitigated? We first demonstrate that in the
traditional estimate-then-optimize (ETO) framework, errors in probabilistic
prediction can result in inconsistent and suboptimal maintenance decisions. To
address this, we propose an integrated estimate-optimize (IEO) framework that
jointly tunes predictive models while directly optimizing for maintenance
outcomes. We establish theoretical finite-sample guarantees on decision
consistency under standard assumptions. Specifically, we develop a stochastic
perturbation gradient descent algorithm suitable for small run-to-failure
datasets. Empirical evaluations on a turbofan maintenance case study show that
the IEO framework reduces average maintenance regret up to 22% compared to ETO.
This study provides a principled approach to managing prediction errors in
data-driven PdM. By aligning prognostic model training with maintenance
objectives, the IEO framework improves robustness under model misspecification
and improves decision quality. The improvement is particularly pronounced when
the decision-making policy is misaligned with the decision-maker's target.
These findings support more reliable maintenance planning in uncertain
operational environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme activation outliers in Large Language Models (LLMs) critically
degrade quantization performance, hindering efficient on-device deployment.
While channel-wise operations and adaptive gradient scaling are recognized
causes, practical mitigation remains challenging. We introduce Outlier-Safe
Pre-Training (OSP), a practical guideline that proactively prevents outlier
formation rather than relying on post-hoc mitigation. OSP combines three key
innovations: (1) the Muon optimizer, eliminating privileged bases while
maintaining training efficiency; (2) Single-Scale RMSNorm, preventing
channel-wise amplification; and (3) a learnable embedding projection,
redistributing activation magnitudes originating from embedding matrices. We
validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is
the first production-scale LLM trained without such outliers. Under aggressive
4-bit quantization, our OSP model achieves a 35.7 average score across 10
benchmarks (compared to 26.5 for an Adam-trained model), with only a 2%
training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis
(0.04) compared to extreme values (1818.56) in standard models, fundamentally
altering LLM quantization behavior. Our work demonstrates that outliers are not
inherent to LLMs but are consequences of training strategies, paving the way
for more efficient LLM deployment. The source code and pretrained checkpoints
are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Can We Reuse a Calibration Set for Multiple Conformal Predictions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. A. Balinsky, A. D. Balinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable uncertainty quantification is crucial for the trustworthiness of
machine learning applications. Inductive Conformal Prediction (ICP) offers a
distribution-free framework for generating prediction sets or intervals with
user-specified confidence. However, standard ICP guarantees are marginal and
typically require a fresh calibration set for each new prediction to maintain
their validity. This paper addresses this practical limitation by demonstrating
how e-conformal prediction, in conjunction with Hoeffding's inequality, can
enable the repeated use of a single calibration set with a high probability of
preserving the desired coverage. Through a case study on the CIFAR-10 dataset,
we train a deep neural network and utilise a calibration set to estimate a
Hoeffding correction. This correction allows us to apply a modified Markov's
inequality, leading to the construction of prediction sets with quantifiable
confidence. Our results illustrate the feasibility of maintaining provable
performance in conformal prediction while enhancing its practicality by
reducing the need for repeated calibration. The code for this work is publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailored Conversations beyond <span class="highlight-title">LLM</span>s: A RL-Based Dialogue Manager 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucie Galland, Catherine Pelachaud, Florian Pecune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel framework that integrates large language
models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a
specific goal. By leveraging hierarchical reinforcement learning to model the
structured phases of dialogue and employ meta-learning to enhance adaptability
across diverse user profiles, our approach enhances adaptability and
efficiency, enabling the system to learn from limited data, transition fluidly
between dialogue phases, and personalize responses to heterogeneous patient
needs. We apply our framework to Motivational Interviews, aiming to foster
behavior change, and demonstrate that the proposed dialogue manager outperforms
a state-of-the-art LLM baseline in terms of reward, showing a potential benefit
of conditioning LLMs to create open-ended dialogue systems with specific goals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Macro Causal Effects in C-DMGs over DMGs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Ferreira, Charles K. Assaad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The do-calculus is a sound and complete tool for identifying causal effects
in acyclic directed mixed graphs (ADMGs) induced by structural causal models
(SCMs). However, in many real-world applications, especially in
high-dimensional setting, constructing a fully specified ADMG is often
infeasible. This limitation has led to growing interest in partially specified
causal representations, particularly through cluster-directed mixed graphs
(C-DMGs), which group variables into clusters and offer a more abstract yet
practical view of causal dependencies. While these representations can include
cycles, recent work has shown that the do-calculus remains sound and complete
for identifying macro-level causal effects in C-DMGs over ADMGs under the
assumption that all clusters size are greater than 1. Nevertheless, real-world
systems often exhibit cyclic causal dynamics at the structural level. To
account for this, input-output structural causal models (ioSCMs) have been
introduced as a generalization of SCMs that allow for cycles. ioSCMs induce
another type of graph structure known as a directed mixed graph (DMG).
Analogous to the ADMG setting, one can define C-DMGs over DMGs as high-level
representations of causal relations among clusters of variables. In this paper,
we prove that, unlike in the ADMG setting, the do-calculus is unconditionally
sound and complete for identifying macro causal effects in C-DMGs over DMGs.
Furthermore, we show that the graphical criteria for non-identifiability of
macro causal effects previously established C-DMGs over ADMGs naturally extends
to a subset of C-DMGs over DMGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the UAI2025 workshop on Causal Abstractions and
  Representations. arXiv admin note: substantial text overlap with
  arXiv:2504.01551</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The receptron is a nonlinear threshold logic gate with intrinsic
  multi-dimensional selective capabilities for analog inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        B. Paroli, F. Borghi, M. A. C. Potenza, P. Milani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Threshold logic gates (TLGs) have been proposed as artificial counterparts of
biological neurons with classification capabilities based on a linear predictor
function combining a set of weights with the feature vector. The linearity of
TLGs limits their classification capabilities requiring the use of networks for
the accomplishment of complex tasks. A generalization of the TLG model called
receptron, characterized by input-dependent weight functions allows for a
significant enhancement of classification performances even with the use of a
single unit. Here we formally demonstrate that a receptron, characterized by
nonlinear input-dependent weight functions, exhibit intrinsic selective
activation properties for analog inputs, when the input vector is within cubic
domains in a 3D space. The proposed model can be extended to the n-dimensional
case for multidimensional applications. Our results suggest that
receptron-based networks can represent a new class of devices capable to manage
a large number of analog inputs, for edge applications requiring high
selectivity and classification capabilities without the burden of complex
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the efficacy of old features for the detection of new bots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rocco De Nicola, Marinella Petrocchi, Manuel Pratelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For more than a decade now, academicians and online platform administrators
have been studying solutions to the problem of bot detection. Bots are computer
algorithms whose use is far from being benign: malicious bots are purposely
created to distribute spam, sponsor public characters and, ultimately, induce a
bias within the public opinion. To fight the bot invasion on our online
ecosystem, several approaches have been implemented, mostly based on
(supervised and unsupervised) classifiers, which adopt the most varied account
features, from the simplest to the most expensive ones to be extracted from the
raw data obtainable through the Twitter public APIs. In this exploratory study,
using Twitter as a benchmark, we compare the performances of four state-of-art
feature sets in detecting novel bots: one of the output scores of the popular
bot detector Botometer, which considers more than 1,000 features of an account
to take a decision; two feature sets based on the account profile and timeline;
and the information about the Twitter client from which the user tweets. The
results of our analysis, conducted on six recently released datasets of Twitter
accounts, hint at the possible use of general-purpose classifiers and
cheap-to-compute account features for the detection of evolved bots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>pre-print version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Time Series Forecasting Via Latent Mean Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Salatiello, Stefan Birr, Manuel Kunz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coherently forecasting the behaviour of a target variable across both coarse
and fine temporal scales is crucial for profit-optimized decision-making in
several business applications, and remains an open research problem in temporal
hierarchical forecasting. Here, we propose a new hierarchical architecture that
tackles this problem by leveraging modules that specialize in forecasting the
different temporal aggregation levels of interest. The architecture, which
learns to encode the average behaviour of the target variable within its hidden
layers, makes accurate and coherent forecasts across the target temporal
hierarchies. We validate our architecture on the challenging, real-world M5
dataset and show that it outperforms established methods, such as the TSMixer
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Uncertainty Calibration Matters for Reliable Perturbation-based
  Explanations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Decker, Volker Tresp, Florian Buettner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perturbation-based explanations are widely utilized to enhance the
transparency of modern machine-learning models. However, their reliability is
often compromised by the unknown model behavior under the specific
perturbations used. This paper investigates the relationship between
uncertainty calibration - the alignment of model confidence with actual
accuracy - and perturbation-based explanations. We show that models frequently
produce unreliable probability estimates when subjected to
explainability-specific perturbations and theoretically prove that this
directly undermines explanation quality. To address this, we introduce ReCalX,
a novel approach to recalibrate models for improved perturbation-based
explanations while preserving their original predictions. Experiments on
popular computer vision models demonstrate that our calibration strategy
produces explanations that are more aligned with human perception and actual
object locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Workshop: XAI4Science: From Understanding Model Behavior to
  Discovering New Scientific Knowledge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoPCDNet: Video Parsing and Prediction with Phase Correlation
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noel José Rodrigues Vicente, Enrique Lehner, Angel Villar-Corrales, Jan Nogga, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and predicting video content is essential for planning and
reasoning in dynamic environments. Despite advancements, unsupervised learning
of object representations and dynamics remains challenging. We present
VideoPCDNet, an unsupervised framework for object-centric video decomposition
and prediction. Our model uses frequency-domain phase correlation techniques to
recursively parse videos into object components, which are represented as
transformed versions of learned object prototypes, enabling accurate and
interpretable tracking. By explicitly modeling object motion through a
combination of frequency domain operations and lightweight learned modules,
VideoPCDNet enables accurate unsupervised object tracking and prediction of
future video frames. In our experiments, we demonstrate that VideoPCDNet
outperforms multiple object-centric baseline models for unsupervised tracking
and prediction on several synthetic datasets, while learning interpretable
object and motion representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication at ICANN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position: Intelligent Science Laboratory Requires the Integration of
  Cognitive and Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sha Zhang, Suorong Yang, Tong Xie, Xiangyuan Xue, Zixuan Hu, Rui Li, Wenxi Qu, Zhenfei Yin, Tianfan Fu, Di Hu, Andres M Bran, Nian Ran, Bram Hoex, Wangmeng Zuo, Philippe Schwaller, Wanli Ouyang, Lei Bai, Yanyong Zhang, Lingyu Duan, Shixiang Tang, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery has long been constrained by human limitations in
expertise, physical capability, and sleep cycles. The recent rise of AI
scientists and automated laboratories has accelerated both the cognitive and
operational aspects of research. However, key limitations persist: AI systems
are often confined to virtual environments, while automated laboratories lack
the flexibility and autonomy to adaptively test new hypotheses in the physical
world. Recent advances in embodied AI, such as generalist robot foundation
models, diffusion-based action policies, fine-grained manipulation learning,
and sim-to-real transfer, highlight the promise of integrating cognitive and
embodied intelligence. This convergence opens the door to closed-loop systems
that support iterative, autonomous experimentation and the possibility of
serendipitous discovery. In this position paper, we propose the paradigm of
Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework
that deeply integrates cognitive and embodied intelligence. ISLs unify
foundation models for scientific reasoning, agent-based workflow orchestration,
and embodied agents for robust physical experimentation. We argue that such
systems are essential for overcoming the current limitations of scientific
discovery and for realizing the full transformative potential of AI-driven
science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordPrompt: Orchestrating <span class="highlight-title">Cross</span>-Modal Prompt Synergy for Multi-<span class="highlight-title">Domain</span>
  Incremental Learning in CLIP <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Wang, Bokui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) empowers pre-trained vision-language models to adapt
effectively to novel or previously underrepresented data distributions without
comprehensive retraining, enhancing their adaptability and efficiency. While
vision-language models like CLIP show great promise, they struggle to maintain
performance across domains in incremental learning scenarios. Existing prompt
learning methods face two main limitations: 1) they primarily focus on
class-incremental learning scenarios, lacking specific strategies for
multi-domain task incremental learning; 2) most current approaches employ
single-modal prompts, neglecting the potential benefits of cross-modal
information exchange. To address these challenges, we propose the \ChordPrompt
framework, which facilitates a harmonious interplay between visual and textual
prompts. \ChordPrompt introduces cross-modal prompts to leverage interactions
between visual and textual information. Our approach also employs
domain-adaptive text prompts to select appropriate prompts for continual
adaptation across multiple domains. Comprehensive experiments on multi-domain
incremental learning benchmarks demonstrate that \ChordPrompt outperforms
state-of-the-art methods in zero-shot generalization and downstream task
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by ECML-PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECCoT: A Framework for Enhancing Effective Cognition via Chain of
  Thought in <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenke Duan, Jiqun Pan, Jiani Tu, Xiaoyi Wang, Yanqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large-scale artificial intelligence, Large Language Models
(LLMs) have made significant strides in natural language processing. However,
they often lack transparency and generate unreliable outputs, raising concerns
about their interpretability. To address this, the Chain of Thought (CoT)
prompting method structures reasoning into step-by-step deductions. Yet, not
all reasoning chains are valid, and errors can lead to unreliable conclusions.
We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation
Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates
the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT
generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By
filtering ineffective chains using structured ordering statistics, ECCoT
improves interpretability, reduces biases, and enhances the trustworthiness of
LLM-based decision-making. Key contributions include the introduction of ECCoT,
MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning
enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotics Under Construction: Challenges on Job Sites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haruki Uchiito, Akhilesh Bhat, Koji Kusaka, Xiaoya Zhang, Hiraku Kinjo, Honoka Uehara, Motoki Koyama, Shinji Natsume
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As labor shortages and productivity stagnation increasingly challenge the
construction industry, automation has become essential for sustainable
infrastructure development. This paper presents an autonomous payload
transportation system as an initial step toward fully unmanned construction
sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous
navigation, fleet management, and GNSS-based localization to facilitate
material transport in construction site environments. While the current system
does not yet incorporate dynamic environment adaptation algorithms, we have
begun fundamental investigations into external-sensor based perception and
mapping system. Preliminary results highlight the potential challenges,
including navigation in evolving terrain, environmental perception under
construction-specific conditions, and sensor placement optimization for
improving autonomy and efficiency. Looking forward, we envision a construction
ecosystem where collaborative autonomous agents dynamically adapt to site
conditions, optimizing workflow and reducing human intervention. This paper
provides foundational insights into the future of robotics-driven construction
automation and identifies critical areas for further technological development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Field Robotics, ICRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">LLM</span>s Replace Humans During Code Chunking? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Glasz, Emily Escamilla, Eric O. Scott, Anand Patel, Jacob Zimmer, Colin Diggs, Michael Doyle, Scott Rosen, Nitin Naik, Justin F. Brunelle, Samruddhi Thaker, Parthav Poudel, Arun Sridharan, Amit Madan, Doug Wendt, William Macke, Thomas Schill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Domain</span> Modeling with Language Models: A Multi-Agent Approach to
  Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harisankar Babu, Philipp Schillinger, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a
multi-agent framework that integrates Large Language Models (LLMs) with
symbolic planning to solve complex tasks without the need for manually defined
environment models. TAPAS employs specialized LLM-based agents that
collaboratively generate and adapt domain models, initial states, and goal
specifications as needed using structured tool-calling mechanisms. Through this
tool-based interaction, downstream agents can request modifications from
upstream agents, enabling adaptation to novel attributes and constraints
without manual domain redefinition. A ReAct (Reason+Act)-style execution agent,
coupled with natural language plan translation, bridges the gap between
dynamically generated plans and real-world robot capabilities. TAPAS
demonstrates strong performance in benchmark planning domains and in the
VirtualHome simulated real-world environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Transformer</span>-Based Time-Series Image Reconstruction for
  Cloud-Filling Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lujun Li, Yiqun Wang, Radu State
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud cover in multispectral imagery (MSI) poses significant challenges for
early season crop mapping, as it leads to missing or corrupted spectral
information. Synthetic aperture radar (SAR) data, which is not affected by
cloud interference, offers a complementary solution, but lack sufficient
spectral detail for precise crop mapping. To address this, we propose a novel
framework, Time-series MSI Image Reconstruction using Vision Transformer (ViT),
to reconstruct MSI data in cloud-covered regions by leveraging the temporal
coherence of MSI and the complementary information from SAR from the attention
mechanism. Comprehensive experiments, using rigorous reconstruction evaluation
metrics, demonstrate that Time-series ViT framework significantly outperforms
baselines that use non-time-series MSI and SAR or time-series MSI without SAR,
effectively enhancing MSI image reconstruction in cloud-covered regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted as a conference paper at the 2025 IEEE
  International Geoscience and Remote Sensing Symposium (IGARSS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language
  Models on Real and 3D-Printed Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Tavella, Kathryn Mearns, Angelo Cangelosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic scene understanding increasingly relies on vision-language models
(VLMs) to generate natural language descriptions of the environment. In this
work, we present a comparative study of captioning strategies for tabletop
scenes captured by a robotic arm equipped with an RGB camera. The robot
collects images of objects from multiple viewpoints, and we evaluate several
models that generate scene descriptions. We compare the performance of various
captioning models, like BLIP and VLMs. Our experiments examine the trade-offs
between single-view and multi-view captioning, and difference between
recognising real-world and 3D printed objects. We quantitatively evaluate
object identification accuracy, completeness, and naturalness of the generated
captions. Results show that VLMs can be used in robotic settings where common
objects need to be recognised, but fail to generalise to novel representations.
Our findings provide practical insights into deploying foundation models for
embodied agents in real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an Introspective Dynamic Model of Globally Distributed Computing
  Infrastructures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozgur O. Kilic, David K. Park, Yihui Ren, Tatiana Korchuganova, Sairam Sri Vatsavai, Joseph Boudreau, Tasnuva Chowdhury, Shengyu Feng, Raees Khan, Jaehyung Kim, Scott Klasky, Tadashi Maeno, Paul Nilsson, Verena Ingrid Martinez Outschoorn, Norbert Podhorszki, Frédéric Suter, Wei Yang, Yiming Yang, Shinjae Yoo, Alexei Klimentov, Adolfy Hoisie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale scientific collaborations like ATLAS, Belle II, CMS, DUNE, and
others involve hundreds of research institutes and thousands of researchers
spread across the globe. These experiments generate petabytes of data, with
volumes soon expected to reach exabytes. Consequently, there is a growing need
for computation, including structured data processing from raw data to
consumer-ready derived data, extensive Monte Carlo simulation campaigns, and a
wide range of end-user analysis. To manage these computational and storage
demands, centralized workflow and data management systems are implemented.
However, decisions regarding data placement and payload allocation are often
made disjointly and via heuristic means. A significant obstacle in adopting
more effective heuristic or AI-driven solutions is the absence of a quick and
reliable introspective dynamic model to evaluate and refine alternative
approaches. In this study, we aim to develop such an interactive system using
real-world data. By examining job execution records from the PanDA workflow
management system, we have pinpointed key performance indicators such as
queuing time, error rate, and the extent of remote data access. The dataset
includes five months of activity. Additionally, we are creating a generative AI
model to simulate time series of payloads, which incorporate visible features
like category, event count, and submitting group, as well as hidden features
like the total computational load-derived from existing PanDA records and
computing site capabilities. These hidden features, which are not visible to
job allocators, whether heuristic or AI-driven, influence factors such as
queuing times and data movement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer
  Set Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanne Wielinga, Jesse Heyninck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) techniques play a pivotal role in high-stakes domains
such as healthcare, where accurate predictions can greatly enhance
decision-making. However, most high-performing methods such as neural networks
and ensemble methods are often opaque, limiting trust and broader adoption. In
parallel, symbolic methods like Answer Set Programming (ASP) offer the
possibility of interpretable logical rules but do not always match the
predictive power of ML models. This paper proposes a hybrid approach that
integrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML
classifiers to selectively correct uncertain predictions and provide
human-readable explanations. Experiments on five medical datasets reveal
statistically significant performance gains in accuracy and F1 score. This
study underscores the potential of combining symbolic reasoning with
conventional ML to achieve high interpretability without sacrificing accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication as a Technical Communication at ICLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Has Machine Translation Evaluation Achieved Human Parity? The Human
  Reference and the Limits of Progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Proietti, Stefano Perrella, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Machine Translation (MT) evaluation, metric performance is assessed based
on agreement with human judgments. In recent years, automatic metrics have
demonstrated increasingly high levels of agreement with humans. To gain a
clearer understanding of metric performance and establish an upper bound, we
incorporate human baselines in the MT meta-evaluation, that is, the assessment
of MT metrics' capabilities. Our results show that human annotators are not
consistently superior to automatic metrics, with state-of-the-art metrics often
ranking on par with or higher than human baselines. Despite these findings
suggesting human parity, we discuss several reasons for caution. Finally, we
explore the broader implications of our results for the research field, asking:
Can we still reliably measure improvements in MT evaluation? With this work, we
aim to shed light on the limits of our ability to measure progress in the
field, fostering discussion on an issue that we believe is crucial to the
entire MT evaluation community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025 Main Conference. 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAF: A Feature-Adaptive Framework for Few-Shot Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengpeng Ouyang, Dong Chen, Tong Yang, Shuo Feng, Zhao Jin, Mingliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task and few-shot time series forecasting tasks are commonly
encountered in scenarios such as the launch of new products in different
cities. However, traditional time series forecasting methods suffer from
insufficient historical data, which stems from a disregard for the generalized
and specific features among different tasks. For the aforementioned challenges,
we propose the Feature-Adaptive Time Series Forecasting Framework (FAF), which
consists of three key components: the Generalized Knowledge Module (GKM), the
Task-Specific Module (TSM), and the Rank Module (RM). During training phase,
the GKM is updated through a meta-learning mechanism that enables the model to
extract generalized features across related tasks. Meanwhile, the TSM is
trained to capture diverse local dynamics through multiple functional regions,
each of which learns specific features from individual tasks. During testing
phase, the RM dynamically selects the most relevant functional region from the
TSM based on input sequence features, which is then combined with the
generalized knowledge learned by the GKM to generate accurate forecasts. This
design enables FAF to achieve robust and personalized forecasting even with
sparse historical observations We evaluate FAF on five diverse real-world
datasets under few-shot time series forecasting settings. Experimental results
demonstrate that FAF consistently outperforms baselines that include three
categories of time series forecasting methods. In particular, FAF achieves a
41.81\% improvement over the best baseline, iTransformer, on the CO$_2$
emissions dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,4 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrivacyXray: Detecting Privacy Breaches in <span class="highlight-title">LLM</span>s through Semantic
  Consistency and Probability Certainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwen He, Yiyang Lu, Zijin Lin, Kai Chen, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely used in sensitive domains, including
healthcare, finance, and legal services, raising concerns about potential
private information leaks during inference. Privacy extraction attacks, such as
jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the
models to output sensitive information. However, these attacks cannot verify
whether the extracted private information is accurate, as no public datasets
exist for cross-validation, leaving a critical gap in private information
detection during inference. To address this, we propose PrivacyXray, a novel
framework detecting privacy breaches by analyzing LLM inner states. Our
analysis reveals that LLMs exhibit higher semantic coherence and probabilistic
certainty when generating correct private outputs. Based on this, PrivacyXray
detects privacy breaches using four metrics: intra-layer and inter-layer
semantic similarity, token-level and sentence-level probability distributions.
PrivacyXray addresses critical challenges in private information detection by
overcoming the lack of open-source private datasets and eliminating reliance on
external data for validation. It achieves this through the synthesis of
realistic private data and a detection mechanism based on the inner states of
LLMs. Experiments show that PrivacyXray achieves consistent performance, with
an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art
methods, PrivacyXray achieves significant improvements, with an average
accuracy increase of 20.06%, highlighting its stability and practical utility
in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjong Cheon, Changbae Mun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning for vision tasks have seen the rise of State
Space Models (SSMs) like Mamba, celebrated for their linear scalability.
However, their adaptation to 2D visual data often necessitates complex
modifications that may diminish efficiency. In this paper, we introduce
MambaOutRS, a novel hybrid convolutional architecture for remote sensing image
classification that re-evaluates the necessity of recurrent SSMs. MambaOutRS
builds upon stacked Gated CNN blocks for local feature extraction and
introduces a novel Fourier Filter Gate (FFG) module that operates in the
frequency domain to capture global contextual information efficiently. Our
architecture employs a four-stage hierarchical design and was extensively
evaluated on challenging remote sensing datasets: UC Merced, AID,
NWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art
(SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant
(24.0M parameters) attained the highest F1-scores of 98.41\% on UC Merced and
95.99\% on AID, significantly outperforming existing baselines, including
larger transformer models and Mamba-based architectures, despite using
considerably fewer parameters. An ablation study conclusively demonstrates the
critical role of the Fourier Filter Gate in enhancing the model's ability to
capture global spatial patterns, leading to robust and accurate classification.
These results strongly suggest that the complexities of recurrent SSMs can be
effectively superseded by a judicious combination of gated convolutions for
spatial mixing and frequency-based gates for spectral global context. Thus,
MambaOutRS provides a compelling and efficient paradigm for developing
high-performance deep learning models in remote sensing and other vision
domains, particularly where computational efficiency is paramount.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Methods Make Great <span class="highlight-title">Domain</span>-specific Foundation Models: A
  Case-study on Fetal Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Ambsdorf, Asbjørn Munk, Sebastian Llambias, Anders Nymark Christensen, Kamil Mikolaj, Randall Balestriero, Martin Tolsgaard, Aasa Feragen, Mads Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With access to large-scale, unlabeled medical datasets, researchers are
confronted with two questions: Should they attempt to pretrain a custom
foundation model on this medical data, or use transfer-learning from an
existing generalist model? And, if a custom model is pretrained, are novel
methods required? In this paper we explore these questions by conducting a
case-study, in which we train a foundation model on a large regional fetal
ultrasound dataset of 2M images. By selecting the well-established DINOv2
method for pretraining, we achieve state-of-the-art results on three fetal
ultrasound datasets, covering data from different countries, classification,
segmentation, and few-shot tasks. We compare against a series of models
pretrained on natural images, ultrasound images, and supervised baselines. Our
results demonstrate two key insights: (i) Pretraining on custom data is worth
it, even if smaller models are trained on less data, as scaling in natural
image pretraining does not translate to ultrasound performance. (ii) Well-tuned
methods from computer vision are making it feasible to train custom foundation
models for a given medical domain, requiring no hyperparameter tuning and
little methodological adaptation. Given these findings, we argue that a bias
towards methodological innovation should be avoided when developing domain
specific foundation models under common computational resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted version of paper accepted at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCStat: A Statistical Framework for using Relative Contextualization in
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work on input-token importance in auto-regressive transformers has
relied on Softmax-normalized attention weights, which obscure the richer
structure of pre-Softmax query-key logits. We introduce RCStat, a statistical
framework that harnesses raw attention logits via Relative Contextualization
(RC), a random variable measuring contextual alignment between token segments,
and derive an efficient upper bound for RC. We demonstrate two applications:
(i) Key-Value compression, where RC-based thresholds drive adaptive key-value
eviction for substantial cache reduction with minimal quality loss; and (ii)
Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level
explanations than post-Softmax methods. Across question answering,
summarization, and attribution benchmarks, RCStat achieves significant
empirical gains, delivering state-of-the-art compression and attribution
performance without any model retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Translation? Converting RegExes for Log Parsing into Dynatrace
  Pattern Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Fragner, Christian Macho, Bernhard Dieber, Martin Pinzger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Log files provide valuable information for detecting and diagnosing problems
in enterprise software applications and data centers. Several log analytics
tools and platforms were developed to help filter and extract information from
logs, typically using regular expressions (RegExes). Recent commercial log
analytics platforms provide domain-specific languages specifically designed for
log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,
users who want to migrate to these platforms must manually convert their
RegExes into the new pattern language, which is costly and error-prone. In this
work, we present Reptile, which combines a rule-based approach for converting
RegExes into DPL patterns with a best-effort approach for cases where a full
conversion is impossible. Furthermore, it integrates GPT-4 to optimize the
obtained DPL patterns. The evaluation with 946 RegExes collected from a large
company shows that Reptile safely converted 73.7% of them. The evaluation of
Reptile's pattern optimization with 23 real-world RegExes showed an F1-score
and MCC above 0.91. These results are promising and have ample practical
implications for companies that migrate to a modern log analytics platform,
such as Dynatrace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 tables, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMAR-DS: Recalibrated Feature Learning for Metal Artifact Reduction and
  CT <span class="highlight-title">Domain</span> Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Rehman, Niki Martinel, Michele Avanzo, Riccardo Spizzo, Christian Micheloni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artifacts in kilo-Voltage CT (kVCT) imaging degrade image quality, impacting
clinical decisions. We propose a deep learning framework for metal artifact
reduction (MAR) and domain transformation from kVCT to Mega-Voltage CT (MVCT).
The proposed framework, ReMAR-DS, utilizes an encoder-decoder architecture with
enhanced feature recalibration, effectively reducing artifacts while preserving
anatomical structures. This ensures that only relevant information is utilized
in the reconstruction process. By infusing recalibrated features from the
encoder block, the model focuses on relevant spatial regions (e.g., areas with
artifacts) and highlights key features across channels (e.g., anatomical
structures), leading to improved reconstruction of artifact-corrupted regions.
Unlike traditional MAR methods, our approach bridges the gap between
high-resolution kVCT and artifact-resistant MVCT, enhancing radiotherapy
planning. It produces high-quality MVCT-like reconstructions, validated through
qualitative and quantitative evaluations. Clinically, this enables oncologists
to rely on kVCT alone, reducing repeated high-dose MVCT scans and lowering
radiation exposure for cancer patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 23rd International Conference on Image Analysis and
  Processing (ICIAP) 2025, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NTRL: Encounter Generation via Reinforcement Learning for Dynamic
  Difficulty Adjustment in Dungeons and Dragons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Romeo, Andrew D. Bagdanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Balancing combat encounters in Dungeons & Dragons (D&D) is a complex task
that requires Dungeon Masters (DM) to manually assess party strength, enemy
composition, and dynamic player interactions while avoiding interruption of the
narrative flow. In this paper, we propose Encounter Generation via
Reinforcement Learning (NTRL), a novel approach that automates Dynamic
Difficulty Adjustment (DDA) in D&D via combat encounter design. By framing the
problem as a contextual bandit, NTRL generates encounters based on real-time
party members attributes. In comparison with classic DM heuristics, NTRL
iteratively optimizes encounters to extend combat longevity (+200%), increases
damage dealt to party members, reducing post-combat hit points (-16.67%), and
raises the number of player deaths while maintaining low total party kills
(TPK). The intensification of combat forces players to act wisely and engage in
tactical maneuvers, even though the generated encounters guarantee high win
rates (70%). Even in comparison with encounters designed by human Dungeon
Masters, NTRL demonstrates superior performance by enhancing the strategic
depth of combat while increasing difficulty in a manner that preserves overall
game fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Posology Structuration : What role for <span class="highlight-title">LLM</span>s? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalia Bobkova, Laura Zanella-Calzada, Anyes Tafoughalt, Raphaël Teboul, François Plesse, Félix Gaschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically structuring posology instructions is essential for improving
medication safety and enabling clinical decision support. In French
prescriptions, these instructions are often ambiguous, irregular, or
colloquial, limiting the effectiveness of classic ML pipelines. We explore the
use of Large Language Models (LLMs) to convert free-text posologies into
structured formats, comparing prompt-based methods and fine-tuning against a
"pre-LLM" system based on Named Entity Recognition and Linking (NERL). Our
results show that while prompting improves performance, only fine-tuned LLMs
match the accuracy of the baseline. Through error analysis, we observe
complementary strengths: NERL offers structural precision, while LLMs better
handle semantic nuances. Based on this, we propose a hybrid pipeline that
routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs
based on confidence scores. This strategy achieves 91% structuration accuracy
while minimizing latency and compute. Our results show that this hybrid
approach improves structuration accuracy while limiting computational cost,
offering a scalable solution for real-world clinical use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Uncertainty Quantification Based on Nearest Neighbors
  A<span class="highlight-title">cross</span> Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel N. Font, José L. Jorro-Aragoneses, Carlos M. Alaíz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at ICANN 2025
  (International Conference on Artificial Neural Networks) and will appear in
  the conference proceedings published by Springer Nature in the Lecture Notes
  in Computer Science (LNCS) series. The final authenticated version will be
  available on the publisher website</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining deep neural network models for electricity price forecasting
  with XAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Pesenti, Aidan OSullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation-Enabled Knowledge Alignment for Generative Semantic
  Communications in AIGC Provisioning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingzhi Hu, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepuNet: A Reputation System for Mitigating Malicious Clients in DFL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Marroqui Penalva, Enrique Tomás Martínez Beltrán, Manuel Gil Pérez, Alberto Huertas Celdrán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATE: <span class="highlight-title">LLM</span>-Powered Multi-Agent Translation Environment for Accessibility
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Algazinov, Matt Laing, Paul Laban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accessibility remains a critical concern in today's society, as many
technologies are not developed to support the full range of user needs.
Existing multi-agent systems (MAS) often cannot provide comprehensive
assistance for users in need due to the lack of customization stemming from
closed-source designs. Consequently, individuals with disabilities frequently
encounter significant barriers when attempting to interact with digital
environments. We introduce MATE, a multimodal accessibility MAS, which performs
the modality conversions based on the user's needs. The system is useful for
assisting people with disabilities by ensuring that data will be converted to
an understandable format. For instance, if the user cannot see well and
receives an image, the system converts this image to its audio description.
MATE can be applied to a wide range of domains, industries, and areas, such as
healthcare, and can become a useful assistant for various groups of users. The
system supports multiple types of models, ranging from LLM API calling to using
custom machine learning (ML) classifiers. This flexibility ensures that the
system can be adapted to various needs and is compatible with a wide variety of
hardware. Since the system is expected to run locally, it ensures the privacy
and security of sensitive information. In addition, the framework can be
effectively integrated with institutional technologies (e.g., digital
healthcare service) for real-time user assistance. Furthermore, we introduce
ModCon-Task-Identifier, a model that is capable of extracting the precise
modality conversion task from the user input. Numerous experiments show that
ModCon-Task-Identifier consistently outperforms other LLMs and statistical
models on our custom data. Our code and data are publicly available at
https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function
  Calling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Jiang, Hao Zhou, LiZhong GU, Ai Han, TianLong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic
  Manipulation with Vision-Language Models <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiteng Chen, Wenbo Li, Shiyi Wang, Huiping Zhuang, Qingyao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a general robotic manipulation system capable of performing a wide
variety of tasks in real-world settings is a challenging task. Vision-Language
Models (VLMs) have demonstrated remarkable potential in robotic manipulation
tasks, primarily due to the extensive world knowledge they gain from
large-scale datasets. In this process, Spatial Representations (such as points
representing object positions or vectors representing object orientations) act
as a bridge between VLMs and real-world scene, effectively grounding the
reasoning abilities of VLMs and applying them to specific task scenarios.
However, existing VLM-based robotic approaches often adopt a fixed spatial
representation extraction scheme for various tasks, resulting in insufficient
representational capability or excessive extraction time. In this work, we
introduce T-Rex, a Task-Adaptive Framework for Spatial Representation
Extraction, which dynamically selects the most appropriate spatial
representation extraction scheme for each entity based on specific task
requirements. Our key insight is that task complexity determines the types and
granularity of spatial representations, and Stronger representational
capabilities are typically associated with Higher overall system operation
costs. Through comprehensive experiments in real-world robotic environments, we
show that our approach delivers significant advantages in spatial
understanding, efficiency, and stability without additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Assessment of Neural 3D Reconstruction for Small UAV-based
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genís Castillo Gómez-Raya, Álmos Veres-Vitályos, Filip Lemic, Pablo Royo, Mario Montagud, Sergi Fernández, Sergi Abadal, Xavier Costa-Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has
expanded their deployment potential to indoor and hard-to-reach areas. However,
this trend introduces distinct challenges, particularly in terms of flight
dynamics and power consumption, which limit the UAVs' autonomy and mission
capabilities. This paper presents a novel approach to overcoming these
limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV
systems for fine-grained 3-Dimensional (3D) digital reconstruction of small
static objects. Specifically, we design, implement, and evaluate an N3DR-based
pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and
Splatfacto, to improve the quality of 3D reconstructions using images of the
object captured by a fleet of small UAVs. We assess the performance of the
considered models using various imagery and pointcloud metrics, comparing them
against the baseline Structure from Motion (SfM) algorithm. The experimental
results demonstrate that the N3DR-enhanced pipeline significantly improves
reconstruction quality, making it feasible for small UAVs to support
high-precision 3D mapping and anomaly detection in constrained environments. In
more general terms, our results highlight the potential of N3DR in advancing
the capabilities of miniaturized UAV systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, 2 tables, accepted at IEEE International
  Symposium on Personal, Indoor and Mobile Radio Communications 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy
  Labelers to Leak Privacy <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Sui, Liang Hu, Jian Cao, Dora D. Liu, Usman Naseem, Zhongyuan Lai, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) technology facilitates the removal of the influence
of specific data instances from trained models on request. Despite rapid
advancements in MU technology, its vulnerabilities are still underexplored,
posing potential risks of privacy breaches through leaks of ostensibly
unlearned information. Current limited research on MU attacks requires access
to original models containing privacy data, which violates the critical
privacy-preserving objective of MU. To address this gap, we initiate an
innovative study on recalling the forgotten class memberships from unlearned
models (ULMs) without requiring access to the original one. Specifically, we
implement a Membership Recall Attack (MRA) framework with a teacher-student
knowledge distillation architecture, where ULMs serve as noisy labelers to
transfer knowledge to student models. Then, it is translated into a Learning
with Noisy Labels (LNL) problem for inferring the correct labels of the
forgetting instances. Extensive experiments on state-of-the-art MU methods with
multiple real datasets demonstrate that the proposed MRA strategy exhibits high
efficacy in recovering class memberships of unlearned instances. As a result,
our study and evaluation have established a benchmark for future research on MU
vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogic Pedagogy for <span class="highlight-title">Large Language Model</span>s: Aligning Conversational AI
  with Proven Theories of Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Russell Beale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are rapidly transforming education by enabling
rich conversational learning experiences. This article provides a comprehensive
review of how LLM-based conversational agents are being used in higher
education, with extensions to secondary and lifelong learning contexts. We
synthesize existing literature on LLMs in education and theories of
conversational and dialogic pedagogy - including Vygotsky's sociocultural
learning (scaffolding and the Zone of Proximal Development), the Socratic
method, and Laurillard's conversational framework - and examine how prompting
strategies and retrieval-augmented generation (RAG) can align LLM behaviors
with these pedagogical theories, and how it can support personalized, adaptive
learning. We map educational theories to LLM capabilities, highlighting where
LLM-driven dialogue supports established learning principles and where it
challenges or falls short of traditional pedagogical assumptions. Notable gaps
in applying prior theories to LLMs are identified, such as the models tendency
to provide direct answers instead of fostering co-construction of knowledge,
and the need to account for the constant availability and broad but non-human
expertise of LLM tutors. In response, we propose practical strategies to better
align LLM interactions with sound pedagogy - for example, designing prompts
that encourage Socratic questioning, scaffolded guidance, and student
reflection, as well as integrating retrieval mechanisms to ensure accuracy and
contextual relevance. Our aim is to bridge the gap between educational theory
and the emerging practice of AI-driven conversational learning, offering
insights and tools for making LLM-based dialogues more educationally productive
and theory-aligned.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Distributed Equivariant Graph Neural Networks by Virtual Node
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelin Zhang, Jiacheng Cen, Jiaqi Han, Wenbing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant Graph Neural Networks (GNNs) have achieved remarkable success
across diverse scientific applications. However, existing approaches face
critical efficiency challenges when scaling to large geometric graphs and
suffer significant performance degradation when the input graphs are sparsified
for computational tractability. To address these limitations, we introduce
FastEGNN and DistEGNN, two novel enhancements to equivariant GNNs for
large-scale geometric graphs. FastEGNN employs a key innovation: a small
ordered set of virtual nodes that effectively approximates the large unordered
graph of real nodes. Specifically, we implement distinct message passing and
aggregation mechanisms for different virtual nodes to ensure mutual
distinctiveness, and minimize Maximum Mean Discrepancy (MMD) between virtual
and real coordinates to achieve global distributedness. This design enables
FastEGNN to maintain high accuracy while efficiently processing large-scale
sparse graphs. For extremely large-scale geometric graphs, we present DistEGNN,
a distributed extension where virtual nodes act as global bridges between
subgraphs in different devices, maintaining consistency while dramatically
reducing memory and computational overhead. We comprehensively evaluate our
models across four challenging domains: N-body systems (100 nodes), protein
dynamics (800 nodes), Water-3D (8,000 nodes), and our new Fluid113K benchmark
(113,000 nodes). Results demonstrate superior efficiency and performance,
establishing new capabilities in large-scale equivariant graph learning. Code
is available at https://github.com/GLAD-RUC/DistEGNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large
  Language Model via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hao, Shuaibo Li, Hongqiu Wang, Zhizhuo Kou, Junhang Zhang, Guang Yang, Lei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant progress has been made in the field of surgical
scene understanding, particularly in the task of Visual Question
Localized-Answering in robotic surgery (Surgical-VQLA). However, existing
Surgical-VQLA models lack deep reasoning capabilities and interpretability in
surgical scenes, which limits their reliability and potential for development
in clinical applications. To address this issue, inspired by the development of
Reasoning Multimodal Large Language Models (MLLMs), we first build the
Surgery-R1-54k dataset, including paired data for Visual-QA, Grounding-QA, and
Chain-of-Thought (CoT). Then, we propose the first Reasoning MLLM for
Surgical-VQLA (Surgery-R1). In our Surgery-R1, we design a two-stage
fine-tuning mechanism to enable the basic MLLM with complex reasoning abilities
by utilizing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT).
Furthermore, for an efficient and high-quality rule-based reward system in our
RFT, we design a Multimodal Coherence reward mechanism to mitigate positional
illusions that may arise in surgical scenarios. Experiment results demonstrate
that Surgery-R1 outperforms other existing state-of-the-art (SOTA) models in
the Surgical-VQLA task and widely-used MLLMs, while also validating its
reasoning capabilities and the effectiveness of our approach. The code and
dataset will be organized in https://github.com/FiFi-HAO467/Surgery-R1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuBench: Assessment of Multilingual Capabilities of Large Language
  Models A<span class="highlight-title">cross</span> 61 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Han, Yifan Zhang, Zhixun Chen, Binbin Liu, Haobin Lin, Bingni Zhang, Taifeng Wang, Mykola Pechenizkiy, Meng Fang, Yin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (LLMs) are advancing rapidly, with new
models frequently claiming support for an increasing number of languages.
However, existing evaluation datasets are limited and lack cross-lingual
alignment, leaving assessments of multilingual capabilities fragmented in both
language and skill coverage. To address this, we introduce MuBench, a benchmark
covering 61 languages and evaluating a broad range of capabilities. We evaluate
several state-of-the-art multilingual LLMs and find notable gaps between
claimed and actual language coverage, particularly a persistent performance
disparity between English and low-resource languages. Leveraging MuBench's
alignment, we propose Multilingual Consistency (MLC) as a complementary metric
to accuracy for analyzing performance bottlenecks and guiding model
improvement. Finally, we pretrain a suite of 1.2B-parameter models on English
and Chinese with 500B tokens, varying language ratios and parallel data
proportions to investigate cross-lingual transfer dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orthogonal Soft Pruning for Efficient Class Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghui Gong, Xue Yang, Xiaohu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Large Language Model</span>s Capture Human Annotator Disagreements? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Ni, Yu Fan, Vilém Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human annotation variation (i.e., annotation disagreements) is common in NLP
and often reflects important information such as task subjectivity and sample
ambiguity. While Large Language Models (LLMs) are increasingly used for
automatic annotation to reduce human effort, their evaluation often focuses on
predicting the majority-voted "ground truth" labels. It is still unclear,
however, whether these models also capture informative human annotation
variation. Our work addresses this gap by extensively evaluating LLMs' ability
to predict annotation disagreements without access to repeated human labels.
Our results show that LLMs struggle with modeling disagreements, which can be
overlooked by majority label-based evaluations. Notably, while RLVR-style
(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM
performance, it degrades performance in disagreement prediction. Our findings
highlight the critical need for evaluating and improving LLM annotators in
disagreement modeling. Code and data at
https://github.com/EdisonNi-hku/Disagreement_Prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap
  for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Li, Jiexiong Liu, Yixuan Chen, Qihang Zhou, KunLun Meta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces KunLunBaizeRAG, a reinforcement learning-driven
reasoning framework designed to enhance the reasoning capabilities of large
language models (LLMs) in complex multi-hop question-answering tasks. The
framework addresses key limitations of traditional RAG, such as retrieval
drift, information redundancy, and strategy rigidity. Key innovations include
the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative
Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)
mechanism, and a progressive hybrid training strategy. Experimental results
demonstrate significant improvements in exact match (EM) and LLM-judged score
(LJ) across four benchmarks, highlighting the framework's robustness and
effectiveness in complex reasoning scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylized Structural Patterns for Improved Neural Network Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnood Salehi, Vandit Sharma, Amirhossein Askari Farsangi, Tunç Ozan Aydın
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep learning models in computer vision require large datasets of real
images, which are difficult to curate and pose privacy and legal concerns,
limiting their commercial use. Recent works suggest synthetic data as an
alternative, yet models trained with it often underperform. This paper proposes
a two-step approach to bridge this gap. First, we propose an improved neural
fractal formulation through which we introduce a new class of synthetic data.
Second, we propose reverse stylization, a technique that transfers visual
features from a small, license-free set of real images onto synthetic datasets,
enhancing their effectiveness. We analyze the domain gap between our synthetic
datasets and real images using Kernel Inception Distance (KID) and show that
our method achieves a significantly lower distributional gap compared to
existing synthetic datasets. Furthermore, our experiments across different
tasks demonstrate the practical impact of this reduced gap. We show that
pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11%
reduction in FID during image generation, compared to models trained on
existing synthetic datasets, and a 20% decrease in autoencoder reconstruction
error, indicating improved performance in data representation. Furthermore, a
ViT-S model trained for classification on this synthetic data achieves over a
10% improvement in ImageNet-100 accuracy. Our work opens up exciting
possibilities for training practical models when sufficiently large real
training sets are not available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Alignment Trap: Complexity Barriers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasper Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that AI alignment is not merely difficult, but is founded
on a fundamental logical contradiction. We first establish The Enumeration
Paradox: we use machine learning precisely because we cannot enumerate all
necessary safety rules, yet making ML safe requires examples that can only be
generated from the very enumeration we admit is impossible. This paradox is
then confirmed by a set of five independent mathematical proofs, or "pillars of
impossibility." Our main results show that: (1) Geometric Impossibility: The
set of safe policies has measure zero, a necessary consequence of projecting
infinite-dimensional world-context requirements onto finite-dimensional models.
(2) Computational Impossibility: Verifying a policy's safety is coNP-complete,
even for non-zero error tolerances. (3) Statistical Impossibility: The training
data required for safety (abundant examples of rare disasters) is a logical
contradiction and thus unobtainable. (4) Information-Theoretic Impossibility:
Safety rules contain more incompressible, arbitrary information than any
feasible network can store. (5) Dynamic Impossibility: The optimization process
for increasing AI capability is actively hostile to safety, as the gradients
for the two objectives are generally anti-aligned. Together, these results
demonstrate that the pursuit of safe, highly capable AI is not a matter of
overcoming technical hurdles, but of confronting fundamental, interlocking
barriers. The paper concludes by presenting a strategic trilemma that these
impossibilities force upon the field. A formal verification of the core
theorems in Lean4 is currently in progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 Pages, 4 Figures. Substantial revision. Restructured around the
  Enumeration Paradox and Five Pillars of Impossibility. Core mathematical
  results unchanged but significantly expanded. Added new impossibility proofs
  from statistical, information-theoretic, and dynamic perspectives</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Long Range Dependency Handling in Code Generation <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannick Assogba, Donghao Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
for many models (up to 2x) when a function references another function that is
defined later in the prompt. We also observe that models that use sliding
window attention mechanisms have difficulty handling references further than
the size of a single window. We perform simple prompt modifications using call
graph information to improve multi-step retrieval performance up to 3x. Our
analysis highlights ways that long-context performance needs deeper
consideration beyond retrieval of single facts within a document.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Benchmark Datasets for Inductive Knowledge Graph
  Completion <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Shomer, Jay Revolinsky, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Completion (KGC) attempts to predict missing facts in a
Knowledge Graph (KG). Recently, there's been an increased focus on designing
KGC methods that can excel in the inductive setting, where a portion or all of
the entities and relations seen in inference are unobserved during training.
Numerous benchmark datasets have been proposed for inductive KGC, all of which
are subsets of existing KGs used for transductive KGC. However, we find that
the current procedure for constructing inductive KGC datasets inadvertently
creates a shortcut that can be exploited even while disregarding the relational
information. Specifically, we observe that the Personalized PageRank (PPR)
score can achieve strong or near SOTA performance on most datasets. In this
paper, we study the root cause of this problem. Using these insights, we
propose an alternative strategy for constructing inductive KGC datasets that
helps mitigate the PPR shortcut. We then benchmark multiple popular methods
using the newly constructed datasets and analyze their performance. The new
benchmark datasets help promote a better understanding of the capabilities and
challenges of inductive KGC by removing any shortcuts that obfuscate
performance. The code and dataset and can be found at
https://github.com/HarryShomer/Better-Inductive-KGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD'25 Datasets & Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaizeField3D: A Curated 3D Point Cloud and Procedural Model Dataset of
  Field-Grown Maize from a Diversity Panel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Kimara, Mozhgan Hadadi, Jackson Godbersen, Aditya Balu, Talukder Jubery, Yawei Li, Adarsh Krishnamurthy, Patrick S. Schnable, Baskar Ganapathysubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of artificial intelligence (AI) and machine learning (ML)
based tools for 3D phenotyping, especially for maize, has been limited due to
the lack of large and diverse 3D datasets. 2D image datasets fail to capture
essential structural details such as leaf architecture, plant volume, and
spatial arrangements that 3D data provide. To address this limitation, we
present MaizeField3D (https://baskargroup.github.io/MaizeField3D/), a curated
dataset of 3D point clouds of field-grown maize plants from a diverse genetic
panel, designed to be AI-ready for advancing agricultural research. Our dataset
includes 1,045 high-quality point clouds of field-grown maize collected using a
terrestrial laser scanner (TLS). Point clouds of 520 plants from this dataset
were segmented and annotated using a graph-based segmentation method to isolate
individual leaves and stalks, ensuring consistent labeling across all samples.
This labeled data was then used for fitting procedural models that provide a
structured parametric representation of the maize plants. The leaves of the
maize plants in the procedural models are represented using Non-Uniform
Rational B-Spline (NURBS) surfaces that were generated using a two-step
optimization process combining gradient-free and gradient-based methods. We
conducted rigorous manual quality control on all datasets, correcting errors in
segmentation, ensuring accurate leaf ordering, and validating metadata
annotations. The dataset also includes metadata detailing plant morphology and
quality, alongside multi-resolution subsampled point cloud data (100k, 50k, 10k
points), which can be readily used for different downstream computational
tasks. MaizeField3D will serve as a comprehensive foundational dataset for
AI-driven phenotyping, plant structural analysis, and 3D applications in
agricultural research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Elvis Kimara and Mozhgan Hadadi contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When <span class="highlight-title">Large Language Model</span>s contradict humans? <span class="highlight-title">Large Language Model</span>s'
  Sycophantic Behaviour 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09410v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09410v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Ranaldi, Giulia Pucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have been demonstrating broadly satisfactory generative
abilities for users, which seems to be due to the intensive use of human
feedback that refines responses. Nevertheless, suggestibility inherited via
human feedback improves the inclination to produce answers corresponding to
users' viewpoints. This behaviour is known as sycophancy and depicts the
tendency of LLMs to generate misleading responses as long as they align with
humans. This phenomenon induces bias and reduces the robustness and,
consequently, the reliability of these models. In this paper, we study the
suggestibility of Large Language Models (LLMs) to sycophantic behaviour,
analysing these tendencies via systematic human-interventions prompts over
different tasks. Our investigation demonstrates that LLMs have sycophantic
tendencies when answering queries that involve subjective opinions and
statements that should elicit a contrary response based on facts. In contrast,
when faced with math tasks or queries with an objective answer, they, at
various scales, do not follow the users' hints by demonstrating confidence in
generating the correct answers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Backdoor Stealthiness in Model Parameter Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on backdoor stealthiness focuses mainly on indistinguishable
triggers in input space and inseparable backdoor representations in feature
space, aiming to circumvent backdoor defenses that examine these respective
spaces. However, existing backdoor attacks are typically designed to resist a
specific type of backdoor defense without considering the diverse range of
defense mechanisms. Based on this observation, we pose a natural question: Are
current backdoor attacks truly a real-world threat when facing diverse
practical defenses?
  To answer this question, we examine 12 common backdoor attacks that focus on
input-space or feature-space stealthiness and 17 diverse representative
defenses. Surprisingly, we reveal a critical blind spot: Backdoor attacks
designed to be stealthy in input and feature spaces can be mitigated by
examining backdoored models in parameter space. To investigate the underlying
causes behind this common vulnerability, we study the characteristics of
backdoor attacks in the parameter space. Notably, we find that input- and
feature-space attacks introduce prominent backdoor-related neurons in parameter
space, which are not thoroughly considered by current backdoor attacks. Taking
comprehensive stealthiness into account, we propose a novel supply-chain attack
called Grond. Grond limits the parameter changes by a simple yet effective
module, Adversarial Backdoor Injection (ABI), which adaptively increases the
parameter-space stealthiness during the backdoor injection. Extensive
experiments demonstrate that Grond outperforms all 12 backdoor attacks against
state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset
of ImageNet. In addition, we show that ABI consistently improves the
effectiveness of common backdoor attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at CCS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic
  Programming for Robot Manipulation Under Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation tasks require robots to reason about cause and effect when
interacting with objects. Yet, many data-driven approaches lack causal
semantics and thus only consider correlations. We introduce COBRA-PPM, a novel
causal Bayesian reasoning architecture that combines causal Bayesian networks
and probabilistic programming to perform interventional inference for robot
manipulation under uncertainty. We demonstrate its capabilities through
high-fidelity Gazebo-based experiments on an exemplar block stacking task,
where it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%)
and performs greedy next-best action selection with a 94.2% task success rate.
We further demonstrate sim2real transfer on a domestic robot, showing
effectiveness in handling real-world uncertainty from sensor noise and
stochastic actions. Our generalised and extensible framework supports a wide
range of manipulation scenarios and lays a foundation for future work at the
intersection of robotics and causality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, accepted to the 2025 IEEE European Conference on
  Mobile Robots (ECMR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuzz-Testing Meets <span class="highlight-title">LLM</span>-Based Agents: An Automated and Efficient
  Framework for Jailbreaking Text-To-Image Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingkai Dong, Xiangtao Meng, Ning Yu, Zheng Li, Shanqing Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) generative models have revolutionized content creation by
transforming textual descriptions into high-quality images. However, these
models are vulnerable to jailbreaking attacks, where carefully crafted prompts
bypass safety mechanisms to produce unsafe content. While researchers have
developed various jailbreak attacks to expose this risk, these methods face
significant limitations, including impractical access requirements, easily
detectable unnatural prompts, restricted search spaces, and high query demands
on the target system. In this paper, we propose JailFuzzer, a novel fuzzing
framework driven by large language model (LLM) agents, designed to efficiently
generate natural and semantically meaningful jailbreak prompts in a black-box
setting. Specifically, JailFuzzer employs fuzz-testing principles with three
components: a seed pool for initial and jailbreak prompts, a guided mutation
engine for generating meaningful variations, and an oracle function to evaluate
jailbreak success. Furthermore, we construct the guided mutation engine and
oracle function by LLM-based agents, which further ensures efficiency and
adaptability in black-box settings. Extensive experiments demonstrate that
JailFuzzer has significant advantages in jailbreaking T2I models. It generates
natural and semantically coherent prompts, reducing the likelihood of detection
by traditional defenses. Additionally, it achieves a high success rate in
jailbreak attacks with minimal query overhead, outperforming existing methods
across all key metrics. This study underscores the need for stronger safety
mechanisms in generative models and provides a foundation for future research
on defending against sophisticated jailbreaking attacks. JailFuzzer is
open-source and available at this repository:
https://github.com/YingkaiD/JailFuzzer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Protein Structure Tokenization: Benchmarking and New Recipe <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Yuan, Zichen Wang, Marcus Collins, Huzefa Rangwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge in the development of protein structural
tokenization methods, which chunk protein 3D structures into discrete or
continuous representations. Structure tokenization enables the direct
application of powerful techniques like language modeling for protein
structures, and large multimodal models to integrate structures with protein
sequences and functional texts. Despite the progress, the capabilities and
limitations of these methods remain poorly understood due to the lack of a
unified evaluation framework. We first introduce StructTokenBench, a framework
that comprehensively evaluates the quality and efficiency of structure
tokenizers, focusing on fine-grained local substructures rather than global
structures, as typical in existing benchmarks. Our evaluations reveal that no
single model dominates all benchmarking perspectives. Observations of codebook
under-utilization led us to develop AminoAseed, a simple yet effective strategy
that enhances codebook gradient updates and optimally balances codebook size
and dimension for improved tokenizer utilization and quality. Compared to the
leading model ESM3, our method achieves an average of 6.31% performance
improvement across 24 supervised tasks, with sensitivity and utilization rates
increased by 12.83% and 124.03%, respectively. Source code and model weights
are available at https://github.com/KatarinaYuan/StructTokenBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large language model</span>s for automated scholarly paper <span class="highlight-title">review</span>: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, Jialiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly impacted human society,
influencing various domains. Among them, academia is not simply a domain
affected by LLMs, but it is also the pivotal force in the development of LLMs.
In academic publication, this phenomenon is represented during the
incorporation of LLMs into the peer review mechanism for reviewing manuscripts.
LLMs hold transformative potential for the full-scale implementation of
automated scholarly paper review (ASPR), but they also pose new issues and
challenges that need to be addressed. In this survey paper, we aim to provide a
holistic view of ASPR in the era of LLMs. We begin with a survey to find out
which LLMs are used to conduct ASPR. Then, we review what ASPR-related
technological bottlenecks have been solved with the incorporation of LLM
technology. After that, we move on to explore new methods, new datasets, new
source code, and new online systems that come with LLMs for ASPR. Furthermore,
we summarize the performance and issues of LLMs in ASPR, and investigate the
attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss
the challenges and future directions associated with the development of LLMs
for ASPR. This survey serves as an inspirational reference for the researchers
and can promote the progress of ASPR for its actual implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please cite the version of Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interrogating AI: Characterizing Emergent Playful Interactions with
  Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ronagh Nikghalb, Jinghui Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era of AI's growing capabilities and influences, recent advancements
are reshaping HCI and CSCW's view of AI. Playful interactions emerged as an
important way for users to make sense of the ever-changing AI technologies, yet
remained underexamined. We target this gap by investigating playful
interactions exhibited by users of a popular AI technology, ChatGPT. Through a
thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we
found that more than half (54\%) of user discourse revolved around playful
interactions. The analysis further allowed us to construct a preliminary
framework to describe these interactions, categorizing them into six types:
reflecting, jesting, imitating, challenging, tricking, and contriving; each
included sub-categories. This study contributes to HCI and CSCW by identifying
the diverse ways users engage in playful interactions with AI. It examines how
these interactions can help users understand AI's agency, shape human-AI
relationships, and provide insights for designing AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CSCW 2025; 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "I know myself better, but not really greatly": How Well Can <span class="highlight-title">LLM</span>s Detect
  and Explain <span class="highlight-title">LLM</span>-Generated Texts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhou Ji, Jie Guo, Weidong Qiu, Zheng Huang, Yang Xu, Xinru Lu, Xiaoyu Jiang, Ruizhe Li, Shujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distinguishing between human- and LLM-generated texts is crucial given the
risks associated with misuse of LLMs. This paper investigates detection and
explanation capabilities of current LLMs across two settings: binary (human vs.
LLM-generated) and ternary classification (including an ``undecided'' class).
We evaluate 6 close- and open-source LLMs of varying sizes and find that
self-detection (LLMs identifying their own outputs) consistently outperforms
cross-detection (identifying outputs from other LLMs), though both remain
suboptimal. Introducing a ternary classification framework improves both
detection accuracy and explanation quality across all models. Through
comprehensive quantitative and qualitative analyses using our human-annotated
dataset, we identify key explanation failures, primarily reliance on inaccurate
features, hallucinations, and flawed reasoning. Our findings underscore the
limitations of current LLMs in self-detection and self-explanation,
highlighting the need for further research to address overfitting and enhance
generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASR-enhanced Multimodal Representation Learning for <span class="highlight-title">Cross</span>-<span class="highlight-title">Domain</span> Product
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce is increasingly multimedia-enriched, with products exhibited in a
broad-domain manner as images, short videos, or live stream promotions. A
unified and vectorized cross-domain production representation is essential. Due
to large intra-product variance and high inter-product similarity in the
broad-domain scenario, a visual-only representation is inadequate. While
Automatic Speech Recognition (ASR) text derived from the short or live-stream
videos is readily accessible, how to de-noise the excessively noisy text for
multimodal representation learning is mostly untouched. We propose ASR-enhanced
Multimodal Product Representation Learning (AMPere). In order to extract
product-specific information from the raw ASR text, AMPere uses an
easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,
together with visual data, is then fed into a multi-branch network to generate
compact multimodal embeddings. Extensive experiments on a large-scale
tri-domain dataset verify the effectiveness of AMPere in obtaining a unified
multimodal product representation that clearly improves cross-domain product
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication as a REGULAR paper in the IEEE Transactions
  on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuseControlLite: Multifunctional Music Generation with Lightweight
  Conditioners <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang-Duo Tsai, Shih-Lun Wu, Weijaw Lee, Sheng-Ping Yang, Bo-Rui Chen, Hao-Chung Cheng, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MuseControlLite, a lightweight mechanism designed to fine-tune
text-to-music generation models for precise conditioning using various
time-varying musical attributes and reference audio signals. The key finding is
that positional embeddings, which have been seldom used by text-to-music
generation models in the conditioner for text conditions, are critical when the
condition of interest is a function of time. Using melody control as an
example, our experiments show that simply adding rotary positional embeddings
to the decoupled cross-attention layers increases control accuracy from 56.6%
to 61.1%, while requiring 6.75 times fewer trainable parameters than
state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion
Transformer model of Stable Audio Open. We evaluate various forms of musical
attribute control, audio inpainting, and audio outpainting, demonstrating
improved controllability over MusicGen-Large and Stable Audio Open ControlNet
at a significantly lower fine-tuning cost, with only 85M trainble parameters.
Source code, model checkpoints, and demo examples are available at:
https://musecontrollite.github.io/web/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 42nd International Conference on Machine Learning
  (ICML 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Bo Wang, Sedigheh Eslami, Scott Martens, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-document retrieval, semantic text similarity, and code search.
Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves
state-of-the-art performance on both single-modal and cross-modal retrieval
tasks, with particular strength in processing visually rich content such as
tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of
this capability, we also introduce Jina-VDR, a novel benchmark specifically
designed for visually rich image retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 1-10 main, 14-22 experimental results, benchmark tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Machine-Generated Texts: Not Just "AI vs Humans" and
  Explainability is Complicated 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs rapidly advance, increasing concerns arise regarding risks about
actual authorship of texts we see online and in real world. The task of
distinguishing LLM-authored texts is complicated by the nuanced and overlapping
behaviors of both machines and humans. In this paper, we challenge the current
practice of considering LLM-generated text detection a binary classification
task of differentiating human from AI. Instead, we introduce a novel ternary
text classification scheme, adding an "undecided" category for texts that could
be attributed to either source, and we show that this new category is crucial
to understand how to make the detection result more explainable to lay users.
This research shifts the paradigm from merely classifying to explaining
machine-generated texts, emphasizing need for detectors to provide clear and
understandable explanations to users. Our study involves creating four new
datasets comprised of texts from various LLMs and human authors. Based on new
datasets, we performed binary classification tests to ascertain the most
effective SOTA detection methods and identified SOTA LLMs capable of producing
harder-to-detect texts. We constructed a new dataset of texts generated by two
top-performing LLMs and human authors, and asked three human annotators to
produce ternary labels with explanation notes. This dataset was used to
investigate how three top-performing SOTA detectors behave in new ternary
classification context. Our results highlight why "undecided" category is much
needed from the viewpoint of explainability. Additionally, we conducted an
analysis of explainability of the three best-performing detectors and the
explanation notes of the human annotators, revealing insights about the
complexity of explainable detection of machine-generated texts. Finally, we
propose guidelines for developing future detection systems with improved
explanatory power.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Certified Proof Checker for Deep Neural Network Verification in
  Imandra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remi Desmartin, Omri Isac, Grant Passmore, Ekaterina Komendantskaya, Kathrin Stark, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the verification of deep neural networks (DNNs) have
opened the way for a broader usage of DNN verification technology in many
application areas, including safety-critical ones. However, DNN verifiers are
themselves complex programs that have been shown to be susceptible to errors
and numerical imprecision; this, in turn, has raised the question of trust in
DNN verifiers. One prominent attempt to address this issue is enhancing DNN
verifiers with the capability of producing certificates of their results that
are subject to independent algorithmic checking. While formulations of Marabou
certificate checking already exist on top of the state-of-the-art DNN verifier
Marabou, they are implemented in C++, and that code itself raises the question
of trust (e.g., in the precision of floating point calculations or guarantees
for implementation soundness). Here, we present an alternative implementation
of the Marabou certificate checking in Imandra -- an industrial functional
programming language and an interactive theorem prover (ITP) -- that allows us
to obtain full proof of certificate correctness. The significance of the result
is two-fold. Firstly, it gives stronger independent guarantees for Marabou
proofs. Secondly, it opens the way for the wider adoption of DNN verifiers in
interactive theorem proving in the same way as many ITPs already incorporate
SMT solvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ITP 2025, Interactive Theorem Proving</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem
  Proving <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Rajaee, Kumar Pratik, Gabriele Cesa, Arash Behboodi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most promising recent methods for AI reasoning require applying variants
of reinforcement learning (RL) either on rolled out trajectories from the LLMs,
even for the step-wise rewards, or large quantities of human-annotated
trajectory data. The reliance on the rolled-out trajectory renders the compute
cost and time prohibitively high. In particular, the correctness of a reasoning
trajectory can typically only be judged at its completion, leading to sparse
rewards in RL or requiring expensive synthetic data generation in expert
iteration-like methods. In this work, we focus on the Automatic Theorem Proving
(ATP) task and propose a novel verifier-in-the-loop design, which, unlike
existing approaches that leverage feedback on the entire reasoning trajectory,
employs an automated verifier to give intermediate feedback at each step of the
reasoning process. Using Lean as the verifier, we empirically show that the
step-by-step local verification produces a global improvement in the model's
reasoning accuracy and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Findings of ACL 2025, Accepted at ICLR 2025 Workshop
  on Reasoning and Planning for Large Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Assisted Transport of Radioactive Ion Beams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06469v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06469v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio Lopez-Caceres, Daniel Santiago-Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beams of radioactive heavy ions allow researchers to study rare and unstable
atomic nuclei, shedding light into the internal structure of exotic nuclei and
on how chemical elements are formed in stars. However, the extraction and
transport of radioactive beams rely on time-consuming expert-driven tuning
methods, where hundreds of parameters are manually optimized. Here, we
introduce a system that employs Artificial Intelligence (AI), specifically
utilizing Bayesian Optimization, to assist in the transport process of
radioactive beams. We apply our methodology to real-life scenarios showing
advantages when compared with standard tuning methods. This AI-assisted
approach can be extended to other radioactive beam facilities around the world
to improve operational efficiency and enhance scientific output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures; Results section expanded. More references and DOI
  added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeurAgenix: Leveraging <span class="highlight-title">LLM</span>s for Solving Complex Combinatorial
  Optimization Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianliang Yang, Ling Zhang, Haolong Qian, Lei Song, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heuristic algorithms play a vital role in solving combinatorial optimization
(CO) problems, yet traditional designs depend heavily on manual expertise and
struggle to generalize across diverse instances. We introduce
\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large
language models (LLMs) that first evolves heuristics and then selects among
them automatically. In the heuristic evolution phase, HeurAgenix leverages an
LLM to compare seed heuristic solutions with higher-quality solutions and
extract reusable evolution strategies. During problem solving, it dynamically
picks the most promising heuristic for each problem state, guided by the LLM's
perception ability. For flexibility, this selector can be either a
state-of-the-art LLM or a fine-tuned lightweight model with lower inference
cost. To mitigate the scarcity of reliable supervision caused by CO complexity,
we fine-tune the lightweight heuristic selector with a dual-reward mechanism
that jointly exploits singals from selection preferences and state perception,
enabling robust selection under noisy annotations. Extensive experiments on
canonical benchmarks show that HeurAgenix not only outperforms existing
LLM-based hyper-heuristics but also matches or exceeds specialized solvers.
Code is available at https://github.com/microsoft/HeurAgenix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOST: MR reconstruction Optimization for multiple downStream Tasks via
  continual learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwihun Jeong, Se Young Chun, Jongho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based Magnetic Resonance (MR) reconstruction methods have
focused on generating high-quality images but often overlook the impact on
downstream tasks (e.g., segmentation) that utilize the reconstructed images.
Cascading separately trained reconstruction network and downstream task network
has been shown to introduce performance degradation due to error propagation
and domain gaps between training datasets. To mitigate this issue, downstream
task-oriented reconstruction optimization has been proposed for a single
downstream task. Expanding this optimization to multi-task scenarios is not
straightforward. In this work, we extended this optimization to sequentially
introduced multiple downstream tasks and demonstrated that a single MR
reconstruction network can be optimized for multiple downstream tasks by
deploying continual learning (MOST). MOST integrated techniques from
replay-based continual learning and image-guided loss to overcome catastrophic
forgetting. Comparative experiments demonstrated that MOST outperformed a
reconstruction network without finetuning, a reconstruction network with
na\"ive finetuning, and conventional continual learning methods. The source
code is available at: https://github.com/SNU-LIST/MOST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lemmanaid: Neuro-Symbolic Lemma Conjecturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yousef Alhessi, Sólrún Halla Einarsdóttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically conjecturing useful, interesting and novel lemmas would greatly
improve automated reasoning tools and lower the bar for formalizing mathematics
in proof assistants. It is however a very challenging task for both neural and
symbolic approaches. We present the first steps towards a practical
neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language
Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the
Isabelle proof assistant. We train an LLM to generate lemma templates that
describe the shape of a lemma, and use symbolic methods to fill in the details.
We compare Lemmanaid against an LLM trained to generate complete lemma
statements as well as previous fully symbolic conjecturing methods. Lemmanaid
outperforms both neural and symbolic methods on test sets from Isabelle's HOL
library and from its Archive of Formal Proofs, discovering between 29-39.5% of
the gold standard human written lemmas. This is 8-15% more lemmas than the
neural-only method. By leveraging the best of both symbolic and neural methods
we can generate useful lemmas for a wide range of input domains, facilitating
computer-assisted theory development and formalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Re-rankers are Fooled by Lexical Similarities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lovisa Hagström, Ercong Nie, Ruben Halifa, Helmut Schmid, Richard Johansson, Alexander Junge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) re-rankers are used to refine retrieval results for
retrieval-augmented generation (RAG). They are more expensive than lexical
matching methods like BM25 but assumed to better process semantic information
and the relations between the query and the retrieved answers. To understand
whether LM re-rankers always live up to this assumption, we evaluate 6
different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show
that LM re-rankers struggle to outperform a simple BM25 baseline on DRUID.
Leveraging a novel separation metric based on BM25 scores, we explain and
identify re-ranker errors stemming from lexical dissimilarities. We also
investigate different methods to improve LM re-ranker performance and find
these methods mainly useful for NQ. Taken together, our work identifies and
explains weaknesses of LM re-rankers and points to the need for more
adversarial and realistic datasets for their evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to FEVER 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Machine Learning in Mental Health: A <span class="highlight-title">Survey</span> of Data,
  Algorithms, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahraa Al Sahili, Ioannis Patras, Matthew Purver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning (MML) is rapidly reshaping the way mental-health
disorders are detected, characterized, and longitudinally monitored. Whereas
early studies relied on isolated data streams -- such as speech, text, or
wearable signals -- recent research has converged on architectures that
integrate heterogeneous modalities to capture the rich, complex signatures of
psychiatric conditions. This survey provides the first comprehensive,
clinically grounded synthesis of MML for mental health. We (i) catalog 26
public datasets spanning audio, visual, physiological signals, and text
modalities; (ii) systematically compare transformer, graph, and hybrid-based
fusion strategies across 28 models, highlighting trends in representation
learning and cross-modal alignment. Beyond summarizing current capabilities, we
interrogate open challenges: data governance and privacy, demographic and
intersectional fairness, evaluation explainability, and the complexity of
mental health disorders in multimodal settings. By bridging methodological
innovation with psychiatric utility, this survey aims to orient both ML
researchers and mental-health practitioners toward the next generation of
trustworthy, multimodal decision-support systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-based Multimodal Biometrics for Detecting Smartphone Distractions:
  Application to Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Mutlu Cukurova, Julian Fierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EC-TEL25: 20th European Conference on Technology Enhanced
  Learning, Newcastle and Durham, UK, 15-19 September 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECG-SMART-NET: A Deep Learning Architecture for Precise ECG Diagnosis of
  Occlusion Myocardial Infarction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan T. Riek, Murat Akcakaya, Zeineb Bouzid, Tanmay Gokhale, Stephanie Helman, Karina Kraevsky-Philips, Rui Qi Ji, Ervin Sejdic, Jessica K. Zègre-Hemsey, Christian Martin-Gill, Clifton W. Callaway, Samir Saba, Salah Al-Zaiti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: In this paper we develop and evaluate ECG-SMART-NET for occlusion
myocardial infarction (OMI) identification. OMI is a severe form of heart
attack characterized by complete blockage of one or more coronary arteries
requiring immediate referral for cardiac catheterization to restore blood flow
to the heart. Two thirds of OMI cases are difficult to visually identify from a
12-lead electrocardiogram (ECG) and can be potentially fatal if not identified
quickly. Previous works on this topic are scarce, and current state-of-the-art
evidence suggests both feature-based random forests and convolutional neural
networks (CNNs) are promising approaches to improve ECG detection of OMI.
Methods: While the ResNet architecture has been adapted for use with ECG
recordings, it is not ideally suited to capture informative temporal features
within each lead and the spatial concordance or discordance across leads. We
propose a clinically informed modification of the ResNet-18 architecture. The
model first learns temporal features through temporal convolutional layers with
1xk kernels followed by a spatial convolutional layer, after the residual
blocks, with 12x1 kernels to learn spatial features. Results: ECG-SMART-NET was
benchmarked against the original ResNet-18 and other state-of-the-art models on
a multisite real-word clinical dataset that consists of 10,393 ECGs from 7,397
unique patients (rate of OMI =7.2%). ECG-SMART-NET outperformed other models in
the classification of OMI with a test AUC of 0.953 [0.921, 0.978]. Conclusion
and Significance: ECG-SMART-NET can outperform the state-of-the-art random
forest for OMI prediction and is better suited for this task than the original
ResNet-18 architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Centered Editable Speech-to-Sign-Language Generation via Streaming
  Conformer-<span class="highlight-title">Transformer</span> and Resampling Hook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing end-to-end sign-language animation systems suffer from low
naturalness, limited facial/body expressivity, and no user control. We propose
a human-centered, real-time speech-to-sign animation framework that integrates
(1) a streaming Conformer encoder with an autoregressive Transformer-MDN
decoder for synchronized upper-body and facial motion generation, (2) a
transparent, editable JSON intermediate representation empowering deaf users
and experts to inspect and modify each sign segment, and (3) a
human-in-the-loop optimization loop that refines the model based on user edits
and ratings. Deployed on Unity3D, our system achieves a 13 ms average
frame-inference time and a 103 ms end-to-end latency on an RTX 4070. Our key
contributions include the design of a JSON-centric editing mechanism for
fine-grained sign-level personalization and the first application of an
MDN-based feedback loop for continuous model adaptation. This combination
establishes a generalizable, explainable AI paradigm for user-adaptive,
low-latency multimodal systems. In studies with 20 deaf signers and 5
professional interpreters, we observe a +13 point SUS improvement, 6.7 point
reduction in cognitive load, and significant gains in naturalness and trust (p
$<$ .001) over baselines. This work establishes a scalable, explainable AI
paradigm for accessible sign-language technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints
  during Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siao Tang, Xinyin Ma, Gongfan Fang, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes are available at https://github.com/tsa18/ConciseHint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KAG-Thinker: Interactive Thinking and Deep Reasoning in <span class="highlight-title">LLM</span>s via
  Knowledge-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn
interactive thinking and deep reasoning framework powered by a dedicated
parameter-light large language model (LLM). Our approach constructs a
structured thinking process for solving complex problems, enhancing the the
logical coherence and contextual consistency of the reasoning process in
question-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within
LLMs. Following the \textbf{Logical Form} guided retrieval and reasoning
technology route of KAG, this framework first decomposes complex questions into
independently solvable sub-problems (which are also referred to as logical
forms) through \textbf{breadth decomposition}. Each such logical form is
represented in two equivalent forms-natural language and logical function-and
subsequently classified as either a Knowledge Retrieval or Reasoning Analysis
task. Dependencies and parameter passing between these tasks are explicitly
modeled via logical function interfaces. In the solving process, the Retrieval
function performs retrieval tasks. It retrieves one-hop structured and
unstructured information of specified knowledge unit. While the Math and Deduce
functions are used to perform reasoning analysis tasks. Secondly, it is worth
noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external
knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge
boundary} module to determine the optimal source using self-regulatory
mechanisms such as confidence calibration and reflective reasoning, and use the
\textbf{depth solving} module to enhance the comprehensiveness of knowledge
acquisition...
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking the Pedagogical Knowledge of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Lelièvre, Amy Waldock, Meng Liu, Natalia Valdés Aspillaga, Alasdair Mackintosh, María José Ogando Portela, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatSR: Multimodal <span class="highlight-title">Large Language Model</span>s for Scientific Formula
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Lina Yu, Weijun Li, Min Wu, Jingyi Liu, Wenqiang Li, Shu Wei, Yusong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formulas are the language of communication between humans and nature. The
discovery of formulas to describe natural laws from observational data is the
purpose of scientific research. It is also an important research topic in
artificial intelligence, which is called a symbolic regression problem. Most of
the existing symbolic regression methods generate expressions directly from
observed data. Although in some methods, we can inject some prior knowledge
into the model by adding constraints or introducing some special character
hints. However, these methods can only introduce a limited amount of prior
knowledge specified in advance. Not to mention understanding natural language
instructions. In this article, based on the powerful knowledge reserve and
language understanding ability of multi-modal large language models, we present
ChatSR, which acts like a knowledgeable human scientist, and we can tell it any
prior knowledge through natural language to guide it in formula generation. By
testing on 13 datasets, ChatSR not only shows state-of-the-art performance on
traditional symbolic regression tasks. More notably, ChatSR can well understand
the prior knowledge contained in natural language prompts and improve the
quality of generated expressions. In addition, it is exciting that ChatSR has a
good zero-shot capability to understand prior knowledge that is not present in
the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning
  with Video <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Cheng Chiu, Jen-Jee Chen, Yu-Chee Tseng, Feng-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently been extended to the video domain,
enabling sophisticated video-language understanding. However, existing Video
LLMs often exhibit limitations in fine-grained temporal reasoning, restricting
their ability to precisely attribute responses to specific video moments,
especially under constrained supervision. We introduce DaMO, a data-efficient
Video LLM explicitly designed for accurate temporal reasoning and multimodal
understanding. At its core, the proposed Temporal-aware Fuseformer employs a
hierarchical dual-stream architecture that progressively captures temporal
dynamics within each modality and effectively fuses complementary visual and
audio information. To further enhance computational efficiency, DaMO integrates
a global residual that reduces spatial redundancy while preserving essential
semantic details. We train DaMO via a structured four-stage progressive
training paradigm, incrementally equipping the model with multimodal alignment,
semantic grounding, and temporal reasoning capabilities. This work also
contributes multiple datasets augmented from existing ones with GPT-generated
temporally grounded QA pairs for tasks requiring temporal supervision.
Comprehensive experiments on temporal grounding and video QA benchmarks
demonstrate that DaMO consistently surpasses prior methods, particularly in
tasks demanding precise temporal alignment and reasoning. Our work establishes
a promising direction for data-efficient video-language modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I would like to request the withdrawal of this submission because the
  current version contains significant errors and incomplete results. I intend
  to revise the manuscript thoroughly before resubmitting. I apologize for the
  oversight and appreciate your understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rich Interoperable Metadata for Cultural Heritage Projects at
  Jagiellonian University 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luiz do Valle Miranda, Krzysztof Kutt, Elżbieta Sroka, Grzegorz J. Nalepa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rich metadata created nowadays for objects stored in libraries has
nowhere to be stored, because core standards, namely MARC 21 and Dublin Core,
are not flexible enough. The aim of this paper is to summarize our
work-in-progress on tackling this problem in research on cultural heritage
objects at the Jagiellonian University (JU). We compared the objects' metadata
currently being collected at the JU (with examples of manuscript, placard, and
obituary) with five widespread metadata standards used by the cultural heritage
community: Dublin Core, EAD, MODS, EDM and Digital Scriptorium. Our preliminary
results showed that mapping between them is indeed problematic, but we
identified requirements that should be followed in further work on the JU
cultural heritage metadata schema in order to achieve maximum interoperability.
As we move forward, based on the successive versions of the conceptual model,
we will conduct experiments to validate the practical feasibility of these
mappings and the degree to which the proposed model will actually enable
integration with data in these various metadata formats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; submitted to TPLD 2025; change in v2: heavily rewritten,
  new content added; change in v3: updated e-mail address</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Stability Prediction in Smart Grids: GAN-based Approach
  under Data Constraints and Adversarial Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emad Efatinasab, Alessandro Brighente, Denis Donadel, Mauro Conti, Mirco Rampazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart grids are crucial for meeting rising energy demands driven by global
population growth and urbanization. By integrating renewable energy sources,
they enhance efficiency, reliability, and sustainability. However, ensuring
their availability and security requires advanced operational control and
safety measures. Although artificial intelligence and machine learning can help
assess grid stability, challenges such as data scarcity and cybersecurity
threats, particularly adversarial attacks, remain. Data scarcity is a major
issue, as obtaining real-world instances of grid instability requires
significant expertise, resources, and time. Yet, these instances are critical
for testing new research advancements and security mitigations. This paper
introduces a novel framework for detecting instability in smart grids using
only stable data. It employs a Generative Adversarial Network (GAN) where the
generator is designed not to produce near-realistic data but instead to
generate Out-Of-Distribution (OOD) samples with respect to the stable class.
These OOD samples represent unstable behavior, anomalies, or disturbances that
deviate from the stable data distribution. By training exclusively on stable
data and exposing the discriminator to OOD samples, our framework learns a
robust decision boundary to distinguish stable conditions from any unstable
behavior, without requiring unstable data during training. Furthermore, we
incorporate an adversarial training layer to enhance resilience against
attacks. Evaluated on a real-world dataset, our solution achieves up to 98.1\%
accuracy in predicting grid stability and 98.9\% in detecting adversarial
attacks. Implemented on a single-board computer, it enables real-time
decision-making with an average response time of under 7ms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Unsupervised Multi-Agent Reinforcement Learning via
  Task-Agnostic Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Zamboni, Mirco Mutti, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, we typically refer to unsupervised pre-training
when we aim to pre-train a policy without a priori access to the task
specification, i.e. rewards, to be later employed for efficient learning of
downstream tasks. In single-agent settings, the problem has been extensively
studied and mostly understood. A popular approach, called task-agnostic
exploration, casts the unsupervised objective as maximizing the entropy of the
state distribution induced by the agent's policy, from which principles and
methods follow.
  In contrast, little is known about it in multi-agent settings, which are
ubiquitous in the real world. What are the pros and cons of alternative problem
formulations in this setting? How hard is the problem in theory, how can we
solve it in practice? In this paper, we address these questions by first
characterizing those alternative formulations and highlighting how the problem,
even when tractable in theory, is non-trivial in practice. Then, we present a
scalable, decentralized, trust-region policy search algorithm to address the
problem in practical settings. Finally, we provide numerical validations to
both corroborate the theoretical findings and pave the way for unsupervised
multi-agent reinforcement learning via task-agnostic exploration in challenging
domains, showing that optimizing for a specific objective, namely mixture
entropy, provides an excellent trade-off between tractability and performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saahil Mahato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrainVerify: Equivalence-Based Verification for Distributed <span class="highlight-title">LLM</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchi Lu, Youshan Miao, Cheng Tan, Peng Huang, Yi Zhu, Xian Zhang, Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AntiGrounding: Lifting Robotic Actions into VLM Representation Space for
  Decision Making <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Li, Shiyi Wang, Yiteng Chen, Huiping Zhuang, Qingyao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for
robotic manipulation within high-dimensional representation spaces. However,
current approaches often project them into compressed intermediate
representations, discarding important task-specific information such as
fine-grained spatial or semantic details. To address this, we propose
AntiGrounding, a new framework that reverses the instruction grounding process.
It lifts candidate actions directly into the VLM representation space, renders
trajectories from multiple views, and uses structured visual question answering
for instruction-based decision making. This enables zero-shot synthesis of
optimal closed-loop robot trajectories for new tasks. We also propose an
offline policy refinement module that leverages past experience to enhance
long-term performance. Experiments in both simulation and real-world
environments show that our method outperforms baselines across diverse robotic
manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to NeurIPS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContactDexNet: Multi-fingered Robotic Hand Grasping in Cluttered
  Environments through Hand-object Contact Semantic Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Kaixin Bai, Guowen Huang, Zhenshan Bing, Zhaopeng Chen, Alois Knoll, Jianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deep learning models has significantly advanced dexterous manipulation
techniques for multi-fingered hand grasping. However, the contact
information-guided grasping in cluttered environments remains largely
underexplored. To address this gap, we have developed a method for generating
multi-fingered hand grasp samples in cluttered settings through contact
semantic map. We introduce a contact semantic conditional variational
autoencoder network (CoSe-CVAE) for creating comprehensive contact semantic map
from object point cloud. We utilize grasp detection method to estimate hand
grasp poses from the contact semantic map. Finally, an unified grasp evaluation
model PointNetGPD++ is designed to assess grasp quality and collision
probability, substantially improving the reliability of identifying optimal
grasps in cluttered scenarios. Our grasp generation method has demonstrated
remarkable success, outperforming state-of-the-art methods by at least 4.65%
with 81.0% average grasping success rate in real-world single-object
environment and 75.3% grasping success rate in cluttered scenes. We also
proposed the multi-modal multi-fingered grasping dataset generation method. Our
multi-fingered hand grasping dataset outperforms previous datasets in scene
diversity, modality diversity. The dataset, code and supplementary materials
can be found at https://sites.google.com/view/contact-dexnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-23T00:00:00Z">2025-06-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Audio-centric <span class="highlight-title">Multi-task</span> Learning Framework for Streaming Ads
  Targeting on Spotify <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Verma, Vivian Chen, Darren Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spotify, a large-scale multimedia platform, attracts over 675 million monthly
active users who collectively consume millions of hours of music, podcasts,
audiobooks, and video content. This diverse content consumption pattern
introduces unique challenges for computational advertising, which must
effectively integrate a variety of ad modalities, including audio, video, and
display, within a single user experience. Traditional ad recommendation models,
primarily designed for foregrounded experiences, often struggle to reconcile
the platform's inherent audio-centrality with the demands of optimizing ad
performance across multiple formats and modalities. To overcome these
challenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a
novel framework for optimizing click-through rate (CTR) prediction in both
audio-centric and multi-modal settings. CAMoE enhances traditional
mixture-of-experts models by incorporating modality-aware task grouping,
adaptive loss masking, and deep-cross networks (DCN) to capture complex feature
interactions within a multi-modal ad ecosystem. Through extensive ablation
studies, we demonstrate that this approach achieves near Pareto-optimal
performance across audio, video, and display ad formats, significantly
improving AUC-PR compared to conventional single-task and content-based
multi-task learning baselines. When deployed at scale on Spotify's ad serving
platform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR
for audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected
cost-per-click (eCPC) for audio slots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing the Power of Reinforcement Learning for Language-Model-Based
  Information Retriever via Query-Document Co-Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both sparse and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Click Models in Light of Carousel Interfaces: Theory-Based
  Categorization and Design of Click Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Kang, Maarten de Rijke, Santiago de Leon-Martinez, Harrie Oosterhuis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click models are a well-established for modeling user interactions with web
interfaces. Previous work has mainly focused on traditional single-list web
search settings; this includes existing surveys that introduced categorizations
based on the first generation of probabilistic graphical model (PGM) click
models that have become standard. However, these categorizations have become
outdated, as their conceptualizations are unable to meaningfully compare PGM
with neural network (NN) click models nor generalize to newer interfaces, such
as carousel interfaces. We argue that this outdated view fails to adequately
explain the fundamentals of click model designs, thus hindering the development
of novel click models.
  This work reconsiders what should be the fundamental concepts in click model
design, grounding them - unlike previous approaches - in their mathematical
properties. We propose three fundamental key-design choices that explain what
statistical patterns a click model can capture, and thus indirectly, what user
behaviors they can capture. Based on these choices, we create a novel click
model taxonomy that allows a meaningful comparison of all existing click
models; this is the first taxonomy of single-list, grid and carousel click
models that includes PGMs and NNs. Finally, we show how our conceptualization
provides a foundation for future click model design by an example derivation of
a novel design for carousel interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICTIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manu Pande, Shahil Kumar, Anay Yatin Damle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PERSCEN: Learning Personalized Interaction Pattern and Scenario
  Preference for <span class="highlight-title">Multi-Scenario</span> Matching <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Du, Yaqing Wang, Fei Xiong, Lei Shao, Ming Liu, Hao Gu, Quanming Yao, Zhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias vs Bias -- Dawn of Justice: A Fair Fight in <span class="highlight-title">Recommendation</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, Sunil Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems play a crucial role in our daily lives by impacting
user experience across various domains, including e-commerce, job
advertisements, entertainment, etc. Given the vital role of such systems in our
lives, practitioners must ensure they do not produce unfair and imbalanced
recommendations. Previous work addressing bias in recommendations overlooked
bias in certain item categories, potentially leaving some biases unaddressed.
Additionally, most previous work on fair re-ranking focused on binary-sensitive
attributes. In this paper, we address these issues by proposing a
fairness-aware re-ranking approach that helps mitigate bias in different
categories of items. This re-ranking approach leverages existing biases to
correct disparities in recommendations across various demographic groups. We
show how our approach can mitigate bias on multiple sensitive attributes,
including gender, age, and occupation. We experimented on three real-world
datasets to evaluate the effectiveness of our re-ranking scheme in mitigating
bias in recommendations. Our results show how this approach helps mitigate
social bias with little to no degradation in performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Team LA at SCIDOCA shared task 2025: Citation Discovery via
  relation-based zero-shot retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trieu An, Long Nguyen, Minh Le Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the Proceedings of SCIDOCA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Document Retrieval in COVID-19 Research: Leveraging Large
  Language Models for Hidden Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang-An Trieu, Dinh-Truong Do, Chau Nguyen, Vu Tran, Minh Le Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the Proceedings of SCIDOCA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LettinGo: Explore User Profile Generation for <span class="highlight-title">Recommendation</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Wang, Di Zhang, Fangkai Yang, Pu Zhao, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User profiling is pivotal for recommendation systems, as it transforms raw
user interaction data into concise and structured representations that drive
personalized recommendations. While traditional embedding-based profiles lack
interpretability and adaptability, recent advances with large language models
(LLMs) enable text-based profiles that are semantically richer and more
transparent. However, existing methods often adhere to fixed formats that limit
their ability to capture the full diversity of user behaviors. In this paper,
we introduce LettinGo, a novel framework for generating diverse and adaptive
user profiles. By leveraging the expressive power of LLMs and incorporating
direct feedback from downstream recommendation tasks, our approach avoids the
rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ
Direct Preference Optimization (DPO) to align the profile generator with
task-specific performance, ensuring that the profiles remain adaptive and
effective. LettinGo operates in three stages: (1) exploring diverse user
profiles via multiple LLMs, (2) evaluating profile quality based on their
impact in recommendation systems, and (3) aligning the profile generation
through pairwise preference data derived from task performance. Experimental
results demonstrate that our framework significantly enhances recommendation
accuracy, flexibility, and contextual awareness. This work enhances profile
generation as a key innovation for next-generation recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Lion and AdamW Optimizers for <span class="highlight-title">Cross</span>-Encoder
  Reranking with MiniLM, GTE, and ModernBERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahil Kumar, Manu Pande, Anay Yatin Damle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern information retrieval systems often employ a two-stage pipeline: an
efficient initial retrieval stage followed by a computationally intensive
reranking stage. Cross-encoders have shown strong effectiveness for reranking
due to their deep analysis of query-document pairs. This paper studies the
impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning
of cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,
and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.
GTE and ModernBERT support extended context lengths (up to 8192 tokens). We
evaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set
(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT
with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,
while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.
Lion also provides superior GPU efficiency, improving utilization by 2.67% to
10.33% across models. We analyze performance trends using standard IR metrics
and discuss the optimizer's impact on training dynamics across architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-SEO Bench: Does Conversational SEO Work? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming search engines into
Conversational Search Engines (CSE). Consequently, Search Engine Optimization
(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).
We are beginning to see dedicated C-SEO methods for modifying web documents to
increase their visibility in CSE responses. However, they are often tested only
for a limited breadth of application domains; we do not understand whether
certain C-SEO methods would be effective for a broad range of domains.
Moreover, existing evaluations consider only a single-actor scenario where only
one web document adopts a C-SEO method; in reality, multiple players are likely
to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy
from the dynamics we have seen in SEO. We present C-SEO Bench, the first
benchmark designed to evaluate C-SEO methods across multiple tasks, domains,
and number of actors. We consider two search tasks, question answering and
product recommendation, with three domains each. We also formalize a new
evaluation protocol with varying adoption rates among involved actors. Our
experiments reveal that most current C-SEO methods are largely ineffective,
contrary to reported results in the literature. Instead, traditional SEO
strategies, those aiming to improve the ranking of the source in the LLM
context, are significantly more effective. We also observe that as we increase
the number of C-SEO adopters, the overall gains decrease, depicting a congested
and zero-sum nature of the problem. Our code and data are available at
https://github.com/parameterlab/c-seo-bench and
https://huggingface.co/datasets/parameterlab/c-seo-bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Affordable AI Assistants with Knowledge Graph of Thoughts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02670v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02670v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Jón Gunnar Hannesson, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, Nils Blach, Haiqiang Zhang, Tao Zhang, Peiran Ma, Grzegorz Kwaśniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are revolutionizing the development of AI
assistants capable of performing diverse tasks across domains. However, current
state-of-the-art LLM-driven agents face significant challenges, including high
operational costs and limited success rates on complex benchmarks like GAIA. To
address these issues, we propose Knowledge Graph of Thoughts (KGoT), an
innovative AI assistant architecture that integrates LLM reasoning with
dynamically constructed knowledge graphs (KGs). KGoT extracts and structures
task-relevant knowledge into a dynamic KG representation, iteratively enhanced
through external tools such as math solvers, web crawlers, and Python scripts.
Such structured representation of task-relevant knowledge enables low-cost
models to solve complex tasks effectively while also minimizing bias and noise.
For example, KGoT achieves a 29% improvement in task success rates on the GAIA
benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,
harnessing a smaller model dramatically reduces operational costs by over 36x
compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and
Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a
scalable, affordable, versatile, and high-performing solution for AI
assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use
  Cases using PubMed articles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Kumar Lahiri, Qinmin Vivian Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative AI have fostered the development of highly
adept Large Language Models (LLMs) that integrate diverse data types to empower
decision-making. Among these, multimodal retrieval-augmented generation (RAG)
applications are promising because they combine the strengths of information
retrieval and generative models, enhancing their utility across various
domains, including clinical use cases. This paper introduces AlzheimerRAG, a
Multimodal RAG application for clinical use cases, primarily focusing on
Alzheimer's Disease case studies from PubMed articles. This application
incorporates cross-modal attention fusion techniques to integrate textual and
visual data processing by efficiently indexing and accessing vast amounts of
biomedical literature. Our experimental results, compared to benchmarks such as
BioASQ and PubMedQA, have yielded improved performance in the retrieval and
synthesis of domain-specific information. We also present a case study using
our multimodal RAG in various Alzheimer's clinical scenarios. We infer that
AlzheimerRAG can generate responses with accuracy non-inferior to humans and
with low rates of hallucination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized News <span class="highlight-title">Recommendation</span> with Multi-granularity Candidate-aware
  User Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Li, Xinze Lin, Shenghao Lv, Faliang Huang, Xiangju Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching candidate news with user interests is crucial for personalized news
recommendations. Most existing methods can represent a user's reading interests
through a single profile based on clicked news, which may not fully capture the
diversity of user interests. Although some approaches incorporate candidate
news or topic information, they remain insufficient because they neglect the
multi-granularity relatedness between candidate news and user interests. To
address this, this study proposed a multi-granularity candidate-aware user
modeling framework that integrated user interest features across various levels
of granularity. It consisted of two main components: candidate news encoding
and user modeling. A news textual information extractor and a
knowledge-enhanced entity information extractor can capture candidate news
features, and word-level, entity-level, and news-level candidate-aware
mechanisms can provide a comprehensive representation of user interests.
Extensive experiments on a real-world dataset demonstrated that the proposed
model could significantly outperform baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIGHTHOUSE: Fast and precise distance to shoreline calculations from
  anywhere on earth <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Beukema, Henry Herzog, Yawen Zhang, Hunter Pitelka, Favyen Bastani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 1 table, ICML 2025 ML4RS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PuckTrick: A Library for Making Synthetic Data More Realistic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandra Agostini, Andrea Maurino, Blerina Spahiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWE-SQL: Illuminating <span class="highlight-title">LLM</span> Pathways to Solve User SQL Issues in
  Real-World Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, Ziwei Tang, Yuanshuai Li, Florensia Widjaja, Xintong Zhu, Feige Zhou, Yongfeng Huang, Yannis Papakonstantinou, Fatma Ozcan, Chenhao Ma, Reynold Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resolution of complex SQL issues persists as a significant bottleneck in
real-world database applications. Current Large Language Models (LLMs), while
adept at text-to-SQL translation, have not been rigorously evaluated on the
more challenging task of debugging SQL issues. To address this gap, we
introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530
PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks
(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within
new environments to facilitate rigorous evaluation. Baseline evaluations
underscore the task's complexity, with the leading reasoning model O3-Mini
achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on
BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks
is crucial for empowering local development while safeguarding data privacy.
Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for
elevating open-source model capabilities for SQL issue debugging. This
environment leverages SQL-Rewind strategy, which automatically generates
executable issue-solution datasets by reverse-engineering issues from verified
SQLs. However, popular trajectory-based fine-tuning methods do not explore
substantial supervisory signals. We further propose f-Plan Boosting, which
extracts high-level debugging plans from SQL solutions, enabling teacher LLMs
to produce 73.7% more successful trajectories for training. We integrate these
components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,
Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on
BIRD-CRITIC-Multi, surpassing leading proprietary models such as
Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing
sophisticated SQL-debugging capabilities. The leaderboard and source code are
available: https://bird-critic.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableVault: Managing Dynamic Data Collections for <span class="highlight-title">LLM</span>-Augmented
  Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjin Zhao, Sanjay Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as powerful tools for automating
and executing complex data tasks. However, their integration into more complex
data workflows introduces significant management challenges. In response, we
present TableVault - a data management system designed to handle dynamic data
collections in LLM-augmented environments. TableVault meets the demands of
these workflows by supporting concurrent execution, ensuring reproducibility,
maintaining robust data versioning, and enabling composable workflow design. By
merging established database methodologies with emerging LLM-driven
requirements, TableVault offers a transparent platform that efficiently manages
both structured data and associated data artifacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Capture of Cell-Level Provenance in Numpy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjin Zhao, Sanjay Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective provenance tracking enhances reproducibility, governance, and data
quality in array workflows. However, significant challenges arise in capturing
this provenance, including: (1) rapidly evolving APIs, (2) diverse operation
types, and (3) large-scale datasets. To address these challenges, this paper
presents a prototype annotation system designed for arrays, which captures
cell-level provenance specifically within the numpy library. With this
prototype, we explore straightforward memory optimizations that substantially
reduce annotation latency. We envision this provenance capture approach for
arrays as part of a broader governance system for tracking for structured data
workflows and diverse data science applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Lineage Constraints for Data Science Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data science workflows often integrate functionalities from a diverse set of
libraries and frameworks. Tasks such as debugging require data lineage that
crosses library boundaries. The problem is that the way that "lineage" is
represented is often intimately tied to particular data models and data
manipulation paradigms. Inspired by the use of intermediate representations
(IRs) in cross-library performance optimizations, this vision paper proposes a
similar architecture for lineage - how do we specify logical lineage across
libraries in a common parameterized way? In practice, cross-library workflows
will contain both known operations and unknown operations, so a key design of
XProv to link both materialized lineage graphs of data transformations and the
aforementioned abstracted logical patterns. We further discuss early ideas on
how to infer logical patterns when only the materialized graphs are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnhanceGraph: A Continuously Enhanced Graph-based Index for
  High-dimensional Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Mingyu Yang, Haoyang Li, Zhitao Shen, Heng Tao Shen, Jingkuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Approximate Nearest Neighbor Search in high-dimensional vector
spaces has garnered considerable attention due to the rapid advancement of deep
learning techniques. We observed that a substantial amount of search and
construction logs are generated throughout the lifespan of a graph-based index.
However, these two types of valuable logs are not fully exploited due to the
static nature of existing indexes. We present the EnhanceGraph framework, which
integrates two types of logs into a novel structure called a conjugate graph.
The conjugate graph is then used to improve search quality. Through theoretical
analyses and observations of the limitations of graph-based indexes, we propose
several optimization methods. For the search logs, the conjugate graph stores
the edges from local optima to global optima to enhance routing to the nearest
neighbor. For the construction logs, the conjugate graph stores the pruned
edges from the proximity graph to enhance retrieving of k nearest neighbors.
Our experimental results on several public and real-world industrial datasets
show that EnhanceGraph significantly improves search accuracy with the greatest
improvement on recall from 41.74% to 93.42%, but does not sacrifices search
efficiency. In addition, our EnhanceGraph algorithm has been integrated into
Ant Group's open-source vector library, VSAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Revisited: Tractable Responsibility Measures for Query Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meghyn Bienvenu, Diego Figueira, Pierre Lafourcade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Shapley value, originating from cooperative game theory, has been
employed to define responsibility measures that quantify the contributions of
database facts to obtaining a given query answer. For non-numeric queries, this
is done by considering a cooperative game whose players are the facts and whose
wealth function assigns 1 or 0 to each subset of the database, depending on
whether the query answer holds in the given subset. While conceptually simple,
this approach suffers from a notable drawback: the problem of computing such
Shapley values is #P-hard in data complexity, even for simple conjunctive
queries. This motivates us to revisit the question of what constitutes a
reasonable responsibility measure and to introduce a new family of
responsibility measures -- weighted sums of minimal supports (WSMS) -- which
satisfy intuitive properties. Interestingly, while the definition of WSMSs is
simple and bears no obvious resemblance to the Shapley value formula, we prove
that every WSMS measure can be equivalently seen as the Shapley value of a
suitably defined cooperative game. Moreover, WSMS measures enjoy tractable data
complexity for a large class of queries, including all unions of conjunctive
queries. We further explore the combined complexity of WSMS computation and
establish (in)tractability results for various subclasses of conjunctive
queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long version of PODS'25 paper, with corrected error on Shapley
  symmetry axiom statement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When <span class="highlight-title">Large Language Model</span>s Meet Vector Databases: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01763v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01763v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey explores the synergistic potential of Large Language Models
(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving
research area. With the proliferation of LLMs comes a host of challenges,
including hallucinations, outdated knowledge, prohibitive commercial
application costs, and memory issues. VecDBs emerge as a compelling solution to
these issues by offering an efficient means to store, retrieve, and manage the
high-dimensional vector representations intrinsic to LLM operations. Through
this nuanced review, we delineate the foundational principles of LLMs and
VecDBs and critically analyze their integration's impact on enhancing LLM
functionalities. This discourse extends into a discussion on the speculative
future developments in this domain, aiming to catalyze further research into
optimizing the confluence of LLMs and VecDBs for advanced data handling and
knowledge extraction capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-22T00:00:00Z">2025-06-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Enhanced Multimodal Fusion for <span class="highlight-title">Cross</span>-<span class="highlight-title">Domain</span> Sequential
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyu Wu, Zhenhong Chen, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2504.15085</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A GenAI System for Improved FAIR Independent Biological Database
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed N. Sakib, Kallol Naha, Sajratul Y. Rubaiat, Hasan M. Jamil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Life sciences research increasingly requires identifying, accessing, and
effectively processing data from an ever-evolving array of information sources
on the Linked Open Data (LOD) network. This dynamic landscape places a
significant burden on researchers, as the quality of query responses depends
heavily on the selection and semantic integration of data sources --processes
that are often labor-intensive, error-prone, and costly. While the adoption of
FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has
aimed to address these challenges, barriers to efficient and accurate
scientific data processing persist.
  In this paper, we introduce FAIRBridge, an experimental natural
language-based query processing system designed to empower scientists to
discover, access, and query biological databases, even when they are not
FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query
intents, map them to relevant databases described in scientific literature, and
generate executable queries via intelligent resource access plans. The system
also includes robust tools for mitigating low-quality query processing,
ensuring high fidelity and responsiveness in the information delivered.
  FAIRBridge's autonomous query processing framework enables users to explore
alternative data sources, make informed choices at every step, and leverage
community-driven crowd curation when needed. By providing a user-friendly,
automated hypothesis-testing platform in natural English, FAIRBridge
significantly enhances the integration and processing of scientific data,
offering researchers a powerful new tool for advancing their inquiries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLAZE: <span class="highlight-title">Cross</span>-Language and <span class="highlight-title">Cross</span>-Project Bug Localization via Dynamic
  Chunking and Hard Example Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17631v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17631v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Partha Chakraborty, Mahmoud Alfadel, Meiyappan Nagappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software bugs require developers to exert significant effort to identify and
resolve them, often consuming about one-third of their time. Bug localization,
the process of pinpointing the exact source code files that need modification,
is crucial in reducing this effort. Existing bug localization tools, typically
reliant on deep learning techniques, face limitations in cross-project
applicability and effectiveness in multi-language environments. Recent
advancements with Large Language Models (LLMs) offer detailed representations
for bug localization. However, they encounter challenges with limited context
windows and mapping accuracy. To address these issues, we propose BLAZE, an
approach that employs dynamic chunking and hard example learning. First, BLAZE
dynamically segments source code to minimize continuity loss. Then, BLAZE
fine-tunes a GPT-based model using challenging bug cases, in order to enhance
cross-project and cross-language bug localization. To support the capability of
BLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29
large and thriving open-source projects across five different programming
languages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on
three benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate
substantial improvements compared to six state-of-the-art baselines.
Specifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%
in Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An
extensive ablation study confirms the contributions of our pipeline components
to the overall performance enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiscRec: Disentangled Semantic-Collaborative Modeling for Generative
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Yimeng Bai, Xiaoyan Zhao, Yang Zhang, Fuli Feng, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation is emerging as a powerful paradigm that directly
generates item predictions, moving beyond traditional matching-based
approaches. However, current methods face two key challenges: token-item
misalignment, where uniform token-level modeling ignores item-level granularity
that is critical for collaborative signal learning, and semantic-collaborative
signal entanglement, where collaborative and semantic signals exhibit distinct
distributions yet are fused in a unified embedding space, leading to
conflicting optimization objectives that limit the recommendation performance.
To address these issues, we propose DiscRec, a novel framework that enables
Disentangled Semantic-Collaborative signal modeling with flexible fusion for
generative Recommendation. First, DiscRec introduces item-level position
embeddings, assigned based on indices within each semantic ID, enabling
explicit modeling of item structure in input token sequences. Second, DiscRec
employs a dual-branch module to disentangle the two signals at the embedding
layer: a semantic branch encodes semantic signals using original token
embeddings, while a collaborative branch applies localized attention restricted
to tokens within the same item to effectively capture collaborative signals. A
gating mechanism subsequently fuses both branches while preserving the model's
ability to model sequential dependencies. Extensive experiments on four
real-world datasets demonstrate that DiscRec effectively decouples these
signals and consistently outperforms state-of-the-art baselines. Our codes are
available on https://github.com/Ten-Mao/DiscRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed the indentation issue in the abstract that caused rendering
  errors on arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Foundation Models for Content-Based Image Retrieval in
  Radiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06567v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06567v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Raphael Stock, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. Jäger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-based image retrieval (CBIR) has the potential to significantly
improve diagnostic aid and medical research in radiology. However, current CBIR
systems face limitations due to their specialization to certain pathologies,
limiting their utility. On the other hand, several vision foundation models
have been shown to produce general-purpose visual features. Therefore, in this
work, we propose using vision foundation models as powerful and versatile
off-the-shelf feature extractors for content-based image retrieval. Our
contributions include: (1) benchmarking a diverse set of vision foundation
models on an extensive dataset comprising 1.6 million 2D radiological images
across four modalities and 161 pathologies; (2) identifying weakly-supervised
models, particularly BiomedCLIP, as highly effective, achieving a achieving a
P@1 of up to 0.594 (P@3: 0.590, P@5: 0.588, P@10: 0.583), comparable to
specialized CBIR systems but without additional training; (3) conducting an
in-depth analysis of the impact of index size on retrieval performance; (4)
evaluating the quality of embedding spaces generated by different models; and
(5) investigating specific challenges associated with retrieving anatomical
versus pathological structures. Despite these challenges, our research
underscores the vast potential of foundation models for CBIR in radiology,
proposing a shift towards versatile, general-purpose medical image retrieval
systems that do not require specific tuning. Our code, dataset splits and
embeddings are publicly available under
https://github.com/MIC-DKFZ/foundation-models-for-cbmir.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeAR: Graph-enhanced Agent for Retrieval-augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Enting Chen, Damien Graux, Andre Melo, Ruofei Lai, Zeren Jiang, Zhongyang Li, YE QI, Yang Ren, Dandan Tu, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented Generation (RAG) relies on effective retrieval
capabilities, yet traditional sparse and dense retrievers inherently struggle
with multi-hop retrieval scenarios. In this paper, we introduce GeAR, a system
that advances RAG performance through two key innovations: (i) an efficient
graph expansion mechanism that augments any conventional base retriever, such
as BM25, and (ii) an agent framework that incorporates the resulting
graph-based retrieval into a multi-step retrieval framework. Our evaluation
demonstrates GeAR's superior retrieval capabilities across three multi-hop
question answering datasets. Notably, our system achieves state-of-the-art
results with improvements exceeding 10% on the challenging MuSiQue dataset,
while consuming fewer tokens and requiring fewer iterations than existing
multi-step retrieval systems. The project page is available at
https://gear-rag.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightRetriever: A <span class="highlight-title">LLM</span>-based Hybrid Retrieval Architecture with 1000x
  Faster Query Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional sparse
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU acceleration, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text2Struct: A Machine Learning Pipeline for Mining Structured Data from
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09044v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09044v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochao Zhou, Bo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many analysis and prediction tasks require the extraction of structured data
from unstructured texts. However, an annotation scheme and a training dataset
have not been available for training machine learning models to mine structured
data from text without special templates and patterns. To solve it, this paper
presents an end-to-end machine learning pipeline, Text2Struct, including a text
annotation scheme, training data processing, and machine learning
implementation. We formulated the mining problem as the extraction of metrics
and units associated with numerals in the text. Text2Struct was trained and
evaluated using an annotated text dataset collected from abstracts of medical
publications regarding thrombectomy. In terms of prediction performance, a dice
coefficient of 0.82 was achieved on the test dataset. By random sampling, most
predicted relations between numerals and entities were well matched to the
ground-truth annotations. These results show that Text2Struct is viable for the
mining of structured data from text without special templates or patterns. It
is anticipated to further improve the pipeline by expanding the dataset and
investigating other machine learning models. A code demonstration can be found
at: https://github.com/zcc861007/Text2Struct
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Floating-Point Data Transformation for Lossless Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samirasadat Jamalidinan, Kazem Cheshmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floating-point data is widely used across various domains. Depending on the
required precision, each floating-point value can occupy several bytes.
Lossless storage of this information is crucial due to its critical accuracy,
as seen in applications such as medical imaging and language model weights. In
these cases, data size is often significant, making lossless compression
essential. Previous approaches either treat this data as raw byte streams for
compression or fail to leverage all patterns within the dataset. However,
because multiple bytes represent a single value and due to inherent patterns in
floating-point representations, some of these bytes are correlated. To leverage
this property, we propose a novel data transformation method called Typed Data
Transformation (\DTT{}) that groups related bytes together to improve
compression. We implemented and tested our approach on various datasets across
both CPU and GPU. \DTT{} achieves a geometric mean compression ratio
improvement of 1.16$\times$ over state-of-the-art compression tools such as
zstd, while also improving both compression and decompression throughput by
1.18--3.79$\times$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farhan, Henning Koehler, Qing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computing the shortest-path distance between any two given vertices in road
networks is an important problem. A tremendous amount of research has been
conducted to address this problem, most of which are limited to static road
networks. Since road networks undergo various real-time traffic conditions,
there is a pressing need to address this problem for dynamic road networks.
Existing state-of-the-art methods incrementally maintain an indexing structure
to reflect dynamic changes on road networks. However, these methods suffer from
either slow query response time or poor maintenance performance, particularly
when road networks are large. In this work, we propose an efficient solution
\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road
networks from a novel perspective, which incorporates two hierarchies with
different but complementary data structures to support efficient query and
update processing. Specifically, our proposed solution is comprised of three
main components: \emph{query hierarchy}, \emph{update hierarchy}, and
\emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient
query answering by exploring only a small subset of vertices in the labels of
two query vertices and \emph{update hierarchy} supports efficient maintenance
of distance labelling under edge weight increase or decrease. We further
develop dynamic algorithms to reflect dynamic changes by efficiently
maintaining the update hierarchy and hierarchical labelling. We also propose a
parallel variant of our dynamic algorithms by exploiting labelling structure.
We evaluate our methods on 10 large road networks and it shows that our methods
significantly outperform the state-of-the-art methods, i.e., achieving
considerably faster construction and update time, while being consistently 2-4
times faster in terms of query processing and consuming only 10\%-20\%
labelling space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SliceGX: Layer-wise GNN Explanation with Model-slicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Zhu, Tingyang Chen, Yinghui Wu, Arijit Khan, Xiangyu Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting
  A<span class="highlight-title">cross</span> Diverse Data Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivass Arunachalam Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-21T00:00:00Z">2025-06-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Relevance Judgments for Medical Case-based Retrieval Task with
  Multimodal <span class="highlight-title">LLM</span>s <span class="chip">SIGIR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catarina Pires, Sérgio Nunes, Luís Filipe Teixeira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high sparsity typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Third Workshop on Large Language Models for
  Evaluation in Information Retrieval (LLM4Eval 2025), co-located with SIGIR
  2025. 9 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CARTS: Collaborative Agents for <span class="highlight-title">Recommendation</span> Textual Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiao Chen, Kehui Yao, Reza Yousefi Maragheh, Kai Zhao, Jianpeng Xu, Jason Cho, Evren Korpeoglu, Sushant Kumar, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current recommendation systems often require some form of textual data
summarization, such as generating concise and coherent titles for product
carousels or other grouped item displays. While large language models have
shown promise in NLP domains for textual summarization, these approaches do not
directly apply to recommendation systems, where explanations must be highly
relevant to the core features of item sets, adhere to strict word limit
constraints. In this paper, we propose CARTS (Collaborative Agents for
Recommendation Textual Summarization), a multi-agent LLM framework designed for
structured summarization in recommendation systems. CARTS decomposes the task
into three stages-Generation Augmented Generation (GAG), refinement circle, and
arbitration, where successive agent roles are responsible for extracting
salient item features, iteratively refining candidate titles based on relevance
and length feedback, and selecting the final title through a collaborative
arbitration process. Experiments on large-scale e-commerce data and live A/B
testing show that CARTS significantly outperforms single-pass and
chain-of-thought LLM baselines, delivering higher title relevance and improved
user engagement metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcing User Interest Evolution in <span class="highlight-title">Multi-Scenario</span> Learning for
  recommender systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Feng, Wenhao Zheng, Xuanji Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world recommendation systems, users would engage in variety
scenarios, such as homepages, search pages, and related recommendation pages.
Each of these scenarios would reflect different aspects users focus on.
However, the user interests may be inconsistent in different scenarios, due to
differences in decision-making processes and preference expression. This
variability complicates unified modeling, making multi-scenario learning a
significant challenge. To address this, we propose a novel reinforcement
learning approach that models user preferences across scenarios by modeling
user interest evolution across multiple scenarios. Our method employs Double
Q-learning to enhance next-item prediction accuracy and optimizes contrastive
learning loss using Q-value to make model performance better. Experimental
results demonstrate that our approach surpasses state-of-the-art methods in
multi-scenario recommendation tasks. Our work offers a fresh perspective on
multi-scenario modeling and highlights promising directions for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel fast short-time root music method for vibration monitoring of
  high-speed spindles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiguang Zhang, Baoguo Liu, Wei Feng, Zongtang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra-high-speed spindle bearings challenge traditional vibration monitoring
due to broadband noise, non-stationarity, and limited time-frequency
resolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that
exploits
  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity
from $\mathcal{O}(N^3)$ to $SN\log_2N+S^2(N+S)+M^2(N+M)$
  while preserving parametric super-resolution. The method constructs Hankel
matrices from 16 ms signal frames and extracts fault frequencies through
polynomial rooting on the unit circle. Experimental validation on the
Politecnico di Torino bearing dataset demonstrates breakthrough micro-defect
detection capabilities. The algorithm reliably identifies 150 $\mu$m defects --
previously undetectable by conventional methods -- providing 72+ hours
additional warning time. Compared to STFT and wavelet methods, fSTrM achieves
1.2 Hz frequency resolution (vs. 12.5 Hz), 93\% detection rate at $-$5 dB SNR,
and quantifies defect severity through harmonic content analysis. Critically,
the algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7
hardware, enabling real-time deployment. This advancement transforms bearing
monitoring from failure prevention to continuous degradation assessment,
establishing a new paradigm for predictive maintenance in aerospace and
precision machining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Scientific Knowledge Extraction on Linked Open Data using
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajratul Y. Rubaiat, Hasan M. Jamil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking and Building Zero-Shot Hindi Retrieval Model with
  Hindi-BEIR and NLLB-E5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05401v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05401v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadeep Acharya, Rudra Murthy, Vishwajeet Kumar, Jaydeep Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the large number of Hindi speakers worldwide, there is a pressing need
for robust and efficient information retrieval systems for Hindi. Despite
ongoing research, comprehensive benchmarks for evaluating retrieval models in
Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark,
comprising 15 datasets across seven distinct tasks. We evaluate
state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark,
identifying task and domain-specific challenges that impact Hindi retrieval
performance. Building on the insights from these results, we introduce NLLB-E5,
a multilingual retrieval model that leverages a zero-shot approach to support
Hindi without the need for Hindi training data. We believe our contributions,
which include the release of the Hindi-BEIR benchmark and the NLLB-E5 model,
will prove to be a valuable resource for researchers and promote advancements
in multilingual retrieval models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2408.09437</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaPuda: <span class="highlight-title">LLM</span>-Enabled Policy-Based Query Optimizer for <span class="highlight-title">Multi-modal</span> Data <span class="chip">KDD
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contributed equally to the work, accepted by PAKDD
  2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bounds for Conjunctive Query Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Mengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this tutorial, we will survey known results on the complexity of
conjunctive query evaluation in different settings, ranging from Boolean
queries over counting to more complex models like enumeration and direct
access. A particular focus will be on showing how different relatively recent
hypotheses from complexity theory connect to query answering and allow showing
that known algorithms in several cases can likely not be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper for the tutorial at PODS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Pattern Mining and Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Li, Daniel Gibney, Sharma V. Thankachan, Solon P. Pissis, Grigorios Loukides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a string $P$ of length $m$, a longer string $T$ of length $n>m$, and
two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of
all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$
occurs in $T$. We introduce two problems related to the notion of context: (1)
the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an
integer $\tau>0$, asks for outputting the context of each substring $P$ of
length $m$ of $T$, provided that the size of the context of $P$ is at least
$\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for
preprocessing $T$ so that the size of the context of a given query string $P$
of length $m$ can be found efficiently.
  For CPM, we propose a linear-work algorithm that either uses only internal
memory, or a bounded amount of internal memory and external memory, which
allows much larger datasets to be handled. For CPC, we propose an
$\widetilde{\mathcal{O}}(n)$-space index that can be constructed in
$\widetilde{\mathcal{O}}n)$ time and answers queries in
$\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the
practical performance of the CPC index by optimizations that exploit the LZ77
factorization of $T$ and an upper bound on the query length. Using
billion-letter datasets from different domains, we show that the external
memory version of our CPM algorithm can deal with very large datasets using a
small amount of internal memory while its runtime is comparable to that of the
internal memory version. Interestingly, we also show that our optimized index
for CPC outperforms an approach based on the state of the art for the reporting
version of CPC [Navarro, SPIRE 2020] in terms of query time, index size,
construction time, and construction space, often by more than an order of
magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaPuda: <span class="highlight-title">LLM</span>-Enabled Policy-Based Query Optimizer for <span class="highlight-title">Multi-modal</span> Data <span class="chip">KDD
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contributed equally to the work, accepted by PAKDD
  2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-20T00:00:00Z">2025-06-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PreQRAG -- Classify and Rewrite for Enhanced RAG <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damian Martinez, Catalina Riano, Hui Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the submission of the UDInfo team to the SIGIR 2025
LiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)
architecture designed to improve retrieval and generation quality through
targeted question preprocessing. PreQRAG incorporates a pipeline that first
classifies each input question as either single-document or multi-document
type. For single-document questions, we employ question rewriting techniques to
improve retrieval precision and generation relevance. For multi-document
questions, we decompose complex queries into focused sub-questions that can be
processed more effectively by downstream components. This classification and
rewriting strategy improves the RAG performance. Experimental evaluation of the
LiveRAG Challenge dataset demonstrates the effectiveness of our
question-type-aware architecture, with PreQRAG achieving the preliminary second
place in Session 2 of the LiveRAG challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, SIGIR 2025 LiveRAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards AI Search Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Hengyi Cai, Rui Kong, Xinran Chen, Jiamin Chen, Jun Yang, Haojie Zhang, Jiayi Li, Jiayi Wu, Yiqun Chen, Changle Qu, Keyi Kong, Wenwen Ye, Lixin Su, Xinyu Ma, Long Xia, Daiting Shi, Jiashu Zhao, Haoyi Xiong, Shuaiqiang Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Drawings to Decisions: A Hybrid Vision-Language Framework for
  Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Tayyab Khan, Lequn Chen, Zane Yong, Jun Ming Tan, Wenhe Feng, Seung Ki Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Music Representations? Evaluating Foundation Models on World
  Music Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISMIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersonalAI: Towards digital twins in the graph form 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Menschikov, Dmitry Evseev, Ruslan Kostoev, Ilya Perepechkin, Ilnaz Salimov, Victoria Dochkina, Petr Anokhin, Evgeny Burnaev, Nikita Semenov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed
  Question Answering <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ines Besrour, Jingbo He, Tobias Schreieder, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RAGentA, a multi-agent retrieval-augmented generation (RAG)
framework for attributed question answering (QA). With the goal of trustworthy
answer generation, RAGentA focuses on optimizing answer correctness, defined by
coverage and relevance to the question and faithfulness, which measures the
extent to which answers are grounded in retrieved documents. RAGentA uses a
multi-agent architecture that iteratively filters retrieved documents,
generates attributed answers with in-line citations, and verifies completeness
through dynamic refinement. Central to the framework is a hybrid retrieval
strategy that combines sparse and dense methods, improving Recall@20 by 12.5%
compared to the best single retrieval model, resulting in more correct and
well-supported answers. Evaluated on a synthetic QA dataset derived from the
FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of
1.09% in correctness and 10.72% in faithfulness. These results demonstrate the
effectiveness of the multi-agent architecture and hybrid retrieval in advancing
trustworthy QA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for
  Sequential <span class="highlight-title">Recommendation</span> <span class="chip">SIGIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Gong, Zhifang Fan, Hui Lu, Qiwei Chen, Chenbin Zhang, Lin Guan, Yuchao Zheng, Feng Zhang, Xiao Yang, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation, a critical task in recommendation systems,
predicts the next user action based on the understanding of the user's
historical behaviors. Conventional studies mainly focus on cross-behavior
modeling with self-attention based methods while neglecting comprehensive user
interest modeling for more dimensions. In this study, we propose a novel
sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer
architecture to achieve efficient and complete modeling of user interests. Our
method learns comprehensive user interests via cross-behavior and cross-feature
user sequence modeling. The mixer layers are stacked in a pyramid way for
cross-period user temporal interest learning. Through extensive offline and
online experiments, we demonstrate the effectiveness and efficiency of our
method, and we obtain a +0.106% improvement in user stay duration and a
+0.0113% increase in user active days in the online A/B test. The Pyramid Mixer
has been successfully deployed on the industrial platform, demonstrating its
scalability and impact in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective <span class="highlight-title">Recommendation</span> in the Era of Generative AI: A <span class="highlight-title">Survey</span> of
  Recent Progress and Future Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Hong, Yushi Wu, Zhiting Zhao, Shanshan Feng, Jianghong Ma, Jiao Liu, Tianjun Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent progress in generative artificial intelligence (Generative
AI), particularly in the development of large language models, recommendation
systems are evolving to become more versatile. Unlike traditional techniques,
generative AI not only learns patterns and representations from complex data
but also enables content generation, data synthesis, and personalized
experiences. This generative capability plays a crucial role in the field of
recommendation systems, helping to address the issue of data sparsity and
improving the overall performance of recommendation systems. Numerous studies
on generative AI have already emerged in the field of recommendation systems.
Meanwhile, the current requirements for recommendation systems have surpassed
the single utility of accuracy, leading to a proliferation of multi-objective
research that considers various goals in recommendation systems. However, to
the best of our knowledge, there remains a lack of comprehensive studies on
multi-objective recommendation systems based on generative AI technologies,
leaving a significant gap in the literature. Therefore, we investigate the
existing research on multi-objective recommendation systems involving
generative AI to bridge this gap. We compile current research on
multi-objective recommendation systems based on generative techniques,
categorizing them by objectives. Additionally, we summarize relevant evaluation
metrics and commonly used datasets, concluding with an analysis of the
challenges and future directions in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eSapiens: A Real-World NLP Framework for Multimodal Document
  Understanding and Enterprise Knowledge Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Shi, Zeyuan Li, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce eSapiens, a unified question-answering system designed for
enterprise settings, which bridges structured databases and unstructured
textual corpora via a dual-module architecture. The system combines a
Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)
pipeline, enabling natural language access to both relational data and
free-form documents. To enhance answer faithfulness, the RAG module integrates
dense and sparse retrieval, commercial reranking, and a citation verification
loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth
benchmark across five leading large language models (LLMs), analyzing
performance across key dimensions such as completeness, hallucination, and
context utilization. Results demonstrate that eSapiens outperforms a FAISS
baseline in contextual relevance and generation quality, with optional
strict-grounding controls for high-stakes scenarios. This work provides a
deployable framework for robust, citation-aware question answering in
real-world enterprise applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Contrastive Framework Of Item Tokenization For Generative
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penglong Zhai, Yifang Yuan, Fanyi Di, Jie Li, Yue Liu, Chen Li, Jie Huang, Sicong Wang, Yao Xu, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to quantize content embeddings
and significantly reduce the embedding size. However, reconstructive
quantization aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
quantization exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
quantization module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Effective Representations for Retrieval Using Self-Distillation
  with Adaptive Relevance Margins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Gienapp, Niklas Deckers, Martin Potthast, Harrisen Scells
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation-based retrieval models, so-called bi-encoders, estimate the
relevance of a document to a query by calculating the similarity of their
respective embeddings. Current state-of-the-art bi-encoders are trained using
an expensive training regime involving knowledge distillation from a teacher
model and batch-sampling. Instead of relying on a teacher model, we contribute
a novel parameter-free loss function for self-supervision that exploits the
pre-trained language modeling capabilities of the encoder model as a training
signal, eliminating the need for batch sampling by performing implicit hard
negative mining. We investigate the capabilities of our proposed approach
through extensive experiments, demonstrating that self-distillation can match
the effectiveness of teacher distillation using only 13.5% of the data, while
offering a speedup in training time between 3x and 15x compared to parametrized
losses. All code and data is made openly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 5 Tables, 6 Figures; published at ICTIR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScholarSearch: Benchmarking Scholar Searching Ability of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junting Zhou, Wang Li, Yiyan Liao, Nengyuan Zhang, Tingjia Miao, Zhihui Qi, Yuhan Wu, Tong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs)' search capabilities have garnered significant
attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on
general search scenarios and fail to adequately address the specific demands of
academic search. These demands include deeper literature tracing and
organization, professional support for academic databases, the ability to
navigate long-tail academic knowledge, and ensuring academic rigor. Here, we
proposed ScholarSearch, the first dataset specifically designed to evaluate the
complex information retrieval capabilities of Large Language Models (LLMs) in
academic research. ScholarSearch possesses the following key characteristics:
Academic Practicality, where question content closely mirrors real academic
learning and research environments, avoiding deliberately misleading models;
High Difficulty, with answers that are challenging for single models (e.g.,
Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring
at least three deep searches to derive; Concise Evaluation, where limiting
conditions ensure answers are as unique as possible, accompanied by clear
sources and brief solution explanations, greatly facilitating subsequent audit
and verification, surpassing the current lack of analyzed search datasets both
domestically and internationally; and Broad Coverage, as the dataset spans at
least 15 different academic disciplines. Through ScholarSearch, we expect to
more precisely measure and promote the performance improvement of LLMs in
complex academic information retrieval tasks. The data is available at:
https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental
  Learning for Document Retrieval <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable Search Index (DSI) utilizes pre-trained language models to
perform indexing and document retrieval via end-to-end learning without relying
on external indexes. However, DSI requires full re-training to index new
documents, causing significant computational inefficiencies. Continual learning
(CL) offers a solution by enabling the model to incrementally update without
full re-training. Existing CL solutions in document retrieval rely on memory
buffers or generative models for rehearsal, which is infeasible when accessing
previous training data is restricted due to privacy concerns. To this end, we
introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach
for document retrieval. PromptDSI follows the Prompt-based Continual Learning
(PCL) framework, using learnable prompts to efficiently index new documents
without accessing previous documents or queries. To improve retrieval latency,
we remove the initial forward pass of PCL, which otherwise greatly increases
training and inference time, with a negligible trade-off in performance.
Additionally, we introduce a novel topic-aware prompt pool that employs neural
topic embeddings as fixed keys, eliminating the instability of prompt key
optimization while maintaining competitive performance with existing PCL prompt
pools. In a challenging rehearsal-free continual learning setup, we demonstrate
that PromptDSI variants outperform rehearsal-based baselines, match the strong
cache-based baseline in mitigating forgetting, and significantly improving
retrieval performance on new corpora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML PKDD 2025 Research track. Camera-ready version. Code is
  available at https://github.com/LouisDo2108/PromptDSI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refining music sample identification with a self-supervised graph neural
  network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Bhattacharjee, Ivan Meresman Higgs, Mark Sandler, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic sample identification (ASID), the detection and identification of
portions of audio recordings that have been reused in new musical works, is an
essential but challenging task in the field of audio query-based retrieval.
While a related task, audio fingerprinting, has made significant progress in
accurately retrieving musical content under "real world" (noisy, reverberant)
conditions, ASID systems struggle to identify samples that have undergone
musical modifications. Thus, a system robust to common music production
transformations such as time-stretching, pitch-shifting, effects processing,
and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture
employing a Graph Neural Network within a contrastive learning framework. Our
model uses only 9% of the trainable parameters compared to the current
state-of-the-art system while achieving comparable performance, reaching a mean
average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of
an initial coarse similarity search for candidate selection, followed by a
cross-attention classifier that rejects irrelevant matches and refines the
ranking of retrieved candidates - an essential capability absent in prior
models. In addition, because queries in real-world applications are often short
in duration, we benchmark our system for short queries using new fine-grained
annotations for the Sample100 dataset, which we publish as part of this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference for Music Information Retrieval
  (ISMIR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepti Raghavan, Keshav Santhanam, Muhammad Shahir Rahman, Nayani Modugula, Luis Gaspar Schroeder, Maximilien Cura, Houjun Liu, Pratiksha Thaker, Philip Levis, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compound AI applications chain together subcomponents such as generative
language models, document retrievers, and embedding models. Applying
traditional systems optimizations such as parallelism and pipelining in
compound AI systems is difficult because each component has different
constraints in terms of the granularity and type of data that it ingests. New
data is often generated during intermediate computations, and text streams may
be split into smaller, independent fragments (such as documents to sentences)
which may then be re-aggregated at later parts of the computation. Due to this
complexity, existing systems to serve compound AI queries do not fully take
advantage of parallelism and pipelining opportunities.
  We present Alto, a framework that automatically optimizes execution of
compound AI queries through streaming and parallelism. Bento introduces a new
abstraction called nested ancestry, a metadata hierarchy that allows the system
to correctly track partial outputs and aggregate data across the heterogeneous
constraints of the components of compound AI applications. This metadata is
automatically inferred from the programming model, allowing developers to
express complex dataflow patterns without needing to reason manually about the
details of routing and aggregation. Implementations of four applications in
Alto outperform or match implementations in LangGraph, a popular existing AI
programming framework. Alto implementations match or improve latency by between
10-30%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Collapse to Stability: A Knowledge-Driven Ensemble Framework for
  Scaling Up Click-Through Rate Prediction Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Li, Lei Sang, Yi Zhang, Guangming Cui, Yiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction plays a crucial role in modern
recommender systems. While many existing methods utilize ensemble networks to
improve CTR model performance, they typically restrict the ensemble to only two
or three sub-networks. Whether increasing the number of sub-networks
consistently enhances CTR model performance to align with scaling laws remains
unclear. In this paper, we investigate larger ensemble networks and find three
inherent limitations in commonly used ensemble methods: (1) performance
degradation as the number of sub-networks increases; (2) sharp declines and
high variance in sub-network performance; and (3) significant discrepancies
between sub-network and ensemble predictions. Meanwhile, we analyze the
underlying causes of these limitations from the perspective of dimensional
collapse: the collapse within sub-networks becomes increasingly severe as the
number of sub-networks grows, leading to a lower knowledge abundance. In this
paper, we employ knowledge transfer methods, such as Knowledge Distillation
(KD) and Deep Mutual Learning (DML), to address the aforementioned limitations.
We find that KD enables CTR models to better follow scaling laws, while DML
reduces variance among sub-networks and minimizes discrepancies with ensemble
predictions. Furthermore, by combining KD and DML, we propose a model-agnostic
and hyperparameter-free Knowledge-Driven Ensemble Framework (KDEF) for CTR
Prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MTGR: Industrial-Scale Generative <span class="highlight-title">Recommendation</span> Framework in Meituan 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18654v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18654v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruidong Han, Bin Yin, Shangyu Chen, He Jiang, Fei Jiang, Xiang Li, Chi Ma, Mincong Huang, Xiaoguang Li, Chunzhen Jing, Yueming Han, Menglei Zhou, Lei Yu, Chuan Liu, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling law has been extensively validated in many domains such as natural
language processing and computer vision. In the recommendation system, recent
work has adopted generative recommendations to achieve scalability, but their
generative approaches require abandoning the carefully constructed cross
features of traditional recommendation models. We found that this approach
significantly degrades model performance, and scaling up cannot compensate for
it at all. In this paper, we propose MTGR (Meituan Generative Recommendation)
to address this issue. MTGR is modeling based on the HSTU architecture and can
retain the original deep learning recommendation model (DLRM) features,
including cross features. Additionally, MTGR achieves training and inference
acceleration through user-level compression to ensure efficient scaling. We
also propose Group-Layer Normalization (GLN) to enhance the performance of
encoding within different semantic spaces and the dynamic masking strategy to
avoid information leakage. We further optimize the training frameworks,
enabling support for our models with 10 to 100 times computational complexity
compared to the DLRM, without significant cost increases. MTGR achieved 65x
FLOPs for single-sample forward inference compared to the DLRM model, resulting
in the largest gain in nearly two years both offline and online. This
breakthrough was successfully deployed on Meituan, the world's largest food
delivery platform, where it has been handling the main traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenUP: Generative User Profilers as In-Context Learners for Next POI
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wilson Wongso, Hao Xue, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Point-of-Interest (POI) recommendation systems often lack
transparency, interpretability, and scrutability due to their reliance on dense
vector-based user embeddings. Furthermore, the cold-start problem -- where
systems have insufficient data for new users -- limits their ability to
generate accurate recommendations. Existing methods often address this by
leveraging similar trajectories from other users, but this approach can be
computationally expensive and increases the context length for LLM-based
methods, making them difficult to scale. To address these limitations, we
propose a method that generates natural language (NL) user profiles from
large-scale, location-based social network (LBSN) check-ins, utilizing robust
personality assessments and behavioral theories. These NL profiles capture user
preferences, routines, and behaviors, improving POI prediction accuracy while
offering enhanced transparency. By incorporating NL profiles as system prompts
to LLMs, our approach reduces reliance on extensive historical data, while
remaining flexible, easily updated, and computationally efficient. Our method
is not only competitive with other LLM-based methods but is also more scalable
for real-world POI recommender systems. Results demonstrate that our approach
consistently outperforms baseline methods, offering a more interpretable and
resource-efficient solution for POI recommendation systems. Our source code is
available at: https://github.com/w11wo/GenUP/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transient Concepts in Streaming Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aida Sheshbolouki, M. Tamer Ozsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Drift (CD) occurs when a change in a hidden context can induce
changes in a target concept. CD is a natural phenomenon in non-stationary
settings such as data streams. Understanding, detection, and adaptation to CD
in streaming data is (i) vital for effective and efficient analytics as
reliable output depends on adaptation to fresh input, (ii) challenging as it
requires efficient operations as well as effective performance evaluations, and
(iii) impactful as it applies to a variety of use cases and is a crucial
initial step for data management systems. Current works are mostly focused on
passive CD detection as part of supervised adaptation, on independently
generated data instances or graph snapshots, on target concepts as a function
of data labels, on static data management, and on specific temporal order of
data record. These methods do not always work. We revisit CD for the streaming
graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for
streaming graph CD detection and prediction. Both frameworks discern the change
of generative source. SGDD detects the CDs due to the changes of generative
parameters with significant delays such that it is difficult to evaluate the
performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds
ahead of their occurrence, without accessing the payloads of data records.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PUL: Pre-load in Software for Caches Wouldn't Always Play Along 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Bernhardt, Sajjad Tamimi, Florian Stock, Andreas Koch, Ilia Petrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory latencies and bandwidth are major factors, limiting system performance
and scalability. Modern CPUs aim at hiding latencies by employing large caches,
out-of-order execution, or complex hardware prefetchers. However,
software-based prefetching exhibits higher efficiency, improving with newer CPU
generations.
  In this paper we investigate software-based, post-Moore systems that offload
operations to intelligent memories. We show that software-based prefetching has
even higher potential in near-data processing settings by maximizing compute
utilization through compute/IO interleaving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Fact Attribution for Query Answering: Aggregate Queries and
  Novel Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Abramovich, Daniel Deutch, Nave Frost, Ahmet Kara, Dan Olteanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel approach to computing the contribution of
input tuples to the result of the query, quantified by the Banzhaf and Shapley
values. In contrast to prior algorithmic work that focuses on
Select-Project-Join-Union queries, ours is the first practical approach for
queries with aggregates. It relies on two novel optimizations that are
essential for its practicality and significantly improve the runtime
performance already for queries without aggregates. The first optimization
exploits the observation that many input tuples have the same contribution to
the query result, so it is enough to compute the contribution of one of them.
The second optimization uses the gradient of the query lineage to compute the
contributions of all tuples with the same complexity as for one of them.
Experiments with a million instances over 3 databases show that our approach
achieves up to 3 orders of magnitude runtime improvements over the
state-of-the-art for queries without aggregates, and that it is practical for
aggregate queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knapsack Optimization-based Schema Linking for <span class="highlight-title">LLM</span>-based Text-to-SQL
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Hao Chen, Zijin Hong, Qinggang Zhang, Feiran Huang, Qing Li, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating SQLs from user queries is a long-standing challenge, where the
accuracy of initial schema linking significantly impacts subsequent SQL
generation performance. However, current schema linking models still struggle
with missing relevant schema elements or an excess of redundant ones. A crucial
reason for this is that commonly used metrics, recall and precision, fail to
capture relevant element missing and thus cannot reflect actual schema linking
performance. Motivated by this, we propose enhanced schema linking metrics by
introducing a restricted missing indicator. Accordingly, we introduce Knapsack
optimization-based Schema Linking Approach (KaSLA), a plug-in schema linking
method designed to prevent the missing of relevant schema elements while
minimizing the inclusion of redundant ones. KaSLA employs a hierarchical
linking strategy that first identifies the optimal table linking and
subsequently links columns within the selected table to reduce linking
candidate space. In each linking process, it utilizes a knapsack optimization
approach to link potentially relevant elements while accounting for a limited
tolerance of potentially redundant ones. With this optimization, KaSLA-1.6B
achieves superior schema linking results compared to large-scale LLMs,
including deepseek-v3 with the state-of-the-art (SOTA) schema linking method.
Extensive experiments on Spider and BIRD benchmarks verify that KaSLA can
significantly improve the SQL generation performance of SOTA Text2SQL models by
substituting their schema linking processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linking Data Citation to Repository Visibility: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fakhri Momeni, Janete Saldanha Bach, Brigitte Mathiak, Peter Mutschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's data-driven research landscape, dataset visibility and
accessibility play a crucial role in advancing scientific knowledge. At the
same time, data citation is essential for maintaining academic integrity,
acknowledging contributions, validating research outcomes, and fostering
scientific reproducibility. As a critical link, it connects scholarly
publications with the datasets that drive scientific progress. This study
investigates whether repository visibility influences data citation rates. We
hypothesize that repositories with higher visibility, as measured by search
engine metrics, are associated with increased dataset citations. Using OpenAlex
data and repository impact indicators (including the visibility index from
Sistrix, the h-index of repositories, and citation metrics such as mean and
median citations), we analyze datasets in Social Sciences and Economics to
explore their relationship. Our findings suggest that datasets hosted on more
visible web domains tend to receive more citations, with a positive correlation
observed between web domain visibility and dataset citation counts,
particularly for datasets with at least one citation. However, when analyzing
domain-level citation metrics, such as the h-index, mean, and median citations,
the correlations are inconsistent and weaker. While higher visibility domains
tend to host datasets with greater citation impact, the distribution of
citations across datasets varies significantly. These results suggest that
while visibility plays a role in increasing citation counts, it is not the sole
factor influencing dataset citation impact. Other elements, such as dataset
quality, research trends, and disciplinary norms, can also contribute to
citation patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AQETuner: Reliable Query-level Configuration Tuning for Analytical Query
  Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixiang Chen, Yuxing Han, Yu Chen, Xing Chen, Chengcheng Yang, Weining Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern analytical query engines (AQEs) are essential for large-scale data
analysis and processing. These systems usually provide numerous query-level
tunable knobs that significantly affect individual query performance. While
several studies have explored automatic DBMS configuration tuning, they have
several limitations to handle query-level tuning. Firstly, they fail to capture
how knobs influence query plans, which directly affect query performance.
Secondly, they overlook query failures during the tuning processing, resulting
in low tuning efficiency. Thirdly, they struggle with cold-start problems for
new queries, leading to prolonged tuning time. To address these challenges, we
propose AQETuner, a novel Bayesian Optimization-based system tailored for
reliable query-level knob tuning in AQEs. AQETuner first applies the attention
mechanisms to jointly encode the knobs and plan query, effectively identifying
the impact of knobs on plan nodes. Then, AQETuner employs a dual-task Neural
Process to predict both query performance and failures, leveraging their
interactions to guide the tuning process. Furthermore, AQETuner utilizes
Particle Swarm Optimization to efficiently generate high-quality samples in
parallel during the initial tuning stage for the new queries. Experimental
results show that AQETuner significantly outperforms existing methods, reducing
query latency by up to 23.7% and query failures by up to 51.2%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Use of Yannakakis' Algorithm to Improve Query Performance:
  Machine Learning to the Rescue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Böhm, Georg Gottlob, Matthias Lanzinger, Davide Longo, Cem Okulmus, Reinhard Pichler, Alexander Selzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query optimization has played a central role in database research for
decades. However, more often than not, the proposed optimization techniques
lead to a performance improvement in some, but not in all, situations.
Therefore, we urgently need a methodology for designing a decision procedure
that decides for a given query whether the optimization technique should be
applied or not.
  In this work, we propose such a methodology with a focus on Yannakakis-style
query evaluation as our optimization technique of interest. More specifically,
we formulate this decision problem as an algorithm selection problem and we
present a Machine Learning based approach for its solution. Empirical results
with several benchmarks on a variety of database systems show that our approach
indeed leads to a statistically significant performance improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influential Slot and Tag Selection in Billboard Advertisement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dildar Ali, Suman Banerjee, Yamuna Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The selection of influential billboard slots remains an important problem in
billboard advertisements. Existing studies on this problem have not considered
the case of context-specific influence probability. To bridge this gap, in this
paper, we introduce the Context Dependent Influential Billboard Slot Selection
Problem. First, we show that the problem is NP-hard. We also show that the
influence function holds the bi-monotonicity, bi-submodularity, and
non-negativity properties. We propose an orthant-wise Stochastic Greedy
approach to solve this problem. We show that this method leads to a
constant-factor approximation guarantee. Subsequently, we propose an
orthant-wise Incremental and Lazy Greedy approach. In a generic sense, this is
a method for maximizing a bi-submodular function under the cardinality
constraint, which may also be of independent interest. We analyze the
performance guarantee of this algorithm as well as time and space complexity.
The proposed solution approaches have been implemented with real-world
billboard and trajectory datasets. We compare the performance of our method
with several baseline methods, and the results are reported. Our proposed
orthant-wise stochastic greedy approach leads to significant results when the
parameters are set properly with reasonable computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-19T00:00:00Z">2025-06-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Outlier Removal with Embedding Models and <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eren Akbiyik, João Almeida, Rik Melis, Ritu Sriram, Viviana Petrescu, Vilhjálmur Vilhjálmsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 63rd Annual Meeting of the Association for
  Computational Linguistics (ACL 2025) Industry Track, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revela: Dense Retriever Learning via Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyu Cai, Tong Chen, Xinran Zhao, Sihao Chen, Hongming Zhang, Sherry Tongshuang Wu, Iryna Gurevych, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Personalisation of <span class="highlight-title">Cross</span>-Channel Marketing Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sami Abboud, Eleanor Hanna, Olivier Jeunen, Vineesha Raheja, Schaun Wheeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Influence of Knowledge Graph Information on Relation
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cedric Möller, Ricardo Usbeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Prioritisation for Web Crawling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesza Pezzuti, Sean MacAvaney, Nicola Tonellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the vast scale of the Web, crawling prioritisation techniques based on
link graph traversal, popularity, link analysis, and textual content are
frequently applied to surface documents that are most likely to be valuable.
While existing techniques are effective for keyword-based search, both
retrieval methods and user search behaviours are shifting from keyword-based
matching to natural language semantic matching. The remarkable success of
applying semantic matching and quality signals during ranking leads us to
hypothesize that crawling could be improved by prioritizing Web pages with high
semantic quality. To investigate this, we propose a semantic quality-driven
prioritisation technique to enhance the effectiveness of crawling and align the
crawler behaviour with recent shift towards natural language search. We embed
semantic understanding directly into the crawling process -- leveraging recent
neural semantic quality estimators to prioritise the crawling frontier -- with
the goal of surfacing content that is semantically rich and valuable for modern
search needs. Our experiments on the English subset of ClueWeb22-B and the
Researchy Questions query set show that, compared to existing crawling
techniques, neural crawling policies significantly improve harvest rate,
maxNDCG, and search effectiveness during the early stages of crawling.
Meanwhile, crawlers based on our proposed neural policies maintain comparable
search performance on keyword queries from the MS MARCO Web Search query set.
While this work does not propose a definitive and complete solution, it
presents a forward-looking perspective on Web crawling and opens the door to a
new line of research on leveraging semantic analysis to effectively align
crawlers with the ongoing shift toward natural language search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ACM ICTIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> GFlowGR: Fine-tuning Generative <span class="highlight-title">Recommendation</span> Frameworks with
  Generative Flow Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejing Wang, Shengyu Zhou, Jinyu Lu, Qidong Liu, Xinhang Li, Wenlin Zhang, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, <span class="highlight-author">Xiangyu Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal
  Document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishesh Tripathi, Tanmay Odapally, Indraneel Das, Uday Allu, Biddwan Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 Figure, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEP-GCN: Leveraging Similar Edge Pairs with Temporal and Spatial
  Contexts for Location-Based Recommender Systems <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tan Loc Nguyen, Tin T. Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a crucial role in enabling personalized content
delivery amidst the challenges of information overload and human mobility.
Although conventional methods often rely on interaction matrices or graph-based
retrieval, recent approaches have sought to exploit contextual signals such as
time and location. However, most existing models focus on node-level
representation or isolated edge attributes, underutilizing the relational
structure between interactions. We propose SEP-GCN, a novel graph-based
recommendation framework that learns from pairs of contextually similar
interaction edges, each representing a user-item check-in event. By identifying
edge pairs that occur within similar temporal windows or geographic proximity,
SEP-GCN augments the user-item graph with contextual similarity links. These
links bridge distant but semantically related interactions, enabling improved
long-range information propagation. The enriched graph is processed via an
edge-aware convolutional mechanism that integrates contextual similarity into
the message-passing process. This allows SEP-GCN to model user preferences more
accurately and robustly, especially in sparse or dynamic environments.
Experiments on benchmark data sets show that SEP-GCN consistently outperforms
strong baselines in both predictive accuracy and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ACM SIGIR Conference on Innovative Concepts and Theories
  in Information Retrieval (ICTIR) 2025, Padua, Itay</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive
  Awareness Capabilities <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiancheng Ruan, Tingyang Chen, Renchi Yang, Xiangyu Ke, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds
extensive applications in databases, information retrieval, recommender
systems, etc. While graph-based methods have emerged as the leading solution
for ANNS due to their superior query performance, they still face several
challenges, such as struggling with local optima and redundant computations.
These issues arise because existing methods (i) fail to fully exploit the
topological information underlying the proximity graph G, and (ii) suffer from
severe distribution mismatches between the base data and queries in practice.
  To this end, this paper proposes GATE, high-tier proximity Graph with
Adaptive Topology and Query AwarEness, as a lightweight and adaptive module
atop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates
the critical problem to identify an optimal entry point in the proximity graph
for a given query, facilitating faster online search. By leveraging the
inherent clusterability of high-dimensional data, GATE first extracts a small
set of hub nodes V as candidate entry points. Then, resorting to a contrastive
learning-based two-tower model, GATE encodes both the structural semantics
underlying G and the query-relevant features into the latent representations of
these hub nodes V. A navigation graph index on V is further constructed to
minimize the model inference overhead. Extensive experiments demonstrate that
GATE achieves a 1.2-2.0X speed-up in query performance compared to
state-of-the-art graph-based indexes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accecpted by KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-Branch Cooperation for Enhanced Click-Through Rate
  Prediction at Taobao 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Chen, Zida Cheng, Yuangang Pan, Shuai Xiao, Xiaoming Liu, Jinsong Lan, Xiaoyong Zhu, Bo Zheng, Ivor W. Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing click-through rate (CTR) prediction works have studied the role of
feature interaction through a variety of techniques. Each interaction technique
exhibits its own strength, and solely using one type usually constrains the
model's capability to capture the complex feature relationships, especially for
industrial data with enormous input feature fields. Recent research shows that
effective CTR models often combine an MLP network with a dedicated feature
interaction network in a two-parallel structure. However, the interplay and
cooperative dynamics between different streams or branches remain
under-researched. In this work, we introduce a novel Multi-Branch Cooperation
Network (MBCnet) which enables multiple branch networks to collaborate with
each other for better complex feature interaction modeling. Specifically,
MBCnet consists of three branches: the Extensible Feature Grouping and Crossing
(EFGC) branch that promotes the model's memorization ability of specific
feature fields, the low rank Cross Net branch and Deep branch to enhance
explicit and implicit feature crossing for improved generalization. Among these
branches, a novel cooperation scheme is proposed based on two principles:
Branch co-teaching and moderate differentiation. Branch co-teaching encourages
well-learned branches to support poorly-learned ones on specific training
samples. Moderate differentiation advocates branches to maintain a reasonable
level of difference in their feature representations on the same inputs. This
cooperation strategy improves learning through mutual knowledge sharing and
boosts the discovery of diverse feature interactions across branches.
Experiments on large-scale industrial datasets and online A/B test at Taobao
app demonstrate MBCnet's superior performance, delivering a 0.09 point increase
in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes are available
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Multi-Positive Contrastive Learning for Patent Image
  Retrieval <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Kavimandan, Angelos Nalmpantis, Emma Beauxis-Aussalet, Robert-Jan Sips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patent images are technical drawings that convey information about a patent's
innovation. Patent image retrieval systems aim to search in vast collections
and retrieve the most relevant images. Despite recent advances in information
retrieval, patent images still pose significant challenges due to their
technical intricacies and complex semantic information, requiring efficient
fine-tuning for domain adaptation. Current methods neglect patents'
hierarchical relationships, such as those defined by the Locarno International
Classification (LIC) system, which groups broad categories (e.g., "furnishing")
into subclasses (e.g., "seats" and "beds") and further into specific patent
designs. In this work, we introduce a hierarchical multi-positive contrastive
loss that leverages the LIC's taxonomy to induce such relations in the
retrieval process. Our approach assigns multiple positive pairs to each patent
image within a batch, with varying similarity scores based on the hierarchical
taxonomy. Our experimental analysis with various vision and multimodal models
on the DeepPatent2 dataset shows that the proposed method enhances the
retrieval results. Notably, our method is effective with low-parameter models,
which require fewer computational resources and can be deployed on environments
with limited hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on
  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located
  with SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrunkAgent: Stealthy Memory Corruption in <span class="highlight-title">LLM</span>-Powered Recommender Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyi Yang, Zhibo Hu, Xinshu Li, Chen Wang, Tong Yu, Xiwei Xu, Liming Zhu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM)-powered agents are increasingly used in
recommender systems (RSs) to achieve personalized behavior modeling, where the
memory mechanism plays a pivotal role in enabling the agents to autonomously
explore, learn and self-evolve from real-world interactions. However, this very
mechanism, serving as a contextual repository, inherently exposes an attack
surface for potential adversarial manipulations. Despite its central role, the
robustness of agentic RSs in the face of such threats remains largely
underexplored. Previous works suffer from semantic mismatches or rely on static
embeddings or pre-defined prompts, all of which hinder their applicability to
systems with dynamic memory states. This challenge is exacerbated by the
black-box nature of commercial RSs.
  To tackle the above problems, in this paper, we present the first systematic
investigation of memory-based vulnerabilities in LLM-powered recommender
agents, revealing their security limitations and guiding efforts to strengthen
system resilience and trustworthiness. Specifically, we propose a novel
black-box attack framework named DrunkAgent. DrunkAgent crafts semantically
meaningful adversarial textual triggers for target item promotions and
introduces a series of strategies to maximize the trigger effect by corrupting
the memory updates during the interactions. The triggers and strategies are
optimized on a surrogate model, enabling DrunkAgent transferable and stealthy.
Extensive experiments on real-world datasets across diverse agentic RSs,
including collaborative filtering, retrieval augmentation and sequential
recommendations, demonstrate the generalizability, transferability and
stealthiness of DrunkAgent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HSTU-BLaIR: Lightweight Contrastive Text Embedding for Generative
  Recommender <span class="chip">KDD
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in recommender systems have underscored the complementary
strengths of generative modeling and pretrained language models. We propose
HSTU-BLaIR, a hybrid framework that augments the Hierarchical Sequential
Transduction Unit (HSTU)-based generative recommender with BLaIR, a lightweight
contrastive text embedding model. This integration enriches item
representations with semantic signals from textual metadata while preserving
HSTU's powerful sequence modeling capabilities.
  We evaluate HSTU-BLaIR on two e-commerce datasets: three subsets from the
Amazon Reviews 2023 dataset and the Steam dataset. We compare its performance
against both the original HSTU-based recommender and a variant augmented with
embeddings from OpenAI's state-of-the-art \texttt{text-embedding-3-large}
model. Despite the latter being trained on a substantially larger corpus with
significantly more parameters, our lightweight BLaIR-enhanced approach --
pretrained on domain-specific data -- achieves better performance in nearly all
cases. Specifically, HSTU-BLaIR outperforms the OpenAI embedding-based variant
on all but one metric, where it is marginally lower, and matches it on another.
These findings highlight the effectiveness of contrastive text embeddings in
compute-efficient recommendation settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Workshop on Large Language Models for E-Commerce, KDD
  2025. Code available at https://www.github.com/snapfinger/HSTU-BLaIR</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Deep Learning: Challenges, Foundations and Next-Generation
  Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Prakash Dwivedi, Charilaos Kanatsoulis, Shenyang Huang, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDI: Localized Data Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Omidvartehrani, Davood Rafiei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing values are a common challenge in real-world tabular data and can
significantly impair downstream analysis. While Large Language Models (LLMs)
have recently shown promise in data imputation, existing methods often rely on
broad, unfiltered prompts that compromise accuracy, scalability, and
explainability. We introduce LDI (Localized Data Imputation), a novel framework
that improves both the accuracy and transparency of LLM-based imputation by
selecting a compact, contextually relevant subset of attributes and tuples for
each missing value. This localized prompting reduces noise, enables
traceability by revealing which data influenced each prediction, and is
effective across both hosted LLMs and lightweight local models. Our extensive
experiments on four real-world datasets show that LDI outperforms
state-of-the-art methods, achieving up to 8% higher accuracy when using hosted
LLMs. The gains are more substantial with lightweight local models, reaching
nearly 17% and 97% accuracy on some datasets when using 3 and 10 examples,
respectively. In addition to higher accuracy, LDI offers improved
interpretability and robustness to data inconsistencies, making it well-suited
for high-stakes and privacy-sensitive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REIS: A High-Performance and Energy-Efficient Retrieval System with
  In-Storage Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangqi Chen, Andreas Kosmas Kakolyris, Rakesh Nadig, Manos Frouzakis, Nika Mansouri Ghiasi, Yu Liang, Haiyu Mao, Jisung Park, Mohammad Sadrosadati, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of our publication at the 52nd International
  Symposium on Computer Architecture (ISCA-52), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PBench: Workload Synthesizer with Real Statistics for Cloud Analytics
  Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhou, Chunwei Liu, Bhuvan Urgaonkar, Zhengle Wang, Magnus Mueller, Chao Zhang, Songyue Zhang, Pascal Pfeil, Dominik Horn, Zhengchun Liu, Davide Pagano, Tim Kraska, Samuel Madden, Ju Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud service providers commonly use standard benchmarks like TPC-H and
TPC-DS to evaluate and optimize cloud data analytics systems. However, these
benchmarks rely on fixed query patterns and fail to capture the real execution
statistics of production cloud workloads. Although some cloud database vendors
have recently released real workload traces, these traces alone do not qualify
as benchmarks, as they typically lack essential components like the original
SQL queries and their underlying databases. To overcome this limitation, this
paper introduces a new problem of workload synthesis with real statistics,
which aims to generate synthetic workloads that closely approximate real
execution statistics, including key performance metrics and operator
distributions, in real cloud workloads. To address this problem, we propose
PBench, a novel workload synthesizer that constructs synthetic workloads by
judiciously selecting and combining workload components (i.e., queries and
databases) from existing benchmarks. This paper studies the key challenges in
PBench. First, we address the challenge of balancing performance metrics and
operator distributions by introducing a multi-objective optimization-based
component selection method. Second, to capture the temporal dynamics of real
workloads, we design a timestamp assignment method that progressively refines
workload timestamps. Third, to handle the disparity between the original
workload and the candidate workload, we propose a component augmentation
approach that leverages large language models (LLMs) to generate additional
workload components while maintaining statistical fidelity. We evaluate PBench
on real cloud workload traces, demonstrating that it reduces approximation
error by up to 6x compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Verification in Ontology-Based Process Models with Parameter
  Interdependencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Jeleniewski, Hamied Nabizada, Jonathan Reif, Felix Gehlhoff, Alexander Fay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at IEEE ETFA 2025 and will be published in the
  conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Data to Decision: Data-Centric Infrastructure for Reproducible ML
  in Collaborative eScience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Carl Kesselman, Tran Huy Nguyen, Benjamin Yixing Xu, Kyle Bolo, Kimberley Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Epistemology with Weighted Authority: A Formal Architecture for
  Truth-Promoting Autonomous Scientific Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig S. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>91 pages, 0 figures, includes mathematical appendix and formal
  proofs. Designed as a foundational submission for a modular autonomous
  epistemic reasoning system. Suitable for logic in computer science, AI
  epistemology, and scientific informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Agnostic Cardinality Learning from Imperfect Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhi Wu, Rong Kang, Tieying Zhang, Jianjun Chen, Ryan Marcus, Zachary G. Ives
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. Technical Report (Extended Version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filter-Centric Vector Indexing: Geometric Transformation for Efficient
  Filtered Vector Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Heidari, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of vector search applications demands efficient handling
of combined vector similarity and attribute filtering; a challenge where
current approaches force an unsatisfying choice between performance and
accuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework
that transforms this fundamental trade-off by directly encoding filter
conditions into the vector space through a mathematically principled
transformation $\psi(v, f, \alpha)$. Unlike specialized solutions, FCVI works
with any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical
guarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI
achieves 2.6-3.0 times higher throughput than state-of-the-art methods while
maintaining comparable recall. More remarkably, FCVI exhibits exceptional
stability under distribution shifts; maintaining consistent performance when
filter patterns or vector distributions change, unlike traditional approaches
that degrade significantly. This combination of performance, compatibility, and
resilience positions FCVI as an immediately applicable solution for production
vector search systems requiring flexible filtering capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive
  Awareness Capabilities <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiancheng Ruan, Tingyang Chen, Renchi Yang, Xiangyu Ke, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds
extensive applications in databases, information retrieval, recommender
systems, etc. While graph-based methods have emerged as the leading solution
for ANNS due to their superior query performance, they still face several
challenges, such as struggling with local optima and redundant computations.
These issues arise because existing methods (i) fail to fully exploit the
topological information underlying the proximity graph G, and (ii) suffer from
severe distribution mismatches between the base data and queries in practice.
  To this end, this paper proposes GATE, high-tier proximity Graph with
Adaptive Topology and Query AwarEness, as a lightweight and adaptive module
atop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates
the critical problem to identify an optimal entry point in the proximity graph
for a given query, facilitating faster online search. By leveraging the
inherent clusterability of high-dimensional data, GATE first extracts a small
set of hub nodes V as candidate entry points. Then, resorting to a contrastive
learning-based two-tower model, GATE encodes both the structural semantics
underlying G and the query-relevant features into the latent representations of
these hub nodes V. A navigation graph index on V is further constructed to
minimize the model inference overhead. Extensive experiments demonstrate that
GATE achieves a 1.2-2.0X speed-up in query performance compared to
state-of-the-art graph-based indexes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accecpted by KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimBank: from Simulation to Solution in Prescriptive Process Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prescriptive Process Monitoring (PresPM) is an emerging area within Process
Mining, focused on optimizing processes through real-time interventions for
effective decision-making. PresPM holds significant promise for organizations
seeking enhanced operational performance. However, the current literature faces
two key limitations: a lack of extensive comparisons between techniques and
insufficient evaluation approaches. To address these gaps, we introduce
SimBank: a simulator designed for accurate benchmarking of PresPM methods.
Modeled after a bank's loan application process, SimBank enables extensive
comparisons of both online and offline PresPM methods. It incorporates a
variety of intervention optimization problems with differing levels of
complexity and supports experiments on key causal machine learning challenges,
such as assessing a method's robustness to confounding in data. SimBank
additionally offers a comprehensive evaluation capability: for each test case,
it can generate the true outcome under each intervention action, which is not
possible using recorded datasets. The simulator incorporates parallel
activities and loops, drawing from common logs to generate cases that closely
resemble real-life process instances. Our proof of concept demonstrates
SimBank's benchmarking capabilities through experiments with various PresPM
methods across different interventions, highlighting its value as a publicly
available simulator for advancing research and practice in PresPM.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-18T00:00:00Z">2025-06-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense,
  and Human Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushaan Singh Kalra, Xinran Zhao, To Eun Kim, Fengyu Cai, Fernando Diaz, Tongshuang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEM1: Learning to Synergize Memory and Reasoning for Efficient
  Long-Horizon Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, Paul Pu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architecture is All You Need: Improving <span class="highlight-title">LLM</span> Recommenders by Dropping the
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Foley, Shaghayegh Agah, Kavya Priyanka Kakinada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an explosion of interest in the applications
of large pre-trained language models (PLMs) to recommender systems, with many
studies showing strong performance of PLMs on common benchmark datasets.
PLM-based recommender models benefit from flexible and customizable prompting,
an unlimited vocabulary of recommendable items, and general ``world knowledge''
acquired through pre-training on massive text corpora. While PLM-based
recommenders show promise in settings where data is limited, they are hard to
implement in practice due to their large size and computational cost.
Additionally, fine-tuning PLMs to improve performance on collaborative signals
may degrade the model's capacity for world knowledge and generalizability. We
propose a recommender model that uses the architecture of large language models
(LLMs) while reducing layer count and dimensions and replacing the text-based
subword tokenization of a typical LLM with discrete tokens that uniquely
represent individual content items. We find that this simplified approach
substantially outperforms both traditional sequential recommender models and
PLM-based recommender models at a tiny fraction of the size and computational
complexity of PLM-based models. Our results suggest that the principal benefit
of LLMs in recommender systems is their architecture, rather than the world
knowledge acquired during extensive pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ cAST: Enhancing Code Retrieval-Augmented Generation with Structural
  Chunking via Abstract Syntax Tree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Zhang, Xinran Zhao, Zora Zhiruo Wang, Chenyang Yang, Jiayi Wei, Tongshuang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Interest <span class="highlight-title">Recommendation</span>: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Qiang Chen, Lixin Zou, Aixin Sun, Chenliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing recommendation methods often struggle to model users' multifaceted
preferences due to the diversity and volatility of user behavior, as well as
the inherent uncertainty and ambiguity of item attributes in practical
scenarios. Multi-interest recommendation addresses this challenge by extracting
multiple interest representations from users' historical interactions, enabling
fine-grained preference modeling and more accurate recommendations. It has
drawn broad interest in recommendation research. However, current
recommendation surveys have either specialized in frontier recommendation
methods or delved into specific tasks and downstream applications. In this
work, we systematically review the progress, solutions, challenges, and future
directions of multi-interest recommendation by answering the following three
questions: (1) Why is multi-interest modeling significantly important for
recommendation? (2) What aspects are focused on by multi-interest modeling in
recommendation? and (3) How can multi-interest modeling be applied, along with
the technical details of the representative modules? We hope that this survey
establishes a fundamental framework and delivers a preliminary overview for
researchers interested in this field and committed to further exploration. The
implementation of multi-interest recommendation summarized in this survey is
maintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next-User Retrieval: Enhancing Cold-Start <span class="highlight-title">Recommendation</span>s via Generative
  Next-User Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Ting Lan, Yang Huo, Yi Shen, Xiao Yang, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The item cold-start problem is critical for online recommendation systems, as
the success of this phase determines whether high-quality new items can
transition to popular ones, receive essential feedback to inspire creators, and
thus lead to the long-term retention of creators. However, modern
recommendation systems still struggle to address item cold-start challenges due
to the heavy reliance on item and historical interactions, which are
non-trivial for cold-start items lacking sufficient exposure and feedback.
Lookalike algorithms provide a promising solution by extending feedback for new
items based on lookalike users. Traditional lookalike algorithms face such
limitations: (1) failing to effectively model the lookalike users and further
improve recommendations with the existing rule- or model-based methods; and (2)
struggling to utilize the interaction signals and incorporate diverse features
in modern recommendation systems.
  Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel
framework for enhancing cold-start recommendations via generative next-user
modeling. Specifically, we employ a transformer-based model to capture the
unidirectional relationships among recently interacted users and utilize these
sequences to generate the next potential user who is most likely to interact
with the item. The additional item features are also integrated as prefix
prompt embeddings to assist the next-user generation. The effectiveness of
Next-User Retrieval is evaluated through both offline experiments and online
A/B tests. Our method achieves significant improvements with increases of
0.0142% in daily active users and +0.1144% in publications in Douyin,
showcasing its practical applicability and scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Loss Functions in Recommender Systems: A Comparative Study
  with a Rényi Divergence-Based Solution <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjia Zhang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loss functions play a pivotal role in optimizing recommendation models. Among
various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are
particularly effective. Their theoretical connections and differences warrant
in-depth exploration. This work conducts comprehensive analyses of these
losses, yielding significant insights: 1) Common strengths -- both can be
viewed as augmentations of traditional losses with Distributional Robust
Optimization (DRO), enhancing robustness to distributional shifts; 2)
Respective limitations -- stemming from their use of different distribution
distance metrics in DRO optimization, SL exhibits high sensitivity to false
negative instances, whereas CCL suffers from low data utilization. To address
these limitations, this work proposes a new loss function, DrRL, which
generalizes SL and CCL by leveraging R\'enyi-divergence in DRO optimization.
DrRL incorporates the advantageous structures of both SL and CCL, and can be
demonstrated to effectively mitigate their limitations. Extensive experiments
have been conducted to validate the superiority of DrRL on both recommendation
accuracy and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contributions to Representation Learning with Graph Autoencoders and
  Applications to Music <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
two powerful groups of unsupervised node embedding methods, with various
applications to graph-based machine learning problems such as link prediction
and community detection. Nonetheless, at the beginning of this Ph.D. project,
GAE and VGAE models were also suffering from key limitations, preventing them
from being adopted in the industry. In this thesis, we present several
contributions to improve these models, with the general aim of facilitating
their use to address industrial-level problems involving graph representations.
Firstly, we propose two strategies to overcome the scalability issues of
previous GAE and VGAE models, permitting to effectively train these models on
large graphs with millions of nodes and edges. These strategies leverage graph
degeneracy and stochastic subgraph decoding techniques, respectively. Besides,
we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of
these models for directed graphs, that are ubiquitous in industrial
applications. We also consider extensions of GAE and VGAE models for dynamic
graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily
complex, and we propose to simplify them by leveraging linear encoders. Lastly,
we introduce Modularity-Aware GAE and VGAE to improve community detection on
graphs, while jointly preserving good performances on link prediction. In the
last part of this thesis, we evaluate our methods on several graphs extracted
from the music streaming service Deezer. We put the emphasis on graph-based
music recommendation problems. In particular, we show that our methods can
improve the detection of communities of similar musical items to recommend to
users, that they can effectively rank similar artists in a cold start setting,
and that they permit modeling the music genre perception across cultures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ph.D. thesis defended at \'Ecole Polytechnique (IPP) in March 2022.
  As mentioned in this thesis, several chapters present results also published
  in scientific articles written with co-authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OM4OV: Leveraging Ontology Matching for Ontology Versioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the dynamic nature of the Semantic Web, version control is necessary
to capture time-varying information, particularly for widely used ontologies.
Despite the long-standing recognition of ontology versioning (OV) as a crucial
component for efficient ontology management, the growing size of ontologies and
accumulating errors caused by manual labour overwhelm current OV approaches. In
this paper, we propose yet another approach to performing OV using existing
ontology matching (OM) techniques and systems. We introduce a unified OM4OV
pipeline. From an OM perspective, we reconstruct a new task formulation and
measurement for OV tasks. Building upon the prior alignment(s) from OM, we
propose a pipeline optimisation method called the cross-reference (CR)
mechanism to enhance overall OV performance. We experimentally validate the
OM4OV pipeline and the cross-reference mechanism in the OV tested originating
from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also
discuss insights into OM used for OV tasks, where some false mappings detected
by OV systems are not actually untrue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Next POI <span class="highlight-title">Recommendation</span> with Semantic ID <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Wang, Yuxi Huang, Shen Gao, Yifan Wang, Chengrui Huang, Shuo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-of-interest (POI) recommendation systems aim to predict the next
destinations of user based on their preferences and historical check-ins.
Existing generative POI recommendation methods usually employ random numeric
IDs for POIs, limiting the ability to model semantic relationships between
similar locations. In this paper, we propose Generative Next POI Recommendation
with Semantic ID (GNPR-SID), an LLM-based POI recommendation model with a novel
semantic POI ID (SID) representation method that enhances the semantic
understanding of POI modeling. There are two key components in our GNPR-SID:
(1) a Semantic ID Construction module that generates semantically rich POI IDs
based on semantic and collaborative features, and (2) a Generative POI
Recommendation module that fine-tunes LLMs to predict the next POI using these
semantic IDs. By incorporating user interaction patterns and POI semantic
features into the semantic ID generation, our method improves the
recommendation accuracy and generalization of the model. To construct
semantically related SIDs, we propose a POI quantization method based on
residual quantized variational autoencoder, which maps POIs into a discrete
semantic space. We also propose a diversity loss to ensure that SIDs are
uniformly distributed across the semantic space. Extensive experiments on three
benchmark datasets demonstrate that GNPR-SID substantially outperforms
state-of-the-art methods, achieving up to 16% improvement in recommendation
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, the paper has been accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERGE -- A Bimodal Audio-Lyrics Dataset for Static Music Emotion
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06060v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06060v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Lima Louro, Hugo Redinho, Ricardo Santos, Ricardo Malheiro, Renato Panda, Rui Pedro Paiva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Music Emotion Recognition (MER) field has seen steady developments in
recent years, with contributions from feature engineering, machine learning,
and deep learning. The landscape has also shifted from audio-centric systems to
bimodal ensembles that combine audio and lyrics. However, a lack of public,
sizable and quality-controlled bimodal databases has hampered the development
and improvement of bimodal audio-lyrics systems. This article proposes three
new audio, lyrics, and bimodal MER research datasets, collectively referred to
as MERGE, which were created using a semi-automatic approach. To
comprehensively assess the proposed datasets and establish a baseline for
benchmarking, we conducted several experiments for each modality, using feature
engineering, machine learning, and deep learning methodologies. Additionally,
we propose and validate fixed train-validation-test splits. The obtained
results confirm the viability of the proposed datasets, achieving the best
overall result of 81.74\% F1-score for bimodal classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures, 8 tables, submitted to IEEE Transactions on
  Affective Computing</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delta: A Learned Mixed Cost-based Query Optimization Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhen Peng, Zheng Qu, Xiaoye Miao, Rong Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query optimizer is a crucial module for database management systems. Existing
optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic
programming with cost models but face search space explosion and heuristic
pruning constraints; (2) value-based ones train value networks to enable
efficient beam search, but incur higher training costs and lower accuracy. They
also lack mechanisms to detect queries where they may perform poorly. To
determine more efficient plans, we propose Delta, a mixed cost-based query
optimization framework that consists of a compatible query detector and a
two-stage planner. Delta first employs a Mahalanobis distancebased detector to
preemptively filter out incompatible queries where the planner might perform
poorly. For compatible queries, Delta activates its two-stage mixed cost-based
planner. Stage I serves as a coarse-grained filter to generate high-quality
candidate plans based on the value network via beam search, relaxing precision
requirements and narrowing the search space. Stage II employs a fine-grained
ranker to determine the best plan from the candidate plans based on a learned
cost model. Moreover, to reduce training costs, we reuse and augment the
training data from stage I to train the model in stage II. Experimental results
on three workloads demonstrate that Delta identifies higher-quality plans,
achieving an average 2.34x speedup over PostgreSQL and outperforming the
state-of-the-art learned methods by 2.21x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Anomaly Detection in the Presence of Concept Drift: Extended
  Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongjun Park, Fei Chiang, Mostafa Milani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data changes to reflect evolving user behaviour, preferences, and changes in
the environment. Such changes may occur due to expected shifts in the data
distribution, i.e., concept drift, or unexpected anomalous changes. The
presence of concept drift poses challenges for anomaly detection in time
series. While anomalies are caused by undesirable changes in the data,
differentiating abnormal changes from varying normal behaviours is difficult
due to differing frequencies of occurrence, varying time intervals when normal
patterns occur. Differentiating between concept drift and anomalies is critical
for accurate analysis as studies have shown that the compounding effects of
error propagation in downstream data analysis tasks lead to lower detection
accuracy and increased overhead due to unnecessary model updates.
Unfortunately, existing work has largely explored anomaly detection and concept
drift detection in isolation. We develop AnDri, a system for Anomaly detection
in the presence of Drift, which adjusts the normal patterns temporally, and
distinguish abnormal subsequences and new concepts. Moreover, it introduces a
new clustering method, Adjacent Hierarchical Clustering (AHC), which groups
similar subsequences while respecting their temporal locality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version (to be updated)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProvSQL: A General System for Keeping Track of the Provenance and
  Probability of Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryak Sen, Silviu Maniu, Pierre Senellart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the data model, design choices, and performance of ProvSQL, a
general and easy-to-deploy provenance tracking and probabilistic database
system implemented as a PostgreSQL extension. ProvSQL's data and query models
closely reflect that of a large core of SQL, including multiset semantics, the
full relational algebra, and aggregation. A key part of its implementation
relies on generic provenance circuits stored in memory-mapped files. We propose
benchmarks to measure the overhead of provenance and probabilistic evaluation
and demonstrate its scalability and competitiveness with respect to other
state-of-the-art systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the
  Filter-and-Verification Tree and MapReduce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Feng, Fangcao Jian, Yixuan Cao, Xiaobin Jian, Jia Wang, Haiyue Feng, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two different collections of sets, the exact set similarity R-S Join
finds all set pairs with similarity no less than a given threshold, which has
widespread applications. While existing algorithms accelerate large-scale R-S
Joins using a two-stage filter-and-verification framework along with the
parallel and distributed MapReduce framework, they suffer from excessive
candidate set pairs, leading to significant I/O, data transfer, and
verification overhead, and ultimately degrading the performance. This paper
proposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate
filtering and verification into a single stage through filter-and-verification
trees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT
(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that
compresses elements and associated sets in memory, enabling single-stage
processing that eliminates the candidate set generation, fast lookups, and
reduced database scans. Correctness proofs are provided. Second, CF-RS-Join
with LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,
which compresses non-branching paths into single nodes and stores them in
linear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and
MR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce
for parallel processing. Empirical studies on 7 real-world datasets have been
conducted to evaluate the performance of the proposed algorithms against
selected existing algorithms in terms of execution time, scalability, memory
usage, and disk usage. Experimental results demonstrate that our algorithm
using MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNN-based Anchor Embedding for Efficient Exact Subgraph Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00031v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00031v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yang, Zhaonian Zou, Jianxiong Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subgraph matching query is a fundamental problem in graph data management and
has a variety of real-world applications. Several recent works utilize deep
learning (DL) techniques to process subgraph matching queries. Most of them
find approximate subgraph matching results without accuracy guarantees. Unlike
these DL-based inexact subgraph matching methods, we propose a learning-based
exact subgraph matching framework, called \textit{graph neural network
(GNN)-based anchor embedding framework} (GNN-AE). In contrast to traditional
exact subgraph matching methods that rely on creating auxiliary summary
structures online for each specific query, our method indexes small feature
subgraphs in the data graph offline and uses GNNs to perform graph isomorphism
tests for these indexed feature subgraphs to efficiently obtain high-quality
candidates. To make a tradeoff between query efficiency and index storage cost,
we use two types of feature subgraphs, namely anchored subgraphs and anchored
paths. Based on the proposed techniques, we transform the exact subgraph
matching problem into a search problem in the embedding space. Furthermore, to
efficiently retrieve all matches, we develop a parallel matching growth
algorithm and design a cost-based DFS query planning method to further improve
the matching growth algorithm. Extensive experiments on 6 real-world and 3
synthetic datasets indicate that GNN-AE is more efficient than the baselines,
especially outperforming the exploration-based baseline methods by up to 1--2
orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-17T00:00:00Z">2025-06-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic Replicability and Comparative Study of BSARec and SASRec
  for Sequential <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiara D'Ercoli, Giulia Di Teodoro, Federico Siciliano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims at comparing two sequential recommender systems:
Self-Attention based Sequential Recommendation (SASRec), and Beyond
Self-Attention based Sequential Recommendation (BSARec) in order to check the
improvement frequency enhancement - the added element in BSARec - has on
recommendations. The models in the study, have been re-implemented with a
common base-structure from EasyRec, with the aim of obtaining a fair and
reproducible comparison. The results obtained displayed how BSARec, by
including bias terms for frequency enhancement, does indeed outperform SASRec,
although the increases in performance obtained, are not as high as those
presented by the authors. This work aims at offering an overview on existing
methods, and most importantly at underlying the importance of implementation
details for performance comparison.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Ran, Shuoqi Sun, Khoi Nguyen Dinh Anh, Damiano Spina, Oleg Zendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG
Challenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies
on generating a hypothetical answer that is used in the retrieval phase,
alongside the original question. GRAG also incorporates a pointwise large
language model (LLM)-based re-ranking step prior to final answer generation. We
describe the system architecture and the rationale behind our design choices.
In particular, a systematic evaluation using the Grid of Points (GoP) framework
and N-way ANOVA enabled comparison across multiple configurations, including
query variant generation, question decomposition, rank fusion strategies, and
prompting techniques for answer generation. Our system achieved a Relevance
score of 1.199 and a Faithfulness score of 0.477 on the private leaderboard,
placing among the top four finalists in the LiveRAG 2025 Challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at SIGIR 2025 LiveRAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vela: Scalable Embeddings with Voice <span class="highlight-title">Large Language Model</span>s for
  Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruofan Hu, Yan Xia, Minjie Hong, Jieming Zhu, Bo Chen, Xiaoda Yang, Minghui Fang, Tao Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have seen substantial progress in
recent years. However, their ability to represent multimodal information in the
acoustic domain remains underexplored. In this work, we introduce Vela, a novel
framework designed to adapt MLLMs for the generation of universal multimodal
embeddings. By leveraging MLLMs with specially crafted prompts and selected
in-context learning examples, Vela effectively bridges the modality gap across
various modalities. We then propose a single-modality training approach, where
the model is trained exclusively on text pairs. Our experiments show that Vela
outperforms traditional CLAP models in standard text-audio retrieval tasks.
Furthermore, we introduce new benchmarks that expose CLAP models' limitations
in handling long texts and complex retrieval tasks. In contrast, Vela, by
harnessing the capabilities of MLLMs, demonstrates robust performance in these
scenarios. Our code will soon be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity = Value? Consultation Value Assessment and Alignment for
  Personalized Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicong Qin, Yi Xu, Weijie Yu, Teng Shi, Chenglei Shen, Ming He, Jianping Fan, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized search systems in e-commerce platforms increasingly involve user
interactions with AI assistants, where users consult about products, usage
scenarios, and more. Leveraging consultation to personalize search services is
trending. Existing methods typically rely on semantic similarity to align
historical consultations with current queries due to the absence of 'value'
labels, but we observe that semantic similarity alone often fails to capture
the true value of consultation for personalization. To address this, we propose
a consultation value assessment framework that evaluates historical
consultations from three novel perspectives: (1) Scenario Scope Value, (2)
Posterior Action Value, and (3) Time Decay Value. Based on this, we introduce
VAPS, a value-aware personalized search model that selectively incorporates
high-value consultations through a consultation-user action interaction module
and an explicit objective that aligns consultations with user actions.
Experiments on both public and commercial datasets show that VAPS consistently
outperforms baselines in both retrieval and ranking tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG
  Systems for the SIGIR LiveRAG Competition <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Cofala, Oleh Astappiev, William Xion, Hailay Teklehaymanot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by
combining their internal, parametric knowledge with external, non-parametric
sources, with the goal of improving factual correctness and minimizing
hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize
accuracy on DataMorgana's QA pairs, which are composed of single-hop and
multi-hop questions. The challenge provides access to sparse OpenSearch and
dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to
LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A
judge-LLM assesses the submitted answers along with human evaluators. By
exploring distinct retriever combinations and RAG solutions under the challenge
conditions, our final solution emerged using InstructRAG in combination with a
Pinecone retriever and a BGE reranker. Our solution achieved a correctness
score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR
2025 LiveRAG Challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures. Report for SIGIR 2025 LiveRAG Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ hyperFA*IR: A hypergeometric approach to fair rankings with finite
  candidate pool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauritz N. Cartier van Dissel, Samuel Martin-Gutierrez, Lisette Espín-Noboa, Ana María Jaramillo, Fariba Karimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking algorithms play a pivotal role in decision-making processes across
diverse domains, from search engines to job applications. When rankings
directly impact individuals, ensuring fairness becomes essential, particularly
for groups that are marginalised or misrepresented in the data. Most of the
existing group fairness frameworks often rely on ensuring proportional
representation of protected groups. However, these approaches face limitations
in accounting for the stochastic nature of ranking processes or the finite size
of candidate pools. To this end, we present hyperFA*IR, a framework for
assessing and enforcing fairness in rankings drawn from a finite set of
candidates. It relies on a generative process based on the hypergeometric
distribution, which models real-world scenarios by sampling without replacement
from fixed group sizes. This approach improves fairness assessment when top-$k$
selections are large relative to the pool or when protected groups are small.
We compare our approach to the widely used binomial model, which treats each
draw as independent with fixed probability, and demonstrate$-$both analytically
and empirically$-$that our method more accurately reproduces the statistical
properties of sampling from a finite population. To operationalise this
framework, we propose a Monte Carlo-based algorithm that efficiently detects
unfair rankings by avoiding computationally expensive parameter tuning.
Finally, we adapt our generative approach to define affirmative action policies
by introducing weights into the sampling process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 2025 ACM Conference on Fairness,
  Accountability, and Transparency (FAccT'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,
  Transparent, and Reproducible Geo-Temporal Information Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Martins, Piotr Szymański, Piotr Gramacki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has transformed information
access, with current LLMs also powering deep research systems that can generate
comprehensive report-style answers, through planned iterative search,
retrieval, and reasoning. Still, current deep research systems lack the
geo-temporal capabilities that are essential for answering context-rich
questions involving geographic and/or temporal constraints, frequently
occurring in domains like public health, environmental science, or
socio-economic analysis. This paper reports our vision towards next generation
systems, identifying important technical, infrastructural, and evaluative
challenges in integrating geo-temporal reasoning into deep research pipelines.
We argue for augmenting retrieval and synthesis processes with the ability to
handle geo-temporal constraints, supported by open and reproducible
infrastructures and rigorous evaluation protocols. Our vision outlines a path
towards more advanced and geo-temporally aware deep research systems, of
potential impact to the future of AI-driven information access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImpReSS: Implicit Recommender System for Support Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omri Haller, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following recent advancements in large language models (LLMs), LLM-based
chatbots have transformed customer support by automating interactions and
providing consistent, scalable service. While LLM-based conversational
recommender systems (CRSs) have attracted attention for their ability to
enhance the quality of recommendations, limited research has addressed the
implicit integration of recommendations within customer support interactions.
In this work, we introduce ImpReSS, an implicit recommender system designed for
customer support conversations. ImpReSS operates alongside existing support
chatbots, where users report issues and chatbots provide solutions. Based on a
customer support conversation, ImpReSS identifies opportunities to recommend
relevant solution product categories (SPCs) that help resolve the issue or
prevent its recurrence -- thereby also supporting business growth. Unlike
traditional CRSs, ImpReSS functions entirely implicitly and does not rely on
any assumption of a user's purchasing intent. Our empirical evaluation of
ImpReSS's ability to recommend relevant SPCs that can help address issues
raised in support conversations shows promising results, including an MRR@1
(and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for
information security support, and 0.85 (0.67) for cybersecurity
troubleshooting. To support future research, our data and code will be shared
upon request.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InsertRank: <span class="highlight-title">LLM</span>s can reason over BM25 scores to Improve Listwise
  Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Seetharaman, Kaustubh D. Dhole, Aman Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant strides across
various information retrieval tasks, particularly as rerankers, owing to their
strong generalization and knowledge-transfer capabilities acquired from
extensive pretraining. In parallel, the rise of LLM-based chat interfaces has
raised user expectations, encouraging users to pose more complex queries that
necessitate retrieval by ``reasoning'' over documents rather than through
simple keyword matching or semantic similarity. While some recent efforts have
exploited reasoning abilities of LLMs for reranking such queries, considerable
potential for improvement remains. In that regards, we introduce InsertRank, an
LLM-based reranker that leverages lexical signals like BM25 scores during
reranking to further improve retrieval performance. InsertRank demonstrates
improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning
12 diverse domains, and R2MED, a specialized medical reasoning retrieval
benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and
several ablation studies and demonstrate that InsertRank consistently improves
retrieval effectiveness across multiple families of LLMs, including GPT,
Gemini, and Deepseek models. %In addition, we also conduct ablation studies on
normalization by varying the scale of the BM25 scores, and positional bias by
shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a
score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark,
surpassing previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Model Acceleration and Optimization Strategies for
  Real-Time <span class="highlight-title">Recommendation</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junli Shao, Jing Dong, Dingzhou Wang, Kowei Shih, Dannier Li, Chengrui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of Internet services, recommendation systems play a
central role in delivering personalized content. Faced with massive user
requests and complex model architectures, the key challenge for real-time
recommendation systems is how to reduce inference latency and increase system
throughput without sacrificing recommendation quality. This paper addresses the
high computational cost and resource bottlenecks of deep learning models in
real-time settings by proposing a combined set of modeling- and system-level
acceleration and optimization strategies. At the model level, we dramatically
reduce parameter counts and compute requirements through lightweight network
design, structured pruning, and weight quantization. At the system level, we
integrate multiple heterogeneous compute platforms and high-performance
inference libraries, and we design elastic inference scheduling and
load-balancing mechanisms based on real-time load characteristics. Experiments
show that, while maintaining the original recommendation accuracy, our methods
cut latency to less than 30% of the baseline and more than double system
throughput, offering a practical solution for deploying large-scale online
recommendation services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hudson de Martim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes an adaptation of Graph Retrieval-Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms. Legal texts are characterized by a predefined hierarchical structure, an
extensive network of references and a continuous evolution through multiple
temporal versions. This temporal dynamism poses a significant challenge for
standard AI systems, demanding a deterministic representation of the law at any
given point in time. To address this, our approach grounds the knowledge graph
construction in a formal, FRBRoo-inspired model that distinguishes abstract
legal works from their concrete textual expressions. We introduce a
multi-layered representation of Temporal Versions (capturing date-specific
changes) and Language Versions (capturing linguistic variations). By modeling
normative evolution as a precise sequence of these versioned entities, we
enable the construction of a knowledge graph that serves as a verifiable
"ground truth". This allows Large Language Models to generate responses based
on accurate, context-aware, and point-in-time correct legal information,
overcoming the risk of temporal inaccuracies. Through a detailed analysis of
this formal Graph RAG approach and its application to legal norm datasets, this
article aims to advance the field of Artificial Intelligence applied to Law,
creating opportunities for more effective and reliable systems in legal
research, legislative analysis, and decision support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version enhances the theoretical underpinnings of the proposed
  Graph RAG methodology, including the introduction of a formal, FRBRoo-based
  model for versioning, and enabling multi-language support for both content
  and metadata</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12420v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12420v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Hernandez Cruz, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, Jiahua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,
particularly concerning the energy consumption of the Proof of Work (PoW)
consensus mechanism and broader Environmental, Social, and Governance (ESG)
issues. However, existing systematic literature reviews of DLT rely on limited
analyses of citations, abstracts, and keywords, failing to fully capture the
field's complexity and ESG concerns. We address these challenges by analyzing
the full text of 24,539 publications using Natural Language Processing (NLP)
with our manually labeled Named Entity Recognition (NER) dataset of 39,427
entities for DLT. This methodology identified 505 key publications at the
DLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP
and temporal graph analysis reveals critical trends in DLT evolution and ESG
impacts, including cryptography and peer-to-peer networks research's
foundational influence, Bitcoin's persistent impact on research and
environmental concerns (a "Lindy effect"), Ethereum's catalytic role on Proof
of Stake (PoS) and smart contract adoption, and the industry's progressive
shift toward energy-efficient consensus mechanisms. Our contributions include
the first DLT-specific NER dataset addressing the scarcity of high-quality
labeled NLP data in blockchain research, a methodology integrating NLP and
temporal graph analysis for large-scale interdisciplinary literature reviews,
and the first NLP-driven literature review focusing on DLT's ESG aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Cell Ontology in the age of single-cell omics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Zheng Kai Tan, Aleix Puig-Barbe, Damien Goutte-Gattat, Caroline Eastwood, Brian Aevermann, Alida Avola, James P Balhoff, Ismail Ugur Bayindir, Jasmine Belfiore, Anita Reane Caron, David S Fischer, Nancy George, Benjamin M Gyori, Melissa A Haendel, Charles Tapley Hoyt, Huseyin Kir, Tiago Lubiana, Nicolas Matentzoglu, James A Overton, Beverly Peng, Bjoern Peters, Ellen M Quardokus, Patrick L Ray, Paola Roncaglia, Andrea D Rivera, Ray Stefancsik, Wei Kheng Teh, Sabrina Toro, Nicole Vasilevsky, Chuan Xu, Yun Zhang, Richard H Scheuermann, Chirstopher J Mungall, Alexander D Diehl, David Osumi-Sutherland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-cell omics technologies have transformed our understanding of cellular
diversity by enabling high-resolution profiling of individual cells. However,
the unprecedented scale and heterogeneity of these datasets demand robust
frameworks for data integration and annotation. The Cell Ontology (CL) has
emerged as a pivotal resource for achieving FAIR (Findable, Accessible,
Interoperable, and Reusable) data principles by providing standardized,
species-agnostic terms for canonical cell types - forming a core component of a
wide range of platforms and tools. In this paper, we describe the wide variety
of uses of CL in these platforms and tools and detail ongoing work to improve
and extend CL content including the addition of transcriptomically defined
types, working closely with major atlasing efforts including the Human Cell
Atlas and the Brain Initiative Cell Atlas Network to support their needs. We
cover the challenges and future plans for harmonising classical and
transcriptomic cell type definitions, integrating markers and using Large
Language Models (LLMs) to improve content and efficiency of CL workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HARMONY: A Scalable Distributed Vector Database for High-Throughput
  Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Xu, Feng Zhang, Chengxi Li, Lei Cao, Zheng Chen, Jidong Zhai, Xiaoyong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) is essential for various
data-intensive applications, including recommendation systems, image retrieval,
and machine learning. Scaling ANNS to handle billions of high-dimensional
vectors on a single machine presents significant challenges in memory capacity
and processing efficiency. To address these challenges, distributed vector
databases leverage multiple nodes for the parallel storage and processing of
vectors. However, existing solutions often suffer from load imbalance and high
communication overhead, primarily due to traditional partition strategies that
fail to effectively distribute the workload. In this paper, we introduce
Harmony, a distributed ANNS system that employs a novel multi-granularity
partition strategy, combining dimension-based and vector-based partition. This
strategy ensures a balanced distribution of computational load across all nodes
while effectively minimizing communication costs. Furthermore, Harmony
incorporates an early-stop pruning mechanism that leverages the monotonicity of
distance computations in dimension-based partition, resulting in significant
reductions in both computational and communication overhead. We conducted
extensive experiments on diverse real-world datasets, demonstrating that
Harmony outperforms leading distributed vector databases, achieving 4.63 times
throughput on average in four nodes and 58% performance improvement over
traditional distribution for skewed workloads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keigo: Co-designing Log-Structured Merge Key-Value Stores with a
  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rúben Adão, Zhongjie Wu, Changjun Zhou, Oana Balmau, João Paulo, Ricardo Macedo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Keigo, a concurrency- and workload-aware storage middleware that
enhances the performance of log-structured merge key-value stores (LSM KVS)
when they are deployed on a hierarchy of storage devices. The key observation
behind Keigo is that there is no one-size-fits-all placement of data across the
storage hierarchy that optimizes for all workloads. Hence, to leverage the
benefits of combining different storage devices, Keigo places files across
different devices based on their parallelism, I/O bandwidth, and capacity. We
introduce three techniques - concurrency-aware data placement, persistent
read-only caching, and context-based I/O differentiation. Keigo is portable
across different LSMs, is adaptable to dynamic workloads, and does not require
extensive profiling. Our system enables established production KVS such as
RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We
evaluate Keigo using synthetic and realistic workloads, showing that it
improves the throughput of production-grade LSMs up to 4x for write- and 18x
for read-heavy workloads when compared to general-purpose storage systems and
specialized LSM KVS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of the full paper to appear in VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PHast -- Perfect Hashing with fast evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Beling, Peter Sanders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perfect hash functions give unique "names" to arbitrary keys requiring only a
few bits per key. This is an essential building block in applications like
static hash tables, databases, or bioinformatics. This paper introduces the
PHast approach that has the currently fastest query time with competitive
construction time and space consumption. PHast improves bucket-placement which
first hashes each key k to a bucket, and then looks for the bucket seed s such
that a secondary hash function maps pairs (s,k) in a collision-free way. PHast
can use small-range primary hash functions with linear mapping, fixed-width
encoding of seeds, and parallel construction. This is achieved using small
overlapping slices of allowed values and bumping to handle unsuccessful seed
assignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abacus: A Cost-Based Optimizer for Semantic Operator Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Russo, Sivaprasad Sudhir, Gerardo Vitagliano, Chunwei Liu, Tim Kraska, Samuel Madden, Michael Cafarella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anqi Liang, Pengcheng Zhang, Bin Yao, Zhongpu Chen, Yitong Song, Guangxu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an efficient and scalable framework for Range Filtered
Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors
associated with attribute values. Given a query vector $q$ and a range $[l,
h]$, RF-ANNS aims to find the approximate $k$ nearest neighbors of $q$ among
data whose attribute values fall within $[l, h]$. Existing methods including
pre-, post-, and hybrid filtering strategies that perform attribute range
filtering before, after, or during the ANNS process, all suffer from
significant performance degradation when query ranges shift. Though building
dedicated indexes for each strategy and selecting the best one based on the
query range can address this problem, it leads to index consistency and
maintenance issues.
  Our framework, called UNIFY, constructs a unified Proximity Graph-based
(PG-based) index that seamlessly supports all three strategies. In UNIFY, we
introduce SIG, a novel Segmented Inclusive Graph, which segments the dataset by
attribute values. It ensures the PG of objects from any segment combinations is
a sub-graph of SIG, thereby enabling efficient hybrid filtering by
reconstructing and searching a PG from relevant segments. Moreover, we present
Hierarchical Segmented Inclusive Graph (HSIG), a variant of SIG which
incorporates a hierarchical structure inspired by HNSW to achieve logarithmic
hybrid filtering complexity. We also implement pre- and post-filtering for HSIG
by fusing skip list connections and compressed HNSW edges into the hierarchical
graph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS
performance across small, mid, and large query ranges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magneto: Combining Small and <span class="highlight-title">Large Language Model</span>s for Schema Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurong Liu, Eduardo Pena, Aecio Santos, Eden Wu, Juliana Freire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in language models opened new opportunities to address
complex schema matching tasks. Schema matching approaches have been proposed
that demonstrate the usefulness of language models, but they have also
uncovered important limitations: Small language models (SLMs) require training
data (which can be both expensive and challenging to obtain), and large
language models (LLMs) often incur high computational costs and must deal with
constraints imposed by context windows. We present Magneto, a cost-effective
and accurate solution for schema matching that combines the advantages of SLMs
and LLMs to address their limitations. By structuring the schema matching
pipeline in two phases, retrieval and reranking, Magneto can use
computationally efficient SLM-based strategies to derive candidate matches
which can then be reranked by LLMs, thus making it possible to reduce runtime
without compromising matching accuracy. We propose a self-supervised approach
to fine-tune SLMs which uses LLMs to generate syntactically diverse training
data, and prompting strategies that are effective for reranking. We also
introduce a new benchmark, developed in collaboration with domain experts,
which includes real biomedical datasets and presents new challenges to schema
matching methods. Through a detailed experimental evaluation, using both our
new and existing benchmarks, we show that Magneto is scalable and attains high
accuracy for datasets from different domains.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-16T00:00:00Z">2025-06-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LTRR: Learning To Rank Retrievers for <span class="highlight-title">LLM</span>s <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        To Eun Kim, Fernando Diaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed
retriever, despite growing evidence that no single retriever performs optimally
across all query types. In this paper, we explore a query routing approach that
dynamically selects from a pool of retrievers based on the query, using both
train-free heuristics and learned routing models. We frame routing as a
learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to
rank retrievers by their expected utility gain to downstream LLM performance.
Our experiments, conducted on synthetic QA data with controlled query type
variations, show that routing-based RAG systems can outperform the best
single-retriever-based systems. Performance gains are especially pronounced in
models trained with the Answer Correctness (AC) metric and with pairwise
learning approaches, especially with XGBoost. We also observe improvements in
generalization to out-of-distribution queries. As part of the SIGIR 2025
LiveRAG challenge, our submitted system demonstrated the practical viability of
our approach, achieving competitive performance in both answer correctness and
faithfulness. These findings highlight the importance of both training
methodology and metric selection in query routing for RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2025 LiveRAG Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneRec Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guorui Zhou, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Qiang Luo, Qianqian Wang, Qigen Hu, Rui Huang, Shiyao Wang, Weifeng Ding, Wuchao Li, Xinchen Luo, Xingmei Wang, Zexuan Cheng, Zixing Zhang, Bin Zhang, Boxuan Wang, Chaoyi Ma, Chengru Song, Chenhui Wang, Di Wang, Dongxue Meng, Fan Yang, Fangyu Zhang, Feng Jiang, Fuxing Zhang, Gang Wang, Guowang Zhang, Han Li, Hengrui Hu, Hezheng Lin, Hongtao Cheng, Hongyang Cao, Huanjie Wang, Jiaming Huang, Jiapeng Chen, Jiaqiang Liu, Jinghui Jia, Kun Gai, Lantao Hu, Liang Zeng, Liao Yu, Qiang Wang, Qidong Zhou, Shengzhe Wang, Shihui He, Shuang Yang, Shujie Yang, Sui Huang, Tao Wu, Tiantian He, Tingting Gao, Wei Yuan, Xiao Liang, Xiaoxiao Xu, Xugang Liu, Yan Wang, Yi Wang, Yiwu Liu, Yue Song, Yufei Zhang, Yunfan Wu, Yunfeng Zhao, Zhanyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been widely used in various large-scale
user-oriented platforms for many years. However, compared to the rapid
developments in the AI community, recommendation systems have not achieved a
breakthrough in recent years. For instance, they still rely on a multi-stage
cascaded architecture rather than an end-to-end approach, leading to
computational fragmentation and optimization inconsistencies, and hindering the
effective application of key breakthrough technologies from the AI community in
recommendation scenarios.
  To address these issues, we propose OneRec, which reshapes the recommendation
system through an end-to-end generative approach and achieves promising
results. Firstly, we have enhanced the computational FLOPs of the current
recommendation model by 10 $\times$ and have identified the scaling laws for
recommendations within certain boundaries. Secondly, reinforcement learning
techniques, previously difficult to apply for optimizing recommendations, show
significant potential in this framework. Lastly, through infrastructure
optimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)
on flagship GPUs during training and inference, respectively, aligning closely
with the LLM community. This architecture significantly reduces communication
and storage overhead, resulting in operating expense that is only 10.6% of
traditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,
it handles 25% of total queries per second, enhancing overall App Stay Time by
0.54% and 1.24%, respectively. Additionally, we have observed significant
increases in metrics such as 7-day Lifetime, which is a crucial indicator of
recommendation experience. We also provide practical lessons and insights
derived from developing, optimizing, and maintaining a production-scale
recommendation system with significant real-world impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors are listed alphabetically by their first name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks:
  Application on Taiwanese Regulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Heng Yu, Yen-Lung Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Retrieval-Augmented Generation (RAG) systems employ brute-force
inner product search to retrieve the top-k most similar documents, then
combined with the user query and passed to a language model. This allows the
model to access external knowledge and reduce hallucinations. However,
selecting an appropriate k value remains a significant challenge in practical
applications: a small k may fail to retrieve sufficient information, while a
large k can introduce excessive and irrelevant content. To address this, we
propose a hierarchical clustering-based retrieval method that eliminates the
need to predefine k. Our approach maintains the accuracy and relevance of
system responses while adaptively selecting semantically relevant content. In
the experiment stage, we applied our method to a Taiwanese legal dataset with
expert-graded queries. The results show that our approach achieves superior
performance in expert evaluations and maintains high precision while
eliminating the need to predefine k, demonstrating improved accuracy and
interpretability in legal text retrieval tasks. Our framework is simple to
implement and easily integrates with existing RAG pipelines, making it a
practical solution for real-world applications under limited resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, Code available at
  https://github.com/arthur422tp/hierachical</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond One-Size-Fits-All: A Study of Neural and Behavioural Variability
  A<span class="highlight-title">cross</span> Different <span class="highlight-title">Recommendation</span> Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Koutroumpas, Sebastian Idesis, Mireia Masias Bruns, Carlos Segura, Joemon M. Jose, Sergi Abadal, Ioannis Arapakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, Recommender Systems (RS) have primarily measured performance
based on the accuracy and relevance of their recommendations. However, this
algorithmic-centric approach overlooks how different types of recommendations
impact user engagement and shape the overall quality of experience. In this
paper, we shift the focus to the user and address for the first time the
challenge of decoding the neural and behavioural variability across distinct
recommendation categories, considering more than just relevance. Specifically,
we conducted a controlled study using a comprehensive e-commerce dataset
containing various recommendation types, and collected Electroencephalography
and behavioural data. We analysed both neural and behavioural responses to
recommendations that were categorised as Exact, Substitute, Complement, or
Irrelevant products within search query results. Our findings offer novel
insights into user preferences and decision-making processes, revealing
meaningful relationships between behavioural and neural patterns for each
category, but also indicate inter-subject variability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decompositional Reasoning for Graph Retrieval with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Six, Evan Dufraisse, Gaël de Chalendar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at many NLP tasks, but struggle with
multi-hop reasoning and factual consistency, limiting their effectiveness on
knowledge-intensive tasks like complex question answering (QA). Linking
Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally
lack the ability to reason efficiently over graph-structured information. To
tackle this problem, we propose a novel retrieval approach that integrates
textual knowledge graphs into the LLM reasoning process via query
decomposition. Our method decomposes complex questions into sub-questions,
retrieves relevant textual subgraphs, and composes a question-specific
knowledge graph to guide answer generation. For that, we use a weighted
similarity function that focuses on both the complex question and the generated
subquestions to extract a relevant subgraph, which allows efficient and precise
retrieval for complex questions and improves the performance of LLMs on
multi-hop QA tasks. This structured reasoning pipeline enhances factual
grounding and interpretability while leveraging the generative strengths of
LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that
it achieves comparable or superior performance to competitive existing methods,
using smaller models and fewer LLM calls.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Transformation of Urban Planning in Australia: Influencing
  Factors and Key Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soheil Sabri, Sherah Kurnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past two decades, several governments in developing and developed
countries have started their journey toward digital transformation. However,
the pace and maturity of digital technologies and strategies are different
between public services. Current literature indicates that research on the
digital transformation of urban planning is still developing. Therefore, the
aim of this study is to understand the influencing factors and key challenges
for the digital transformation of urban planning in Australia. The study adopts
the inter-organisational theory and Planning Support Science (PSScience) under
the Technological, Organisational, and External Environmental (TOE) framework.
It involves a multiple case study, administered semi-structured interviews with
thirteen IT and urban planning experts across Victoria and New South Wales
governments and private industries. The study findings indicate that the main
challenges for digital transformation of the Australian urban planning system
are related to organisational and external environmental factors. Furthermore,
a digital maturity model is absent in the Australian urban planning industry.
This study offers important implications to research and practice related to
digital transformation in urban planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 2 figures, Master's Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gated Rotary-Enhanced Linear Attention for Long-term Sequential
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntao Hu, Wei Zhou, Huayi Shen, Xiao Du, Jie Liao, Junhao Wen, Min Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Sequential Recommendation Systems (SRSs), Transformer models show
remarkable performance but face computation cost challenges when modeling
long-term user behavior sequences due to the quadratic complexity of the
dot-product attention mechanism. By approximating the dot-product attention,
linear attention provides an efficient option with linear complexity. However,
existing linear attention methods face two limitations: 1) they often use
learnable position encodings, which incur extra computational costs in
long-term sequence scenarios, and 2) they may not consider the user's
fine-grained local preferences and confuse these with the actual change of
long-term interests. To remedy these drawbacks, we propose a long-term
sequential Recommendation model with Gated Rotary Enhanced Linear Attention
(RecGRELA). Specifically, we first propose a Rotary-Enhanced Linear Attention
(RELA) module to model long-range dependency within the user's historical
information using rotary position encodings. We then introduce a local short
operation to incorporate local preferences and demonstrate the theoretical
insight. We further introduce a SiLU-based Gated mechanism for RELA (GRELA) to
help the model determine whether a user's behavior indicates local interest or
a genuine shift in long-term preferences. Experimental results on four public
datasets demonstrate that our RecGRELA achieves state-of-the-art performance
compared to existing SRSs while maintaining low memory overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accessibility Barriers in Multi-Terabyte Public Datasets: The Gap
  Between Promise and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Bara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The promise of "free and open" multi-terabyte datasets often collides with
harsh realities. While these datasets may be technically accessible, practical
barriers -- from processing complexity to hidden costs -- create a system that
primarily serves well-funded institutions. This study examines accessibility
challenges across web crawls, satellite imagery, scientific data, and
collaborative projects, revealing a consistent two-tier system where
theoretical openness masks practical exclusivity. Our analysis demonstrates
that datasets marketed as "publicly accessible" typically require minimum
investments of \$1,000+ for meaningful analysis, with complex processing
pipelines demanding \$10,000-100,000+ in infrastructure costs. The
infrastructure requirements -- distributed computing knowledge, domain
expertise, and substantial budgets -- effectively gatekeep these datasets
despite their "open" status, limiting practical accessibility to those with
institutional support or substantial resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 28 references. Analysis of practical barriers to accessing
  multi-terabyte public datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vector Ontologies as an <span class="highlight-title">LLM</span> world view extraction method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaspar Rothenfusser, Bekk Blando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess intricate internal representations of
the world, yet these latent structures are notoriously difficult to interpret
or repurpose beyond the original prediction task. Building on our earlier work
(Rothenfusser, 2025), which introduced the concept of vector ontologies as a
framework for translating high-dimensional neural representations into
interpretable geometric structures, this paper provides the first empirical
validation of that approach. A vector ontology defines a domain-specific vector
space spanned by ontologically meaningful dimensions, allowing geometric
analysis of concepts and relationships within a domain. We construct an
8-dimensional vector ontology of musical genres based on Spotify audio features
and test whether an LLM's internal world model of music can be consistently and
accurately projected into this space. Using GPT-4o-mini, we extract genre
representations through multiple natural language prompts and analyze the
consistency of these projections across linguistic variations and their
alignment with ground-truth data. Our results show (1) high spatial consistency
of genre projections across 47 query formulations, (2) strong alignment between
LLM-inferred genre locations and real-world audio feature distributions, and
(3) evidence of a direct relationship between prompt phrasing and spatial
shifts in the LLM's inferred vector ontology. These findings demonstrate that
LLMs internalize structured, repurposable knowledge and that vector ontologies
offer a promising method for extracting and analyzing this knowledge in a
transparent and verifiable way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPOT: Bridging Natural Language and Geospatial Search for Investigative
  Journalists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Khellaf, Ipek Baris Schlicht, Tilman Mirass, Julia Bayer, Tilman Wagner, Ruben Bouwmeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenStreetMap (OSM) is a vital resource for investigative journalists doing
geolocation verification. However, existing tools to query OSM data such as
Overpass Turbo require familiarity with complex query languages, creating
barriers for non-technical users. We present SPOT, an open source natural
language interface that makes OSM's rich, tag-based geographic data more
accessible through intuitive scene descriptions. SPOT interprets user inputs as
structured representations of geospatial object configurations using fine-tuned
Large Language Models (LLMs), with results being displayed in an interactive
map interface. While more general geospatial search tasks are conceivable, SPOT
is specifically designed for use in investigative journalism, addressing
real-world challenges such as hallucinations in model output, inconsistencies
in OSM tagging, and the noisy nature of user input. It combines a novel
synthetic data pipeline with a semantic bundling system to enable robust,
accurate query generation. To our knowledge, SPOT is the first system to
achieve reliable natural language access to OSM data at this level of accuracy.
By lowering the technical barrier to geolocation verification, SPOT contributes
a practical tool to the broader efforts to support fact-checking and combat
disinformation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Synthesizing Data for Context Attribution in Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gorjan Radevski, Kiril Gashteovski, Shahbaz Syed, Christopher Malon, Sebastien Nicolas, Chia-Chien Hung, Timo Sztyler, Verena Heußer, Wiem Ben Rim, Masafumi Enomoto, Kunihiro Takeoka, Masafumi Oyamada, Goran Glavaš, Carolin Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) accounts for a significant portion of LLM usage "in
the wild". However, LLMs sometimes produce false or misleading responses, also
known as "hallucinations". Therefore, grounding the generated answers in
contextually provided information -- i.e., providing evidence for the generated
text -- is paramount for LLMs' trustworthiness. Providing this information is
the task of context attribution. In this paper, we systematically study
LLM-based approaches for this task, namely we investigate (i) zero-shot
inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic
data generated by larger LLMs. Our key contribution is SynQA: a novel
generative strategy for synthesizing context attribution data. Given selected
context sentences, an LLM generates QA pairs that are supported by these
sentences. This leverages LLMs' natural strengths in text generation while
ensuring clear attribution paths in the synthetic training data. We show that
the attribution data synthesized via SynQA is highly effective for fine-tuning
small LMs for context attribution in different QA tasks and domains. Finally,
with a user study, we validate the usefulness of small LMs (fine-tuned on
synthetic data from SynQA) in context attribution for QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Reference Model and Patterns for Production Event Data Enrichment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark van der Pas, Remco Dijkman, Alp Akçay, Ivo Adan, John Walker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of digital transformation, organisations are increasingly
generating large volumes of data through the execution of various processes
across disparate systems. By integrating data from these heterogeneous sources,
it becomes possible to derive new insights essential for tasks such as
monitoring and analysing process performance. Typically, this information is
extracted during a data pre-processing or engineering phase. However, this step
is often performed in an ad-hoc manner and is time-consuming and
labour-intensive. To streamline this process, we introduce a reference model
and a collection of patterns designed to enrich production event data. The
reference model provides a standard way for storing and extracting production
event data. The patterns describe common information extraction tasks and how
such tasks can be automated effectively. The reference model is developed by
combining the ISA-95 industry standard with the Event Knowledge Graph
formalism. The patterns are developed based on empirical observations from
event data sets originating in manufacturing processes and are formalised using
the reference model. We evaluate the relevance and applicability of these
patterns by demonstrating their application to use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper submitted to EDOC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reference-Aligned Retrieval-Augmented Question Answering over
  Heterogeneous Proprietary Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nayoung Choi, Grace Byun, Andrew Chung, Ellie S. Paek, Shinsun Lee, Jinho D. Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proprietary corporate documents contain rich domain-specific knowledge, but
their overwhelming volume and disorganized structure make it difficult even for
employees to access the right information when needed. For example, in the
automotive industry, vehicle crash-collision tests, each costing hundreds of
thousands of dollars, produce highly detailed documentation. However,
retrieving relevant content during decision-making remains time-consuming due
to the scale and complexity of the material. While Retrieval-Augmented
Generation (RAG)-based Question Answering (QA) systems offer a promising
solution, building an internal RAG-QA system poses several challenges: (1)
handling heterogeneous multi-modal data sources, (2) preserving data
confidentiality, and (3) enabling traceability between each piece of
information in the generated answer and its original source document. To
address these, we propose a RAG-QA framework for internal enterprise use,
consisting of: (1) a data pipeline that converts raw multi-modal documents into
a structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving
architecture, and (3) a lightweight reference matcher that links answer
segments to supporting content. Applied to the automotive domain, our system
improves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),
and helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale
ratings from both human and LLM judge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEPA4Rec: Learning Effective Language Representations for Sequential
  <span class="highlight-title">Recommendation</span> via Joint Embedding Predictive Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Anh Nguyen, Dung D. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language representation learning has emerged as a promising approach for
sequential recommendation, thanks to its ability to learn generalizable
representations. However, despite its advantages, this approach still struggles
with data sparsity and a limited understanding of common-sense user
preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a
framework that combines $\textbf{J}$oint $\textbf{E}$mbedding
$\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item
textual descriptions. JEPA4Rec captures semantically rich and transferable
representations, improving recommendation performance and reducing reliance on
large-scale pre-training data. Specifically, JEPA4Rec represents items as text
sentences by flattening descriptive information such as $\textit{title,
category}$, and other attributes. To encode these sentences, we employ a
bidirectional Transformer encoder with modified embedding layers tailored for
capturing item information in recommendation datasets. We apply masking to text
sentences and use them to predict the representations of the unmasked
sentences, helping the model learn generalizable item embeddings. To further
improve recommendation performance and language understanding, we employ a
two-stage training strategy incorporating self-supervised learning losses.
Experiments on six real-world datasets demonstrate that JEPA4Rec consistently
outperforms state-of-the-art methods, particularly in cross-domain,
cross-platform, and low-resource scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask Optimal Questions: Aligning <span class="highlight-title">Large Language Model</span>s with Retriever's
  Preference in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational search, unlike single-turn retrieval tasks, requires
understanding the current question within a dialogue context. The common
approach of rewrite-then-retrieve aims to decontextualize questions to be
self-sufficient for off-the-shelf retrievers, but most existing methods produce
sub-optimal query rewrites due to the limited ability to incorporate signals
from the retrieval results. To overcome this limitation, we present a novel
framework RetPO (Retriever's Preference Optimization), which is designed to
optimize a language model (LM) for reformulating search queries in line with
the preferences of the target retrieval systems. The process begins by
prompting a large LM to produce various potential rewrites and then collects
retrieval performance for these rewrites as the retrievers' preferences.
Through the process, we construct a large-scale dataset called RF collection,
containing Retrievers' Feedback on over 410K query rewrites across 12K
conversations. Furthermore, we fine-tune a smaller LM on this dataset to align
it with the retrievers' feedback. Our resulting model demonstrates superiority
on two benchmarks, surpassing the previous state-of-the-art performance of
rewrite-then-retrieve approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 (findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TongSearch-QR: Reinforced Query Reasoning for Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubo Qin, Jun Bai, Jiaqi Li, Zixia Jia, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional information retrieval (IR) methods excel at textual and semantic
matching but struggle in reasoning-intensive retrieval tasks that require
multi-hop inference or complex semantic understanding between queries and
documents. One promising solution is to explicitly rewrite or augment queries
using large language models (LLMs) to elicit reasoning-relevant content prior
to retrieval. However, the widespread use of large-scale language models like
GPT-4 or LLaMA3-70B remains impractical due to their high inference cost and
limited deployability in real-world systems. In this work, we introduce
TongSearch QR (Previously Known as "TongSearch Reasoner"), a family of
small-scale language models for query reasoning and rewriting in
reasoning-intensive retrieval. With a novel semi-rule-based reward function, we
employ reinforcement learning approaches enabling smaller language models, e,g,
Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning
performance rivaling large-scale language models without their prohibitive
inference costs. Experiment results on BRIGHT benchmark show that with BM25 as
retrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly
outperform existing baselines, including prompt-based query reasoners and some
latest dense retrievers trained for reasoning-intensive retrieval tasks,
offering superior adaptability for real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Representational Learning of Foundation Models for
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheli Zhou, Chenxu Zhu, Jianghao Lin, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing a single foundation model with the capability to excel across
diverse tasks has been a long-standing objective in the field of artificial
intelligence. As the wave of general-purpose foundation models sweeps across
various domains, their influence has significantly extended to the field of
recommendation systems. While recent efforts have explored recommendation
foundation models for various generative tasks, they often overlook crucial
embedding tasks and struggle with the complexities of multi-task learning,
including knowledge sharing & conflict resolution, and convergence speed
inconsistencies. To address these limitations, we introduce RecFound, a
generative representational learning framework for recommendation foundation
models. We construct the first comprehensive dataset for recommendation
foundation models covering both generative and embedding tasks across diverse
scenarios. Based on this dataset, we propose a novel multi-task training scheme
featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge
sharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)
to address inconsistent convergence, and a Model Merge module to balance the
performance across tasks. Experiments demonstrate that RecFound achieves
state-of-the-art performance across various recommendation tasks, outperforming
existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at https://junkfood436.github.io/RecFound/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketched Sum-Product Networks for Joins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Tsan, Abylay Amanbayev, Asoke Datta, Florin Rusu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sketches have shown high accuracy in multi-way join cardinality estimation, a
critical problem in cost-based query optimization. Accurately estimating the
cardinality of a join operation -- analogous to its computational cost --
allows the optimization of query execution costs in relational database
systems. However, although sketches have shown high efficacy in query
optimization, they are typically constructed specifically for predefined
selections in queries that are assumed to be given a priori, hindering their
applicability to new queries. As a more general solution, we propose for
Sum-Product Networks to dynamically approximate sketches on-the-fly.
Sum-Product Networks can decompose and model multivariate distributions, such
as relations, as linear combinations of multiple univariate distributions. By
representing these univariate distributions as sketches, Sum-Product Networks
can combine them element-wise to efficiently approximate the sketch of any
query selection. These approximate sketches can then be applied to join
cardinality estimation. In particular, we implement the Fast-AGMS and Bound
Sketch methods, which have successfully been used in prior work, despite their
costly construction. By accurately approximating them instead, our work
provides a practical alternative to apply these sketches to query optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parachute: Single-Pass Bi-Directional Information Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihail Stoian, Andreas Zimmerer, Skander Krid, Amadou Latyr Ngom, Jialin Ding, Tim Kraska, Andreas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sideways information passing is a well-known technique for mitigating the
impact of large build sides in a database query plan. As currently implemented
in production systems, sideways information passing enables only a
uni-directional information flow, as opposed to instance-optimal algorithms,
such as Yannakakis'. On the other hand, the latter require an additional pass
over the input, which hinders adoption in production systems.
  In this paper, we make a step towards enabling single-pass bi-directional
information passing during query execution. We achieve this by statically
analyzing between which tables the information flow is blocked and by
leveraging precomputed join-induced fingerprint columns on FK-tables. On the
JOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time
without and with semi-join filtering by 1.54x and 1.24x, respectively, when
allowed to use 15% extra space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Integer Linear Programming All You Need for Deletion Propagation? A
  Unified and Practical Approach for Generalized Deletion Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Makhija, Wolfgang Gatterbauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deletion Propagation (DP) refers to a family of database problems rooted in
the classical view-update problem: how to propagate intended deletions in a
view (query output) back to the source database while satisfying constraints
and minimizing side effects. Although studied for over 40 years, DP variants,
their complexities, and practical algorithms have been typically explored in
isolation.
  This work presents a unified and generalized framework for DP with several
key benefits: (1) It unifies and generalizes all previously known DP variants,
effectively subsuming them within a broader class of problems, including new,
well-motivated variants. (2) It comes with a practical and general-purpose
algorithm that is ``coarse-grained instance-optimal'': it runs in PTIME for all
known PTIME cases and can automatically exploit structural regularities in the
data, i.e. it does not rely on hints about such regularities as part of the
input. (3) It is complete: our framework handles all known DP variants in all
settings (including those involving self-joins, unions, and bag semantics), and
allows us to provide new complexity results. (4) It is easy to implement and,
in many cases, outperforms prior variant-specific solutions, sometimes by
orders of magnitude. We provide the first experimental results for several DP
variants previously studied only in theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Vector Database: Storage and Retrieval
  Technique, Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Ma, Ran Zhang, Yikun Han, Shirui Yu, Zaitian Wang, Zhiyuan Ning, Jinghan Zhang, Ping Xu, Pengjiang Li, Wei Ju, Chong Chen, Dongjie Wang, Kunpeng Liu, Pengyang Wang, Pengfei Wang, Yanjie Fu, Chunjiang Liu, Yuanchun Zhou, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector databases (VDBs) have emerged to manage high-dimensional data that
exceed the capabilities of traditional database management systems, and are now
tightly integrated with large language models as well as widely applied in
modern artificial intelligence systems. Although relatively few studies
describe existing or introduce new vector database architectures, the core
technologies underlying VDBs, such as approximate nearest neighbor search, have
been extensively studied and are well documented in the literature. In this
work, we present a comprehensive review of the relevant algorithms to provide a
general understanding of this booming research area. Specifically, we first
provide a review of storage and retrieval techniques in VDBs, with detailed
design principles and technological evolution. Then, we conduct an in-depth
comparison of several advanced VDB solutions with their strengths, limitations,
and typical application scenarios. Finally, we also outline emerging
opportunities for coupling VDBs with large language models, including open
research problems and trends, such as novel indexing strategies. This survey
aims to serve as a practical resource, enabling readers to quickly gain an
overall understanding of the current knowledge landscape in this rapidly
developing area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Heuristic Framework for Resource-Efficient Querying of
  Scientific Experiments Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayank Patel, Minal Bhise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific experiments and modern applications are generating large amounts
of data every day. Most organizations utilize In-house servers or Cloud
resources to manage application data and workload. The traditional database
management system (DBMS) and HTAP systems spend significant time & resources to
load the entire dataset into DBMS before starting query execution. On the other
hand, in-situ engines may reparse required data multiple times, increasing
resource utilization and data processing costs. Additionally, over or
under-allocation of resources also increases application running costs. This
paper proposes a lightweight Resource Availability &Workload aware Hybrid
Framework (RAW-HF) to optimize querying raw data by utilizing existing finite
resources efficiently. RAW-HF includes modules that help optimize the resources
required to execute a given workload and maximize the utilization of existing
resources. The impact of applying RAW-HF to real-world scientific dataset
workloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data
(LOD) presented over 90% and 85% reduction in workload execution time (WET)
compared to widely used traditional DBMS PostgreSQL. The overall CPU, IO
resource utilization, and WET have been reduced by 26%, 25%, and 26%,
respectively, while improving memory utilization by 33%, compared to the
state-of-the-art workload-aware partial loading technique (WA) proposed for
hybrid systems. A comparison of MUAR technique used by RAW-HF with machine
learning based resource allocation techniques like PCC is also presented.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-15T00:00:00Z">2025-06-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive
  Query Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems address factual inconsistencies
in Large Language Models by grounding generation in external knowledge, yet
they face a fundamental efficiency problem: simple queries consume
computational resources equivalent to complex multi-hop reasoning tasks. We
present SymRAG, a neuro-symbolic framework that introduces adaptive query
routing based on real-time complexity and system load assessments. SymRAG
dynamically selects symbolic, neural, or hybrid processing paths to align
resource use with query demands. Evaluated on 2,000 queries from HotpotQA and
DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%
exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and
processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%
increase in processing time, highlighting the framework's impact. These results
underscore the potential of adaptive neuro-symbolic routing for scalable,
sustainable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Investigating Global News Coverage of Critical Events
  Such as Disasters and Terrorist Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica Cai, Xi Chen, Reagan Grey Keeney, Ethan Zuckerman, Brendan O'Connor, Przemyslaw A. Grabowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparative studies of news coverage are challenging to conduct because
methods to identify news articles about the same event in different languages
require expertise that is difficult to scale. We introduce an AI-powered method
for identifying news articles based on an event FINGERPRINT, which is a minimal
set of metadata required to identify critical events. Our event coverage
identification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),
efficiently identifies news articles about critical world events, specifically
terrorist attacks and several types of natural disasters. FAME does not require
training data and is able to automatically and efficiently identify news
articles that discuss an event given its fingerprint: time, location, and class
(such as storm or flood). The method achieves state-of-the-art performance and
scales to massive databases of tens of millions of news articles and hundreds
of events happening globally. We use FAME to identify 27,441 articles that
cover 470 natural disaster and terrorist attack events that happened in 2020.
To this end, we use a massive database of news articles in three languages from
MediaCloud, and three widely used, expert-curated databases of critical events:
EM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior
literature: coverage of disasters and terrorist attacks correlates to death
counts, to the GDP of a country where the event occurs, and to trade volume
between the reporting country and the country where the event occurred. We
share our NLP annotations and cross-country media attention data to support the
efforts of researchers and media monitoring organizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Performance Gap Between Lexical and Semantic Models for
  Information Retrieval With Formulaic Legal Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa Mori, Carlos Sousa de Oliveira, Yuehwern Yih, Mario Ventresca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal passage retrieval is an important task that assists legal practitioners
in the time-intensive process of finding relevant precedents to support legal
arguments. This study investigates the task of retrieving legal passages or
paragraphs from decisions of the Court of Justice of the European Union (CJEU),
whose language is highly structured and formulaic, leading to repetitive
patterns. Understanding when lexical or semantic models are more effective at
handling the repetitive nature of legal language is key to developing retrieval
systems that are more accurate, efficient, and transparent for specific legal
domains. To this end, we explore when this routinized legal language is better
suited for retrieval using methods that rely on lexical and statistical
features, such as BM25, or dense retrieval models trained to capture semantic
and contextual information. A qualitative and quantitative analysis with three
complementary metrics shows that both lexical and dense models perform well in
scenarios with more repetitive usage of language, whereas BM25 performs better
than the dense models in more nuanced scenarios where repetition and
verbatim~quotes are less prevalent and in longer queries. Our experiments also
show that BM25 is a strong baseline, surpassing off-the-shelf dense models in 4
out of 7 performance metrics. However, fine-tuning a dense model on
domain-specific data led to improved performance, surpassing BM25 in most
metrics, and we analyze the effect of the amount of data used in fine-tuning on
the model's performance and temporal robustness. The code, dataset and appendix
related to this work are available on:
https://github.com/larimo/lexsem-legal-ir.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Zhang, Jiaxiang Chen, Zhucong Li, Jie Ding, Kui Zhao, Zenglin Xu, Xin Pang, Yinghui Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Recommendation</span> systems in e-commerce applications with machine learning
  methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneta Poniszewska-Maranda, Magdalena Pakula, Bozena Borowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce platforms are increasingly reliant on recommendation systems to
enhance user experience, retain customers, and, in most cases, drive sales. The
integration of machine learning methods into these systems has significantly
improved their efficiency, personalization, and scalability. This paper aims to
highlight the current trends in e-commerce recommendation systems, identify
challenges, and evaluate the effectiveness of various machine learning methods
used, including collaborative filtering, content-based filtering, and hybrid
models. A systematic literature review (SLR) was conducted, analyzing 38
publications from 2013 to 2025. The methods used were evaluated and compared to
determine their performance and effectiveness in addressing e-commerce
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29th International Conference on Evaluation and Assessment in
  Software Engineering, 17-20 June, 2025, Istanbul, Turkey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Versatile and Fast Location-Based Private Information Retrieval with
  Fully Homomorphic Encryption over the Torus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joon Soo Yoo, Taeho Kim, Ji Won Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Location-based services often require users to share sensitive locational
data, raising privacy concerns due to potential misuse or exploitation by
untrusted servers. In response, we present VeLoPIR, a versatile location-based
private information retrieval (PIR) system designed to preserve user privacy
while enabling efficient and scalable query processing. VeLoPIR introduces
three operational modes-interval validation, coordinate validation, and
identifier matching-that support a broad range of real-world applications,
including information and emergency alerts. To enhance performance, VeLoPIR
incorporates multi-level algorithmic optimizations with parallel structures,
achieving significant scalability across both CPU and GPU platforms. We also
provide formal security and privacy proofs, confirming the system's robustness
under standard cryptographic assumptions. Extensive experiments on real-world
datasets demonstrate that VeLoPIR achieves up to 11.55 times speed-up over a
prior baseline. The implementation of VeLoPIR is publicly available at
https://github.com/PrivStatBool/VeLoPIR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Group-wise Ranking Framework for <span class="highlight-title">Recommendation</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YaChen Yan, Liubo Li, Ravi Choudhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern recommender systems, CTR/CVR models are increasingly trained with
ranking objectives to improve item ranking quality. While this shift aligns
training more closely with serving goals, most existing methods rely on
in-batch negative sampling, which predominantly surfaces easy negatives. This
limits the model's ability to capture fine-grained user preferences and weakens
overall ranking performance. To address this, we propose a Hierarchical
Group-wise Ranking Framework with two key components. First, we apply residual
vector quantization to user embeddings to generate hierarchical user codes that
partition users into hierarchical, trie-structured clusters. Second, we apply
listwise ranking losses to user-item pairs at each level of the hierarchy,
where shallow levels group loosely similar users and deeper levels group highly
similar users, reinforcing learning-to-rank signals through progressively
harder negatives. Since users with similar preferences and content exposure
tend to yield more informative negatives, applying ranking losses within these
hierarchical user groups serves as an effective approximation of hard negative
mining. Our approach improves ranking performance without requiring complex
real-time context collection or retrieval infrastructure. Extensive experiments
demonstrate that the proposed framework consistently enhances both model
calibration and ranking accuracy, offering a scalable and practical solution
for industrial recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciSage: A Multi-Agent Framework for High-Quality Scientific <span class="highlight-title">Survey</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Shi, Qian Kou, Yuduo Li, Ning Tang, Jinxin Xie, Longbin Yu, Songjing Wang, Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of scientific literature demands robust tools for automated
survey-generation. However, current large language model (LLM)-based methods
often lack in-depth analysis, structural coherence, and reliable citations. To
address these limitations, we introduce SciSage, a multi-agent framework
employing a reflect-when-you-write paradigm. SciSage features a hierarchical
Reflector agent that critically evaluates drafts at outline, section, and
document levels, collaborating with specialized agents for query
interpretation, content retrieval, and refinement. We also release SurveyScope,
a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11
computer science domains, with strict recency and citation-based quality
controls. Evaluations demonstrate that SciSage outperforms state-of-the-art
baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document
coherence and +32% in citation F1 scores. Human evaluations reveal mixed
outcomes (3 wins vs. 7 losses against human-written surveys), but highlight
SciSage's strengths in topical breadth and retrieval efficiency. Overall,
SciSage offers a promising foundation for research-assistive writing tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Device-Cloud Collaborative Correction for On-Device <span class="highlight-title">Recommendation</span> <span class="chip">IJCAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Zhan, Shengyu Zhang, Zheqi Lv, Jieming Zhu, Jiwei Li, Fan Wu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of recommendation models and device computing
power, device-based recommendation has become an important research area due to
its better real-time performance and privacy protection. Previously,
Transformer-based sequential recommendation models have been widely applied in
this field because they outperform Recurrent Neural Network (RNN)-based
recommendation models in terms of performance. However, as the length of
interaction sequences increases, Transformer-based models introduce
significantly more space and computational overhead compared to RNN-based
models, posing challenges for device-based recommendation. To balance real-time
performance and high performance on devices, we propose Device-Cloud
\underline{Co}llaborative \underline{Corr}ection Framework for On-Device
\underline{Rec}ommendation (CoCorrRec). CoCorrRec uses a self-correction
network (SCN) to correct parameters with extremely low time cost. By updating
model parameters during testing based on the input token, it achieves
performance comparable to current optimal but more complex Transformer-based
models. Furthermore, to prevent SCN from overfitting, we design a global
correction network (GCN) that processes hidden states uploaded from devices and
provides a global correction solution. Extensive experiments on multiple
datasets show that CoCorrRec outperforms existing Transformer-based and
RNN-based device recommendation models in terms of performance, with fewer
parameters and lower FLOPs, thereby achieving a balance between real-time
performance and high efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IJCAI-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DKT2: Revisiting Applicable and Comprehensive Knowledge Tracing in
  Large-Scale Data <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyun Zhou, Wenkang Han, Jingyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Tracing (KT) is a fundamental component of Intelligent Tutoring
Systems (ITS), enabling the modeling of students' knowledge states to predict
future performance. The introduction of Deep Knowledge Tracing (DKT), the first
deep learning-based KT (DLKT) model, has brought significant advantages in
terms of applicability and comprehensiveness. However, recent DLKT models, such
as Attentive Knowledge Tracing (AKT), have often prioritized predictive
performance at the expense of these benefits. While deep sequential models like
DKT have shown potential, they face challenges related to parallel computing,
storage decision modification, and limited storage capacity. To address these
limitations, we propose DKT2, a novel KT model that leverages the recently
developed xLSTM architecture. DKT2 enhances applicable input representation
using the Rasch model and incorporates Item Response Theory (IRT) for output
interpretability, allowing for the decomposition of learned knowledge into
familiar and unfamiliar knowledge. By integrating this knowledge with predicted
questions, DKT2 generates comprehensive knowledge states. Extensive experiments
conducted across three large-scale datasets demonstrate that DKT2 consistently
outperforms 18 baseline models in various prediction tasks, underscoring its
potential for real-world educational applications. This work bridges the gap
between theoretical advancements and practical implementation in KT. Our code
and datasets are fully available at https://github.com/zyy-2001/DKT2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML-PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Humans, Machine Learning, and Language Models in Union: A Cognitive
  Study on Table Unionability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreeram Marimuthu, Nina Klimenkova, Roee Shraga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data discovery and table unionability in particular became key tasks in
modern Data Science. However, the human perspective for these tasks is still
under-explored. Thus, this research investigates the human behavior in
determining table unionability within data discovery. We have designed an
experimental survey and conducted a comprehensive analysis, in which we assess
human decision-making for table unionability. We use the observations from the
analysis to develop a machine learning framework to boost the (raw) performance
of humans. Furthermore, we perform a preliminary study on how LLM performance
is compared to humans indicating that it is typically better to consider a
combination of both. We believe that this work lays the foundations for
developing future Human-in-the-Loop systems for efficient data discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 4 figures, ACM SIGMOD HILDA '25 (Status-Accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Visualizing Electronic Medical Records via Natural Language
  Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodi Zhang, Siqi Ning, Qiyong Zheng, Jinyin Nie, Liangjie Zhang, Weicheng Wang, Yuanfeng Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic medical records (EMRs) contain essential data for patient care and
clinical research. With the diversity of structured and unstructured data in
EHR, data visualization is an invaluable tool for managing and explaining these
complexities. However, the scarcity of relevant medical visualization data and
the high cost of manual annotation required to develop such datasets pose
significant challenges to advancing medical visualization techniques. To
address this issue, we propose an innovative approach using large language
models (LLMs) for generating visualization data without labor-intensive manual
annotation. We introduce a new pipeline for building text-to-visualization
benchmarks suitable for EMRs, enabling users to visualize EMR statistics
through natural language queries (NLQs). The dataset presented in this paper
primarily consists of paired text medical records, NLQs, and corresponding
visualizations, forming the first large-scale text-to-visual dataset for
electronic medical record information called MedicalVis with 35,374 examples.
Additionally, we introduce an LLM-based approach called MedCodeT5, showcasing
its viability in generating EMR visualizations from NLQs, outperforming various
strong text-to-visualization baselines. Our work facilitates standardized
evaluation of EMR visualization methods while providing researchers with tools
to advance this influential field of application. In a nutshell, this study and
dataset have the potential to promote advancements in eliciting medical
insights through visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL, which enables natural language interaction with databases,
serves as a pivotal method across diverse industries. With new, more powerful
large language models (LLMs) emerging every few months, fine-tuning has become
incredibly costly, labor-intensive, and error-prone. As an alternative,
zero-shot Text-to-SQL, which leverages the growing knowledge and reasoning
capabilities encoded in LLMs without task-specific fine-tuning, presents a
promising and more challenging direction. To address this challenge, we propose
Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)
framework to iteratively infer SQL construction actions based on partial
reasoning states. To enhance the framework's reasoning capabilities, we
introduce LLM-as-Action-Model to dynamically generate SQL construction actions
during the MCTS process, steering the search toward more promising SQL queries.
Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the
quality of candidate SQL queries, ensuring more accurate and efficient query
generation. Experimental results show that Alpha-SQL achieves 69.7% execution
accuracy on the BIRD development set, using a 32B open-source LLM without
fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based
on GPT-4o by 2.5% on the BIRD development set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Text-to-SQL in the Era of <span class="highlight-title">LLM</span>s: Where are we, and where are
  we going? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05109v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05109v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, Guoliang Li, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating users' natural language queries (NL) into SQL queries (i.e.,
Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing
relational databases and support various commercial applications. The
performance of Text-to-SQL has been greatly enhanced with the emergence of
Large Language Models (LLMs). In this survey, we provide a comprehensive review
of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from
the following four aspects: (1) Model: Text-to-SQL translation techniques that
tackle not only NL ambiguity and under-specification, but also properly map NL
with database schema and instances; (2) Data: From the collection of training
data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks;
(3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using
different metrics and granularities; and (4) Error Analysis: analyzing
Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to
evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL
solutions. Finally, we discuss the research challenges and open problems of
Text-to-SQL in the LLMs era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2SQL-BUGs: A Benchmark for Detecting Semantic Errors in NL2SQL
  Translation <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Shuyu Shen, Boyan Li, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language to SQL (i.e., NL2SQL) translation is crucial for
democratizing database access, but even state-of-the-art models frequently
generate semantically incorrect SQL queries, hindering the widespread adoption
of these techniques by database vendors. While existing NL2SQL benchmarks
primarily focus on correct query translation, we argue that a benchmark
dedicated to identifying common errors in NL2SQL translations is equally
important, as accurately detecting these errors is a prerequisite for any
subsequent correction-whether performed by humans or models. To address this
gap, we propose NL2SQL-BUGs, the first benchmark dedicated to detecting and
categorizing semantic errors in NL2SQL translation. NL2SQL-BUGs adopts a
two-level taxonomy to systematically classify semantic errors, covering 9 main
categories and 31 subcategories. The benchmark consists of 2,018
expert-annotated instances, each containing a natural language query, database
schema, and SQL query, with detailed error annotations for semantically
incorrect queries. Through comprehensive experiments, we demonstrate that
current large language models exhibit significant limitations in semantic error
detection, achieving an average detection accuracy of 75.16%. Specifically, our
method successfully detected 106 errors (accounting for 6.91%) in BIRD, a
widely-used NL2SQL dataset, which were previously undetected annotation errors.
This highlights the importance of semantic error detection in NL2SQL systems.
The benchmark is publicly available at https://nl2sql-bugs.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 4 tables, KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-14T00:00:00Z">2025-06-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INTERPOS: Interaction Rhythm Guided Positional Morphing for Mobile App
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. H. Maqbool, Moghis Fereidouni, Umar Farooq, A. B. Siddique, Hassan Foroosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mobile app market has expanded exponentially, offering millions of apps
with diverse functionalities, yet research in mobile app recommendation remains
limited. Traditional sequential recommender systems utilize the order of items
in users' historical interactions to predict the next item for the users.
Position embeddings, well-established in transformer-based architectures for
natural language processing tasks, effectively distinguish token positions in
sequences. In sequential recommendation systems, position embeddings can
capture the order of items in a user's historical interaction sequence.
Nevertheless, this ordering does not consider the time elapsed between two
interactions of the same user (e.g., 1 day, 1 week, 1 month), referred to as
"user rhythm". In mobile app recommendation datasets, the time between
consecutive user interactions is notably longer compared to other domains like
movies, posing significant challenges for sequential recommender systems. To
address this phenomenon in the mobile app domain, we introduce INTERPOS, an
Interaction Rhythm Guided Positional Morphing strategy for autoregressive
mobile app recommender systems. INTERPOS incorporates rhythm-guided position
embeddings, providing a more comprehensive representation that considers both
the sequential order of interactions and the temporal gaps between them. This
approach enables a deep understanding of users' rhythms at a fine-grained
level, capturing the intricacies of their interaction patterns over time. We
propose three strategies to incorporate the morphed positional embeddings in
two transformer-based sequential recommendation system architectures. Our
extensive evaluations show that INTERPOS outperforms state-of-the-art models
using 7 mobile app recommendation datasets on NDCG@K and HIT@K metrics. The
source code of INTERPOS is available at https://github.com/dlgrad/INTERPOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Generating Conversational <span class="highlight-title">Recommendation</span> Datasets from
  Behavioral Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinaik Chhetri, Yousaf Reza, Moghis Fereidouni, Srijata Maji, Umar Farooq, AB Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommendation systems typically follow two complementary paradigms:
collaborative filtering, which models long-term user preferences from
historical interactions, and conversational recommendation systems (CRS), which
interact with users in natural language to uncover immediate needs. Each
captures a different dimension of user intent. While CRS models lack
collaborative signals, leading to generic or poorly personalized suggestions,
traditional recommenders lack mechanisms to interactively elicit immediate
needs. Unifying these paradigms promises richer personalization but remains
challenging due to the lack of large-scale conversational datasets grounded in
real user behavior. We present ConvRecStudio, a framework that uses large
language models (LLMs) to simulate realistic, multi-turn dialogs grounded in
timestamped user-item interactions and reviews. ConvRecStudio follows a
three-stage pipeline: (1) Temporal Profiling, which constructs user profiles
and community-level item sentiment trajectories over fine-grained aspects; (2)
Semantic Dialog Planning, which generates a structured plan using a DAG of
flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the
plan using paired LLM agents for the user and system, constrained by
executional and behavioral fidelity checks. We apply ConvRecStudio to three
domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K
multi-turn dialogs per dataset. Human and automatic evaluations confirm the
naturalness, coherence, and behavioral grounding of the generated
conversations. To demonstrate utility, we build a cross-attention transformer
model that jointly encodes user history and dialog context, achieving gains in
Hit@K and NDCG@K over baselines using either signal alone or naive fusion.
Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the
strongest baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 tables,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gradient Meta-Learning Joint Optimization for Beamforming and Antenna
  Position in Pinching-Antenna Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Zhou, Weixi Zhou, Donghong Cai, Xianfu Lei, Yanqing Xu, Zhiguo Ding, Pingzhi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider a novel optimization design for multi-waveguide
pinching-antenna systems, aiming to maximize the weighted sum rate (WSR) by
jointly optimizing beamforming coefficients and antenna position. To handle the
formulated non-convex problem, a gradient-based meta-learning joint
optimization (GML-JO) algorithm is proposed. Specifically, the original problem
is initially decomposed into two sub-problems of beamforming optimization and
antenna position optimization through equivalent substitution. Then, the convex
approximation methods are used to deal with the nonconvex constraints of
sub-problems, and two sub-neural networks are constructed to calculate the
sub-problems separately. Different from alternating optimization (AO), where
two sub-problems are solved alternately and the solutions are influenced by the
initial values, two sub-neural networks of proposed GML-JO with fixed channel
coefficients are considered as local sub-tasks and the computation results are
used to calculate the loss function of joint optimization. Finally, the
parameters of sub-networks are updated using the average loss function over
different sub-tasks and the solution that is robust to the initial value is
obtained. Simulation results demonstrate that the proposed GML-JO algorithm
achieves 5.6 bits/s/Hz WSR within 100 iterations, yielding a 32.7\% performance
enhancement over conventional AO with substantially reduced computational
complexity. Moreover, the proposed GML-JO algorithm is robust to different
choices of initialization and yields better performance compared with the
existing optimization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating Financial Statement Audits with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushi Wang, Jiateng Liu, Weijie Zhao, Shenglan Li, Denghui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial statement auditing is essential for stakeholders to understand a
company's financial health, yet current manual processes are inefficient and
error-prone. Even with extensive verification procedures, auditors frequently
miss errors, leading to inaccurate financial statements that fail to meet
stakeholder expectations for transparency and reliability. To this end, we
harness large language models (LLMs) to automate financial statement auditing
and rigorously assess their capabilities, providing insights on their
performance boundaries in the scenario of automated auditing. Our work
introduces a comprehensive benchmark using a curated dataset combining
real-world financial tables with synthesized transaction data. In the
benchmark, we developed a rigorous five-stage evaluation framework to assess
LLMs' auditing capabilities. The benchmark also challenges models to map
specific financial statement errors to corresponding violations of accounting
standards, simulating real-world auditing scenarios through test cases. Our
testing reveals that current state-of-the-art LLMs successfully identify
financial statement errors when given historical transaction data. However,
these models demonstrate significant limitations in explaining detected errors
and citing relevant accounting standards. Furthermore, LLMs struggle to execute
complete audits and make necessary financial statement revisions. These
findings highlight a critical gap in LLMs' domain-specific accounting
knowledge. Future research must focus on enhancing LLMs' understanding of
auditing principles and procedures. Our benchmark and evaluation framework
establish a foundation for developing more effective automated auditing tools
that will substantially improve the accuracy and efficiency of real-world
financial statement auditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DoTA-RAG: Dynamic of Thought Aggregation RAG <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saksorn Ruangtanusak, Natthapath Rungseesiripak, Peerawat Rojratchadakorn, Monthol Charattrakool, Natapong Nitarach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a
retrieval-augmented generation system optimized for high-throughput,
large-scale web knowledge indexes. Traditional RAG pipelines often suffer from
high latency and limited accuracy over massive, diverse datasets. DoTA-RAG
addresses these challenges with a three-stage pipeline: query rewriting,
dynamic routing to specialized sub-indexes, and multi-stage retrieval and
ranking. We further enhance retrieval by evaluating and selecting a superior
embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we
create a diverse Q&A dataset of 500 questions generated via the DataMorgana
setup across a broad range of WebOrganizer topics and formats. DoTA-RAG
improves the answer correctness score from 0.752 (baseline, using LiveRAG
pre-built vector store) to 1.478 while maintaining low latency, and it achieves
a 0.929 correctness score on the Live Challenge Day. These results highlight
DoTA-RAG's potential for practical deployment in domains requiring fast,
reliable access to large and evolving knowledge sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR LiveRAG 2025 (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuocheng Zhang, Yang Feng, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large
language model applications, with numerous existing frameworks offering a wide
range of functionalities to facilitate the development of RAG systems. However,
we have identified several persistent challenges in these frameworks, including
difficulties in algorithm reproduction and sharing, lack of new techniques, and
high system overhead. To address these limitations, we introduce
\textbf{FlexRAG}, an open-source framework specifically designed for research
and prototyping. FlexRAG supports text-based, multimodal, and network-based
RAG, providing comprehensive lifecycle support alongside efficient asynchronous
processing and persistent caching capabilities. By offering a robust and
flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and
share advanced RAG systems. Our toolkit and resources are available at
\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Demo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORONA: A Coarse-to-Fine Framework for Graph-based <span class="highlight-title">Recommendation</span> with
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junze Chen, Xinjie Yang, Cheng Yang, Junfei Bao, Zeyuan Guo, Yawen Li, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) are designed to retrieve candidate items a user
might be interested in from a large pool. A common approach is using graph
neural networks (GNNs) to capture high-order interaction relationships. As
large language models (LLMs) have shown strong capabilities across domains,
researchers are exploring their use to enhance recommendation. However, prior
work limits LLMs to re-ranking results or dataset augmentation, failing to
utilize their power during candidate filtering - which may lead to suboptimal
performance. Instead, we propose to leverage LLMs' reasoning abilities during
the candidate filtering process, and introduce Chain Of Retrieval ON grAphs
(CORONA) to progressively narrow down the range of candidate items on
interaction graphs with the help of LLMs: (1) First, LLM performs preference
reasoning based on user profiles, with the response serving as a query to
extract relevant users and items from the interaction graph as
preference-assisted retrieval; (2) Then, using the information retrieved in the
previous step along with the purchase history of target user, LLM conducts
intent reasoning to help refine an even smaller interaction subgraph as
intent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order
collaborative filtering information from the extracted subgraph, performing
GNN-enhanced retrieval to generate the final recommendation results. The
proposed framework leverages the reasoning capabilities of LLMs during the
retrieval process, while seamlessly integrating GNNs to enhance overall
recommendation performance. Extensive experiments on various datasets and
settings demonstrate that our proposed CORONA achieves state-of-the-art
performance with an 18.6% relative improvement in recall and an 18.4% relative
improvement in NDCG on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scholar Inbox: Personalized Paper <span class="highlight-title">Recommendation</span>s for Scientists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Flicke, Glenn Angrabeit, Madhav Iyengar, Vitalii Protsenko, Illia Shakun, Jovan Cicvaric, Bora Kargi, Haoyu He, Lukas Schuler, Lewin Scholz, Kavyanjali Agnihotri, Yong Cao, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scholar Inbox is a new open-access platform designed to address the
challenges researchers face in staying current with the rapidly expanding
volume of scientific literature. We provide personalized recommendations,
continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual
paper summaries, semantic search, and a range of tools to streamline research
workflows and promote open research access. The platform's personalized
recommendation system is trained on user ratings, ensuring that recommendations
are tailored to individual researchers' interests. To further enhance the user
experience, Scholar Inbox also offers a map of science that provides an
overview of research across domains, enabling users to easily explore specific
topics. We use this map to address the cold start problem common in recommender
systems, as well as an active learning strategy that iteratively prompts users
to rate a selection of papers, allowing the system to learn user preferences
quickly. We evaluate the quality of our recommendation system on a novel
dataset of 800k user ratings, which we make publicly available, as well as via
an extensive user study. https://www.scholar-inbox.com/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://www.scholar-inbox.com/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co<span class="highlight-title">LLM</span>: Integrating Collaborative Embeddings into <span class="highlight-title">Large Language Model</span>s
  for <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Large Language Models as Recommenders (LLMRec) has gained
significant attention and introduced fresh perspectives in user preference
modeling. Existing LLMRec approaches prioritize text semantics, usually
neglecting the valuable collaborative information from user-item interactions
in recommendations. While these text-emphasizing approaches excel in cold-start
scenarios, they may yield sub-optimal performance in warm-start situations. In
pursuit of superior recommendations for both cold and warm start scenarios, we
introduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates
collaborative information into LLMs for recommendation. CoLLM captures
collaborative information through an external traditional model and maps it to
the input token embedding space of LLM, forming collaborative embeddings for
LLM usage. Through this external integration of collaborative information,
CoLLM ensures effective modeling of collaborative information without modifying
the LLM itself, providing the flexibility to employ various collaborative
information modeling techniques. Extensive experiments validate that CoLLM
adeptly integrates collaborative information into LLMs, resulting in enhanced
recommendation performance. We release the code and data at
https://github.com/zyang1580/CoLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE 2025 (add new LLM backbone Qwen2-1.5B and NDCG
  metric)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Representation for Interactive <span class="highlight-title">Recommendation</span> <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Li, Zhiyong Feng, Dongxiao He, Hongqi Chen, Qinghang Gao, Guoli Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive Recommendation (IR) has gained significant attention recently for
its capability to quickly capture dynamic interest and optimize both short and
long term objectives. IR agents are typically implemented through Deep
Reinforcement Learning (DRL), because DRL is inherently compatible with the
dynamic nature of IR. However, DRL is currently not perfect for IR. Due to the
large action space and sample inefficiency problem, training DRL recommender
agents is challenging. The key point is that useful features cannot be
extracted as high-quality representations for the recommender agent to optimize
its policy. To tackle this problem, we propose Contrastive Representation for
Interactive Recommendation (CRIR). CRIR efficiently extracts latent, high-level
preference ranking features from explicit interaction, and leverages the
features to enhance users' representation. Specifically, the CRIR provides
representation through one representation network, and refines it through our
proposed Preference Ranking Contrastive Learning (PRCL). The key insight of
PRCL is that it can perform contrastive learning without relying on
computations involving high-level representations or large potential action
sets. Furthermore, we also propose a data exploiting mechanism and an agent
training mechanism to better adapt CRIR to the DRL backbone. Extensive
experiments have been carried out to show our method's superior improvement on
the sample efficiency while training an DRL-based IR agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI-2025 Accepted paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redbench: A Benchmark Reflecting Real Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skander Krid, Mihail Stoian, Andreas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance-optimized components have made their way into production systems. To
some extent, this adoption is due to the characteristics of customer workloads,
which can be individually leveraged during the model training phase. However,
there is a gap between research and industry that impedes the development of
realistic learned components: the lack of suitable workloads. Existing ones,
such as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail
to exhibit real workload patterns, particularly distribution shifts.
  In this paper, we introduce Redbench, a collection of 30 workloads that
reflect query patterns observed in the real world. The workloads were obtained
by sampling queries from support benchmarks and aligning them with workload
characteristics observed in Redset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Eighth International Workshop on Exploiting Artificial Intelligence
  Techniques for Data Management (aiDM 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in <span class="highlight-title">LLM</span>s with Focus on Reasoning, Adaptability, Efficiency and
  Ethics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asifullah khan, Muhammad Zaeem Khan, Saleha Jamshed, Sadia Ahmad, Aleesha Zainab, Kaynat Khatib, Faria Bibi, Abdul Rehman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper outlines the key developments in the field of Large
Language Models (LLMs), such as enhancing their reasoning skills, adaptability
to various tasks, increased computational efficiency, and ability to make
ethical decisions. The techniques that have been most effective in bridging the
gap between human and machine communications include the Chain-of-Thought
prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.
The improvements in multimodal learning and few-shot or zero-shot techniques
have further empowered LLMs to handle complex jobs with minor input. They also
manage to do more with less by applying scaling and optimization tricks for
computing power conservation. This survey also offers a broader perspective on
recent advancements in LLMs going beyond isolated aspects such as model
architecture or ethical concerns. It categorizes emerging methods that enhance
LLM reasoning, efficiency, and ethical alignment. It also identifies
underexplored areas such as interpretability, cross-modal integration and
sustainability. With recent progress, challenges like huge computational costs,
biases, and ethical risks remain constant. Addressing these requires bias
mitigation, transparent decision-making, and clear ethical guidelines. Future
research will focus on enhancing models ability to handle multiple input,
thereby making them more intelligent, safe, and reliable.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-13T00:00:00Z">2025-06-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Research Agents are a prominent category of LLM-based agents. By
autonomously orchestrating multistep web exploration, targeted retrieval, and
higher-order synthesis, they transform vast amounts of online information into
analyst-grade, citation-rich reports--compressing hours of manual desk research
into minutes. However, a comprehensive benchmark for systematically evaluating
the capabilities of these agents remains absent. To bridge this gap, we present
DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,
each meticulously crafted by domain experts across 22 distinct fields.
Evaluating DRAs is inherently complex and labor-intensive. We therefore propose
two novel methodologies that achieve strong alignment with human judgment. The
first is a reference-based method with adaptive criteria to assess the quality
of generated research reports. The other framework is introduced to evaluate
DRA's information retrieval and collection capabilities by assessing its
effective citation count and overall citation accuracy. We have open-sourced
DeepResearch Bench and key components of these frameworks at
https://github.com/Ayanami0730/deep_research_bench to accelerate the
development of practical LLM-based agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphRAG-Causal: A novel graph-augmented framework for causal reasoning
  and annotation in news 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Haque, Umm e Hani, Ahmad Din, Muhammad Babar, Ali Abbas, Insaf Ullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GraphRAG-Causal introduces an innovative framework that combines graph-based
retrieval with large language models to enhance causal reasoning in news
analysis. Traditional NLP approaches often struggle with identifying complex,
implicit causal links, especially in low-data scenarios. Our approach addresses
these challenges by transforming annotated news headlines into structured
causal knowledge graphs. It then employs a hybrid retrieval system that merges
semantic embeddings with graph-based structural cues leveraging Neo4j to
accurately match and retrieve relevant events. The framework is built on a
three-stage pipeline: First, during Data Preparation, news sentences are
meticulously annotated and converted into causal graphs capturing cause,
effect, and trigger relationships. Next, the Graph Retrieval stage stores these
graphs along with their embeddings in a Neo4j database and utilizes hybrid
Cypher queries to efficiently identify events that share both semantic and
structural similarities with a given query. Finally, the LLM Inference stage
utilizes these retrieved causal graphs in a few-shot learning setup with
XML-based prompting, enabling robust classification and tagging of causal
relationships. Experimental evaluations demonstrate that GraphRAG-Causal
achieves an impressive F1-score of 82.1% on causal classification using just 20
few-shot examples. This approach significantly boosts accuracy and consistency,
making it highly suitable for real-time applications in news reliability
assessment, misinformation detection, and policy analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chunk Twice, Embed Once: A Systematic Study of Segmentation and
  Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Amiri, Thomas Bocklitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems are increasingly vital for
navigating the ever-expanding body of scientific literature, particularly in
high-stakes domains such as chemistry. Despite the promise of RAG, foundational
design choices -- such as how documents are segmented and represented -- remain
underexplored in domain-specific contexts. This study presents the first
large-scale, systematic evaluation of chunking strategies and embedding models
tailored to chemistry-focused RAG systems. We investigate 25 chunking
configurations across five method families and evaluate 48 embedding models on
three chemistry-specific benchmarks, including the newly introduced
QuestChemRetrieval dataset. Our results reveal that recursive token-based
chunking (specifically R100-0) consistently outperforms other approaches,
offering strong performance with minimal resource overhead. We also find that
retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --
substantially outperform domain-specialized models like SciBERT. By releasing
our datasets, evaluation framework, and empirical benchmarks, we provide
actionable guidelines for building effective and efficient chemistry-aware RAG
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-View Disentangled Multi-Intent Learning for Enhanced Collaborative
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanfan Zhang, Yongyi Lin, Yuan Rao, Chenlong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disentangling user intentions from implicit feedback has become a promising
strategy to enhance recommendation accuracy and interpretability. Prior methods
often model intentions independently and lack explicit supervision, thus
failing to capture the joint semantics that drive user-item interactions. To
address these limitations, we propose DMICF, a unified framework that
explicitly models interaction-level intent alignment while leveraging
structural signals from both user and item perspectives. DMICF adopts a
dual-view architecture that jointly encodes user-item interaction graphs from
both sides, enabling bidirectional information fusion. This design enhances
robustness under data sparsity by allowing the structural redundancy of one
view to compensate for the limitations of the other. To model fine-grained
user-item compatibility, DMICF introduces an intent interaction encoder that
performs sub-intent alignment within each view, uncovering shared semantic
structures that underlie user decisions. This localized alignment enables
adaptive refinement of intent embeddings based on interaction context, thus
improving the model's generalization and expressiveness, particularly in
long-tail scenarios. Furthermore, DMICF integrates an intent-aware scoring
mechanism that aggregates compatibility signals from matched intent pairs
across user and item subspaces, enabling personalized prediction grounded in
semantic congruence rather than entangled representations. To facilitate
semantic disentanglement, we design a discriminative training signal via
multi-negative sampling and softmax normalization, which pulls together
semantically aligned intent pairs while pushing apart irrelevant or noisy ones.
Extensive experiments demonstrate that DMICF consistently delivers robust
performance across datasets with diverse interaction distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Reference Documents for Zero-Shot Ranking via Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieran Li, Xiuyuan Hu, Yang Zhao, Shengyao Zhuang, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional performance in the
task of text ranking for information retrieval. While Pointwise ranking
approaches offer computational efficiency by scoring documents independently,
they often yield biased relevance estimates due to the lack of inter-document
comparisons. In contrast, Pairwise methods improve ranking accuracy by
explicitly comparing document pairs, but suffer from substantial computational
overhead with quadratic complexity ($O(n^2)$). To address this tradeoff, we
propose \textbf{RefRank}, a simple and effective comparative ranking method
based on a fixed reference document. Instead of comparing all document pairs,
RefRank prompts the LLM to evaluate each candidate relative to a shared
reference anchor. By selecting the reference anchor that encapsulates the core
query intent, RefRank implicitly captures relevance cues, enabling indirect
comparison between documents via this common anchor. This reduces computational
cost to linear time ($O(n)$) while importantly, preserving the advantages of
comparative evaluation. To further enhance robustness, we aggregate multiple
RefRank outputs using a weighted averaging scheme across different reference
choices. Experiments on several benchmark datasets and with various LLMs show
that RefRank significantly outperforms Pointwise baselines and could achieve
performance at least on par with Pairwise approaches with a significantly lower
computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Augmented Code Generation Using Programming Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Saberi, Fatemeh Fard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Litigation: Graphs and <span class="highlight-title">LLM</span>s for Retrieval and Reasoning in
  eDiscovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sounak Lahiri, Sumit Pai, Tim Weninger, Sanmitra Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Discovery (eDiscovery) requires identifying relevant documents
from vast collections for legal production requests. While artificial
intelligence (AI) and natural language processing (NLP) have improved document
review efficiency, current methods still struggle with legal entities,
citations, and complex legal artifacts. To address these challenges, we
introduce DISCOvery Graph (DISCOG), an emerging system that integrates
knowledge graphs for enhanced document ranking and classification, augmented by
LLM-driven reasoning. DISCOG outperforms strong baselines in F1-score,
precision, and recall across both balanced and imbalanced datasets. In
real-world deployments, it has reduced litigation-related document review costs
by approximately 98\%, demonstrating significant business impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated with Camera Ready Copy for ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Personalized Conversational Sales Agents: Contextual User
  Profiling for Strategic Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08754v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08754v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyoung Kim, Jeongeun Lee, Soojin Yoon, Sunghwan Kim, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRSs)aim to engage users in dialogue to
provide tailored recommendations. While traditional CRSs focus on eliciting
preferences and retrieving items, real-world e-commerce interactions involve
more complex decision-making, where users consider multiple factors beyond
simple attributes. To capture this complexity, we introduce Conversational
Sales (CSALES), a novel task that integrates preference elicitation,
recommendation, and persuasion within a unified conversational framework. To
support realistic and systematic evaluation, we present CSUSER, an evaluation
protocol with LLM-based user simulator grounded in real-world behavioral data
by modeling fine-grained user profiles for personalized interaction. We also
propose CSI, a conversational sales agent that proactively infers contextual
user profiles and strategically selects actions through conversation.
Comprehensive experiments show that CSI significantly improves both
recommendation success and persuasive effectiveness across diverse user
profiles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Datasets for Information <span class="highlight-title">Diffusion</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxia Guo, Xiaowen Wang, Yanwei Xie, Zehao Wang, Jingqiu Li, Lanjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information diffusion across various new media platforms gradually influences
perceptions, decisions, and social behaviors of individual users. In
communication studies, the famous Five W's of Communication model (5W Model)
has displayed the process of information diffusion clearly. At present,
although plenty of studies and corresponding datasets about information
diffusion have emerged, a systematic categorization of tasks and an integration
of datasets are still lacking. To address this gap, we survey a systematic
taxonomy of information diffusion tasks and datasets based on the "5W Model"
framework. We first categorize the information diffusion tasks into ten
subtasks with definitions and datasets analysis, from three main tasks of
information diffusion prediction, social bot detection, and misinformation
detection. We also collect the publicly available dataset repository of
information diffusion tasks with the available links and compare them based on
six attributes affiliated to users and content: user information, social
network, bot label, propagation content, propagation network, and veracity
label. In addition, we discuss the limitations and future directions of current
datasets and research topics to advance the future development of information
diffusion. The dataset repository can be accessed at our website
https://github.com/fuxiaG/Information-Diffusion-Datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. Martinez-Sevilla, Joan Cerveto-Serrano, Noelia Luna, Greg Chapman, Craig Sapp, David Rizo, Jorge Calvo-Zaragoza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six
hundred and eighty-five pages specifically designed to benchmark Optical Music
Recognition (OMR) research. SMB encompasses a diverse array of musical
textures, including monophony, pianoform, quartet, and others, all encoded in
Common Western Modern Notation using the Humdrum **kern format. Alongside SMB,
we introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored
explicitly for evaluating OMR performance. OMR-NED builds upon the widely-used
Symbol Error Rate (SER), offering a fine-grained and detailed error analysis
that covers individual musical elements such as note heads, beams, pitches,
accidentals, and other critical notation features. The resulting numeric score
provided by OMR-NED facilitates clear comparisons, enabling researchers and
end-users alike to identify optimal OMR approaches. Our work thus addresses a
long-standing gap in OMR evaluation, and we support our contributions with
baseline experiments using standardized SMB dataset splits for training and
assessing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-Assisted Relevance Assessments: When Should We Ask <span class="highlight-title">LLM</span>s for Help? <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06877v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06877v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai, Ian Soboroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test collections are information-retrieval tools that allow researchers to
quickly and easily evaluate ranking algorithms. While test collections have
become an integral part of IR research, the process of data creation involves
significant manual-annotation effort, which often makes it very expensive and
time-consuming. Consequently, test collections can become too small when the
budget is limited, which may lead to unstable evaluations. As a cheaper
alternative, recent studies have proposed using large language models (LLMs) to
completely replace human assessors. However, while LLMs correlate to some
extent with human judgments, their predictions are not perfect and often show
bias. Thus, a complete replacement with LLMs is considered too risky and not
fully reliable.
  In this paper, we propose LLM-Assisted Relevance Assessments (LARA), an
effective method to balance manual annotations with LLM annotations, helping
build a rich and reliable test collection even under a low budget. We use the
LLM's predicted relevance probabilities to select the most profitable documents
for manual annotation under a budget constraint. Guided by theoretical
reasoning, LARA actively learns to calibrate the LLM's predicted relevance
probabilities, directing the human-annotation process. Then, using the
calibration model learned from the limited manual annotations, LARA debiases
the LLM predictions to annotate the remaining non-assessed data. Experiments on
TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and TREC-COVID datasets show
that LARA outperforms alternative solutions under almost any budget constraint.
While the community debates humans versus LLMs in relevance assessments, we
contend that, given the same amount of human effort, it is reasonable to
leverage LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. Accepted at SIGIR 2025 (48th International ACM SIGIR
  Conference on Research and Development in Information Retrieval)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust <span class="highlight-title">Recommendation</span>: A <span class="highlight-title">Review</span> and an Adversarial Robustness
  Evaluation Library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Cheng, Xiaowen Huang, Jitao Sang, Jian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, recommender system has achieved significant success. However, due
to the openness of recommender systems, they remain vulnerable to malicious
attacks. Additionally, natural noise in training data and issues such as data
sparsity can also degrade the performance of recommender systems. Therefore,
enhancing the robustness of recommender systems has become an increasingly
important research topic. In this survey, we provide a comprehensive overview
of the robustness of recommender systems. Based on our investigation, we
categorize the robustness of recommender systems into adversarial robustness
and non-adversarial robustness. In the adversarial robustness, we introduce the
fundamental principles and classical methods of recommender system adversarial
attacks and defenses. In the non-adversarial robustness, we analyze
non-adversarial robustness from the perspectives of data sparsity, natural
noise, and data imbalance. Additionally, we summarize commonly used datasets
and evaluation metrics for evaluating the robustness of recommender systems.
Finally, we also discuss the current challenges in the field of recommender
system robustness and potential future research directions. Additionally, to
facilitate fair and efficient evaluation of attack and defense methods in
adversarial robustness, we propose an adversarial robustness evaluation
library--ShillingREC, and we conduct evaluations of basic attack models and
recommendation models. ShillingREC project is released at
https://github.com/chengleileilei/ShillingREC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on
  Technical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, Andrew Drozdov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FreshStack, a holistic framework for automatically building
information retrieval (IR) evaluation benchmarks by incorporating challenging
questions and answers. FreshStack conducts the following steps: (1) automatic
corpus collection from code and technical documentation, (2) nugget generation
from community-asked questions and answers, and (3) nugget-level support,
retrieving documents using a fusion of retrieval techniques and hybrid
architectures. We use FreshStack to build five datasets on fast-growing,
recent, and niche topics to ensure the tasks are sufficiently challenging. On
FreshStack, existing retrieval models, when applied out-of-the-box,
significantly underperform oracle approaches on all five topics, denoting
plenty of headroom to improve IR quality. In addition, we identify cases where
rerankers do not improve first-stage retrieval accuracy (two out of five
topics) and oracle context helps an LLM generator generate a high-quality RAG
answer. We hope FreshStack will facilitate future work toward constructing
realistic, scalable, and uncontaminated IR and RAG evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Schema-R1: A reasoning training approach for schema linking in
  Text-to-SQL Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuzhenghong Wen, Su Pan, yuwei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-based Dynamic Differential Testing for Database Connectors with
  Reinforcement Learning-Guided Prompt Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Lyu, Minghao Zhao, Yanhao Wang, Liang Jie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database connectors are critical components enabling applications to interact
with underlying database management systems (DBMS), yet their security
vulnerabilities often remain overlooked. Unlike traditional software defects,
connector vulnerabilities exhibit subtle behavioral patterns and are inherently
challenging to detect. Besides, nonstandardized implementation of connectors
leaves potential risks (a.k.a. unsafe implementations) but is more elusive. As
a result, traditional fuzzing methods are incapable of finding such
vulnerabilities. Even for LLM-enable test case generation, due to a lack of
domain knowledge, they are also incapable of generating test cases that invoke
all interface and internal logic of connectors. In this paper, we propose
reinforcement learning (RL)-guided LLM test-case generation for database
connector testing. Specifically, to equip the LLM with sufficient and
appropriate domain knowledge, a parameterized prompt template is composed which
can be utilized to generate numerous prompts. Test cases are generated via LLM
with a prompt, and are dynamically evaluated through differential testing
across multiple connectors. The testing is iteratively conducted, with each
round RL is adopted to select optimal prompt based on prior-round behavioral
feedback, so as to maximize control flow coverage. We implement aforementioned
methodology in a practical tool and evaluate it on two widely used JDBC
connectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported
16 bugs, among them 10 are officially confirmed and the rest are acknowledged
as unsafe implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCPQ: Object-Centric Process Querying & Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Küsters, Wil M. P. van der Aalst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process querying is used to extract information and insights from process
execution data. Similarly, process constraints can be checked against input
data, yielding information on which process instances violate them.
Traditionally, such process mining techniques use case-centric event data as
input. However, with the uptake of Object-Centric Process Mining (OCPM),
existing querying and constraint checking techniques are no longer applicable.
Object-Centric Event Data (OCED) removes the requirement to pick a single case
notion (i.e., requiring that events belong to exactly one case) and can thus
represent many real-life processes much more accurately. In this paper, we
present a novel highly-expressive approach for object-centric process querying,
called OCPQ. It supports a wide variety of applications, including OCED-based
constraint checking and filtering. The visual representation of nested queries
in OCPQ allows users to intuitively read and create queries and constraints. We
implemented our approach using (1) a high-performance execution engine backend
and (2) an easy-to-use editor frontend. Additionally, we evaluated our approach
on a real-life dataset, showing the lack in expressiveness of prior work and
runtime performance significantly better than the general querying solutions
SQLite and Neo4j, as well as comparable to the performance-focused DuckDB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conjunctive Queries with Free Access Patterns under Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09032v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09032v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Kara, Milos Nikolic, Dan Olteanu, Haozhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of answering conjunctive queries with free access
patterns (CQAPs) under updates. A free access pattern is a partition of the
free variables of the query into input and output. The query returns tuples
over the output variables given a tuple of values over the input variables.
  We introduce a fully dynamic evaluation approach that works for all CQAPs and
is optimal for two classes of CQAPs. This approach recovers prior work on the
dynamic evaluation of conjunctive queries without access patterns.
  We first give a syntactic characterisation of all CQAPs that admit constant
time per single-tuple update and whose output tuples can be enumerated with
constant delay given a tuple of values over the input variables.
  We further chart the complexity trade-off between the preprocessing time,
update time and enumeration delay for a class of CQAPs. For some of these
CQAPs, our approach achieves optimal, albeit non-constant, update time and
delay. This optimality is predicated on the Online Matrix-Vector Multiplication
conjecture.
  We finally adapt our approach to the dynamic evaluation of tractable CQAPs
over probabilistic databases under updates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Algorithms for Fair Max-Min Diversification in $\mathbb{R}^d$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Kurkure, Miles Shamo, Joseph Wiseman, Sainyam Galhotra, Stavros Sintos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of extracting a diverse subset from a dataset, often referred to as
maximum diversification, plays a pivotal role in various real-world
applications that have far-reaching consequences. In this work, we delve into
the realm of fairness-aware data subset selection, specifically focusing on the
problem of selecting a diverse set of size $k$ from a large collection of $n$
data points (FairDiv).
  The FairDiv problem is well-studied in the data management and theory
community. In this work, we develop the first constant approximation algorithm
for FairDiv that runs in near-linear time using only linear space. In contrast,
all previously known constant approximation algorithms run in super-linear time
(with respect to $n$ or $k$) and use super-linear space. Our approach achieves
this efficiency by employing a novel combination of the Multiplicative Weight
Update method and advanced geometric data structures to implicitly and
approximately solve a linear program. Furthermore, we improve the efficiency of
our techniques by constructing a coreset. Using our coreset, we also propose
the first efficient streaming algorithm for the FairDiv problem whose
efficiency does not depend on the distribution of data points. Empirical
evaluation on million-sized datasets demonstrates that our algorithm achieves
the best diversity within a minute. All prior techniques are either highly
inefficient or do not generate a good solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable
  with DBOS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vasquez-Grinnell, Alex Poliakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To meet the needs of a large pharmaceutical organization, we set out to
create S3Mirror - an application for transferring large genomic sequencing
datasets between S3 buckets quickly, reliably, and observably. We used the DBOS
Transact durable execution framework to achieve these goals and benchmarked the
performance and cost of the application. S3Mirror is an open source DBOS Python
application that can run in a variety of environments, including DBOS Cloud
Pro, where it runs as much as 40x faster than AWS DataSync at a fraction of the
cost. Moreover, S3Mirror is resilient to failures and allows for real-time
filewise observability of ongoing and past transfers.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-12T00:00:00Z">2025-06-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been increasingly applied to automated
harmful content detection tasks, assisting moderators in identifying policy
violations and improving the overall efficiency and accuracy of content review.
However, existing resources for harmful content detection are predominantly
focused on English, with Chinese datasets remaining scarce and often limited in
scope. We present a comprehensive, professionally annotated benchmark for
Chinese content harm detection, which covers six representative categories and
is constructed entirely from real-world data. Our annotation process further
yields a knowledge rule base that provides explicit expert knowledge to assist
LLMs in Chinese harmful content detection. In addition, we propose a
knowledge-augmented baseline that integrates both human-annotated knowledge
rules and implicit knowledge from large language models, enabling smaller
models to achieve performance comparable to state-of-the-art LLMs. Code and
data are available at https://github.com/zjunlp/ChineseHarm-bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Zero-Shot Pointwise Ranking with <span class="highlight-title">LLM</span>s through Post-Aggregated
  Global Context Information <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehan Long, Shasha Li, Chen Xu, Jintao Tang, Ting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have successfully harnessed the power of Large Language
Models (LLMs) for zero-shot document ranking, exploring a variety of prompting
strategies. Comparative approaches like pairwise and listwise achieve high
effectiveness but are computationally intensive and thus less practical for
larger-scale applications. Scoring-based pointwise approaches exhibit superior
efficiency by independently and simultaneously generating the relevance scores
for each candidate document. However, this independence ignores critical
comparative insights between documents, resulting in inconsistent scoring and
suboptimal performance. In this paper, we aim to improve the effectiveness of
pointwise methods while preserving their efficiency through two key
innovations: (1) We propose a novel Global-Consistent Comparative Pointwise
Ranking (GCCP) strategy that incorporates global reference comparisons between
each candidate and an anchor document to generate contrastive relevance scores.
We strategically design the anchor document as a query-focused summary of
pseudo-relevant candidates, which serves as an effective reference point by
capturing the global context for document comparison. (2) These contrastive
relevance scores can be efficiently Post-Aggregated with existing pointwise
methods, seamlessly integrating essential Global Context information in a
training-free manner (PAGC). Extensive experiments on the TREC DL and BEIR
benchmark demonstrate that our approach significantly outperforms previous
pointwise methods while maintaining comparable efficiency. Our method also
achieves competitive performance against comparative methods that require
substantially more computational resources. More analyses further validate the
efficacy of our anchor construction strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation
  through Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Mukta Maddipatla, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)
framework composed of specialized agents for subtasks such as planning,
searching, reasoning, and coordination. Our system uses a self-training
paradigm with reward-guided trajectory sampling to optimize inter-agent
collaboration and enhance response generation. Evaluated on DataMorgana-derived
datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms
conventional RAG baselines. We further analyze competition outcomes and
showcase the framework's strengths with case studies, demonstrating its
efficacy for complex, real-world RAG tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing and Evaluating Declarative RAG Pipelines in PyTerrier <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig Macdonald, Jinyuan Fang, Andrew Parry, Zaiqiao Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engines often follow a pipeline architecture, where complex but
effective reranking components are used to refine the results of an initial
retrieval. Retrieval augmented generation (RAG) is an exciting application of
the pipeline architecture, where the final component generates a coherent
answer for the users from the retrieved documents. In this demo paper, we
describe how such RAG pipelines can be formulated in the declarative PyTerrier
architecture, and the advantages of doing so. Our PyTerrier-RAG extension for
PyTerrier provides easy access to standard RAG datasets and evaluation
measures, state-of-the-art LLM readers, and using PyTerrier's unique operator
notation, easy-to-build pipelines. We demonstrate the succinctness of indexing
and RAG pipelines on standard datasets (including Natural Questions) and how to
build on the larger PyTerrier ecosystem with state-of-the-art sparse,
learned-sparse, and dense retrievers, and other neural rankers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 tables, Accepted to SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TaxoAdapt: Aligning <span class="highlight-title">LLM</span>-Based Multidimensional Taxonomy Construction to
  Evolving Research Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Kargupta, Nan Zhang, Yunyi Zhang, Rui Zhang, Prasenjit Mitra, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of scientific fields introduces challenges in organizing
and retrieving scientific literature. While expert-curated taxonomies have
traditionally addressed this need, the process is time-consuming and expensive.
Furthermore, recent automatic taxonomy construction methods either (1)
over-rely on a specific corpus, sacrificing generalizability, or (2) depend
heavily on the general knowledge of large language models (LLMs) contained
within their pre-training datasets, often overlooking the dynamic nature of
evolving scientific domains. Additionally, these approaches fail to account for
the multi-faceted nature of scientific literature, where a single research
paper may contribute to multiple dimensions (e.g., methodology, new tasks,
evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a
framework that dynamically adapts an LLM-generated taxonomy to a given corpus
across multiple dimensions. TaxoAdapt performs iterative hierarchical
classification, expanding both the taxonomy width and depth based on corpus'
topical distribution. We demonstrate its state-of-the-art performance across a
diverse set of computer science conferences over the years to showcase its
ability to structure and capture the evolution of scientific fields. As a
multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more
granularity-preserving and 50.41% more coherent than the most competitive
baselines judged by LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Main Conference. Code available at:
  https://github.com/pkargupta/taxoadapt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond True or False: Retrieval-Augmented Hierarchical Analysis of
  Nuanced Claims 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Kargupta, Runchu Tian, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Claims made by individuals or entities are oftentimes nuanced and cannot be
clearly labeled as entirely "true" or "false" -- as is frequently the case with
scientific and political claims. However, a claim (e.g., "vaccine A is better
than vaccine B") can be dissected into its integral aspects and sub-aspects
(e.g., efficacy, safety, distribution), which are individually easier to
validate. This enables a more comprehensive, structured response that provides
a well-rounded perspective on a given problem while also allowing the reader to
prioritize specific angles of interest within the claim (e.g., safety towards
children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based
framework for automatically constructing a hierarchy of aspects typically
considered when addressing a claim and enriching them with corpus-specific
perspectives. This structure hierarchically partitions an input corpus to
retrieve relevant segments, which assist in discovering new sub-aspects.
Moreover, these segments enable the discovery of varying perspectives towards
an aspect of the claim (e.g., support, neutral, or oppose) and their respective
prevalence (e.g., "how many biomedical papers believe vaccine A is more
transportable than B?"). We apply ClaimSpect to a wide variety of real-world
scientific and political claims featured in our constructed dataset, showcasing
its robustness and accuracy in deconstructing a nuanced claim and representing
perspectives within a corpus. Through real-world case studies and human
evaluation, we validate its effectiveness over multiple baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Main Conference. Code available at:
  https://github.com/pkargupta/claimspect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Matrix Completion with Denoising and Augmented Graph Views
  for Robust <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narges Nemati, Mostafa Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix completion is a widely adopted framework in recommender systems, as
predicting the missing entries in the user-item rating matrix enables a
comprehensive understanding of user preferences. However, current graph neural
network (GNN)-based approaches are highly sensitive to noisy or irrelevant
edges--due to their inherent message-passing mechanisms--and are prone to
overfitting, which limits their generalizability. To overcome these challenges,
we propose a novel method called Matrix Completion using Contrastive Learning
(MCCL). Our approach begins by extracting local neighborhood subgraphs for each
interaction and subsequently generates two distinct graph representations. The
first representation emphasizes denoising by integrating GNN layers with an
attention mechanism, while the second is obtained via a graph variational
autoencoder that aligns the feature distribution with a standard prior. A
mutual learning loss function is employed during training to gradually
harmonize these representations, enabling the model to capture common patterns
and significantly enhance its generalizability. Extensive experiments on
several real-world datasets demonstrate that our approach not only improves the
numerical accuracy of the predicted scores--achieving up to a 0.8% improvement
in RMSE--but also produces superior rankings with improvements of up to 36% in
ranking metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversational Search: From Fundamentals to Frontiers in the <span class="highlight-title">LLM</span> Era <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengran Mo, Chuan Meng, Mohammad Aliannejadi, Jian-Yun Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational search enables multi-turn interactions between users and
systems to fulfill users' complex information needs. During this interaction,
the system should understand the users' search intent within the conversational
context and then return the relevant information through a flexible,
dialogue-based interface. The recent powerful large language models (LLMs) with
capacities of instruction following, content generation, and reasoning, attract
significant attention and advancements, providing new opportunities and
challenges for building up intelligent conversational search systems. This
tutorial aims to introduce the connection between fundamentals and the emerging
topics revolutionized by LLMs in the context of conversational search. It is
designed for students, researchers, and practitioners from both academia and
industry. Participants will gain a comprehensive understanding of both the core
principles and cutting-edge developments driven by LLMs in conversational
search, equipping them with the knowledge needed to contribute to the
development of next-generation conversational search systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Tutorial Track in SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Macro Graph of Experts for Billion-Scale <span class="highlight-title">Multi-Task</span> <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Yao, Zijin Hong, Hao Chen, Yuanchen Bei, Zhiqing Li, Qijie Shen, Zuobin Ying, Huan Gong, Feiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based multi-task learning at billion-scale presents a significant
challenge, as different tasks correspond to distinct billion-scale graphs.
Traditional multi-task learning methods often neglect these graph structures,
relying solely on individual user and item embeddings. However, disregarding
graph structures overlooks substantial potential for improving performance. In
this paper, we introduce the Macro Graph of Expert (MGOE) framework, the first
approach capable of leveraging macro graph embeddings to capture task-specific
macro features while modeling the correlations between task-specific experts.
Specifically, we propose the concept of a Macro Graph Bottom, which, for the
first time, enables multi-task learning models to incorporate graph information
effectively. We design the Macro Prediction Tower to dynamically integrate
macro knowledge across tasks. MGOE has been deployed at scale, powering
multi-task learning for the homepage of a leading billion-scale recommender
system. Extensive offline experiments conducted on three public benchmark
datasets demonstrate its superiority over state-of-the-art multi-task learning
methods, establishing MGOE as a breakthrough in multi-task graph-based
recommendation. Furthermore, online A/B tests confirm the superiority of MGOE
in billion-scale recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHORE: A Long-term User Lifetime Value Prediction Model in Digital Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congde Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital gaming, long-term user lifetime value (LTV) prediction is
essential for monetization strategy, yet presents major challenges due to
delayed payment behavior, sparse early user data, and the presence of
high-value outliers. While existing models typically rely on either short-cycle
observations or strong distributional assumptions, such approaches often
underestimate long-term value or suffer from poor robustness. To address these
issues, we propose SHort-cycle auxiliary with Order-preserving REgression
(SHORE), a novel LTV prediction framework that integrates short-horizon
predictions (e.g., LTV-15 and LTV-30) as auxiliary tasks to enhance long-cycle
targets (e.g., LTV-60). SHORE also introduces a hybrid loss function combining
order-preserving multi-class classification and a dynamic Huber loss to
mitigate the influence of zero-inflation and outlier payment behavior.
Extensive offline and online experiments on real-world datasets demonstrate
that SHORE significantly outperforms existing baselines, achieving a 47.91\%
relative reduction in prediction error in online deployment. These results
highlight SHORE's practical effectiveness and robustness in industrial-scale
LTV prediction for digital games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been removed by arXiv administrators as the
  submitter did not have the right to agree to the license at the time of
  submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for
  Monolingual and <span class="highlight-title">Cross</span>lingual Fact-Checked Claim Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.17272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.17272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youzheng Liu, Jiyan Liu, Xiaoman Xu, Taihang Wang, Yimin Wang, Ye Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning RAG via System 1 or System 2: A <span class="highlight-title">Survey</span> on Reasoning Agentic
  Retrieval-Augmented Generation for Industry Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, Ziyue Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
overcome the knowledge limitations of Large Language Models (LLMs) by
integrating external retrieval with language generation. While early RAG
systems based on static pipelines have shown effectiveness in well-structured
tasks, they struggle in real-world scenarios requiring complex reasoning,
dynamic retrieval, and multi-modal integration. To address these challenges,
the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds
decision-making and adaptive tool use directly into the retrieval process. In
this paper, we present a comprehensive review of Reasoning Agentic RAG methods,
categorizing them into two primary systems: predefined reasoning, which follows
fixed modular pipelines to boost reasoning, and agentic reasoning, where the
model autonomously orchestrates tool interaction during inference. We analyze
representative techniques under both paradigms, covering architectural design,
reasoning strategies, and tool coordination. Finally, we discuss key research
challenges and propose future directions to advance the flexibility,
robustness, and applicability of reasoning agentic RAG systems. Our collection
of the relevant research has been organized into a
https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous
  Document Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Yu, Pu Jian, Chong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has demonstrated considerable
effectiveness in open-domain question answering. However, when applied to
heterogeneous documents, comprising both textual and tabular components,
existing RAG approaches exhibit critical limitations. The prevailing practice
of flattening tables and chunking strategies disrupts the intrinsic tabular
structure, leads to information loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose
TableRAG, an hybrid framework that unifies textual understanding and complex
manipulations over tabular data. TableRAG iteratively operates in four steps:
context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop
HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous
reasoning capabilities. Experimental results demonstrate that TableRAG
consistently outperforms existing baselines on both public datasets and our
HeteQA, establishing a new state-of-the-art for heterogeneous document question
answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Codes are available at
  https://github.com/yxh-y/TableRAG/tree/main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightKG: Efficient Knowledge-Aware <span class="highlight-title">Recommendation</span>s with Simplified GNN
  Architecture <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhui Li, Dongxia Wang, Zhu Sun, Haonan Zhang, Huizhong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Graph Neural Networks (GNNs) have become the dominant approach for
Knowledge Graph-aware Recommender Systems (KGRSs) due to their proven
effectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL)
has been incorporated to address the sparity issue, leading to longer training
time. However, through extensive experiments, we reveal that: (1)compared to
other KGRSs, the existing GNN-based KGRSs fail to keep their superior
performance under sparse interactions even with SSL. (2) More complex models
tend to perform worse in sparse interaction scenarios and complex mechanisms,
like attention mechanism, can be detrimental as they often increase learning
difficulty. Inspired by these findings, we propose LightKG, a simple yet
powerful GNN-based KGRS to address sparsity issues. LightKG includes a
simplified GNN layer that encodes directed relations as scalar pairs rather
than dense embeddings and employs a linear aggregation framework, greatly
reducing the complexity of GNNs. Additionally, LightKG incorporates an
efficient contrastive layer to implement SSL. It directly minimizes the node
similarity in original graph, avoiding the time-consuming subgraph generation
and comparison required in previous SSL methods. Experiments on four benchmark
datasets show that LightKG outperforms 12 competitive KGRSs in both sparse and
dense scenarios while significantly reducing training time. Specifically, it
surpasses the best baselines by an average of 5.8\% in recommendation accuracy
and saves 84.3\% of training time compared to KGRSs with SSL. Our code is
available at https://github.com/1371149/LightKG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Datasets, Metrics and Models in Keyphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Boudin, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyphrase generation refers to the task of producing a set of words or
phrases that summarises the content of a document. Continuous efforts have been
dedicated to this task over the past few years, spreading across multiple lines
of research, such as model architectures, data resources, and use-case
scenarios. Yet, the current state of keyphrase generation remains unknown as
there has been no attempt to review and analyse previous work. In this paper,
we bridge this gap by presenting an analysis of over 50 research papers on
keyphrase generation, offering a comprehensive overview of recent progress,
limitations, and open challenges. Our findings highlight several critical
issues in current evaluation practices, such as the concerning similarity among
commonly-used benchmark datasets and inconsistencies in metric calculations
leading to overestimated performances. Additionally, we address the limited
availability of pre-trained models by releasing a strong PLM-based model for
keyphrase generation as an effort to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GEM^2 paper @ ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Adaptive Graph Neural Networks for Next POI <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Lei, Limin Shen, Zhu Sun, Tiantian He, Yew-Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next Point-of-Interest (POI) recommendation is a critical task in
location-based services, aiming to predict users' next visits based on their
check-in histories. While many existing methods leverage Graph Neural Networks
(GNNs) to incorporate collaborative information and improve recommendation
accuracy, most of them model each type of context using separate graphs,
treating different factors in isolation. This limits their ability to model the
co-influence of multiple contextual factors on user transitions during message
propagation, resulting in suboptimal attention weights and recommendation
performance. Furthermore, they often prioritize sequential components as the
primary predictor, potentially undermining the semantic and structural
information encoded in the POI embeddings learned by GNNs. To address these
limitations, we propose a Context-Adaptive Graph Neural Networks (CAGNN) for
next POI recommendation, which dynamically adjusts attention weights using
edge-specific contextual factors and enables mutual enhancement between
graph-based and sequential components. Specifically, CAGNN introduces (1) a
context-adaptive attention mechanism that jointly incorporates different types
of contextual factors into the attention computation during graph propagation,
enabling the model to dynamically capture collaborative and context-dependent
transition patterns; (2) a graph-sequential mutual enhancement module, which
aligns the outputs of the graph- and sequential-based modules via the KL
divergence, enabling mutual enhancement of both components. Experimental
results on three real-world datasets demonstrate that CAGNN consistently
outperforms state-of-the-art methods. Meanwhile, theoretical guarantees are
provided that our context-adaptive attention mechanism improves the
expressiveness of POI representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Bias in Synthetic Data for Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein A. Rahmani, Varsha Ramineni, Nick Craswell, Bhaskar Mitra, Emine Yilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test collections are crucial for evaluating Information Retrieval (IR)
systems. Creating a diverse set of user queries for these collections can be
challenging, and obtaining relevance judgments, which indicate how well
retrieved documents match a query, is often costly and resource-intensive.
Recently, generating synthetic datasets using Large Language Models (LLMs) has
gained attention in various applications. While previous work has used LLMs to
generate synthetic queries or documents to improve ranking models, using LLMs
to create synthetic test collections is still relatively unexplored. Previous
work~\cite{rahmani2024synthetic} showed that synthetic test collections have
the potential to be used for system evaluation, however, more analysis is
needed to validate this claim. In this paper, we thoroughly investigate the
reliability of synthetic test collections constructed using LLMs, where LLMs
are used to generate synthetic queries, labels, or both. In particular, we
examine the potential biases that might occur when such test collections are
used for evaluation. We first empirically show the presence of such bias in
evaluation results and analyse the effects it might have on system evaluation.
We further validate the presence of such bias using a linear mixed-effects
model. Our analysis shows that while the effect of bias present in evaluation
results obtained using synthetic test collections could be significant, for
e.g.~computing absolute system performance, its effect may not be as
significant in comparing relative system performance. Codes and data are
available at: https://github.com/rahmanidashti/BiasSyntheticData.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Models in <span class="highlight-title">Recommendation</span> Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Ruen Wei, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems remain an essential topic due to its wide application in
various domains and the business potential behind them. Given the great
generation capability exhibited by diffusion models in computer vision
recently, many recommender systems have adopted diffusion models and found
improvements in performance for various tasks. Research in this domain has been
growing rapidly and calling for a systematic survey. In this survey paper, we
present and propose a taxonomy in recommender systems that utilize diffusion
models. Distinct from a prior survey paper that categorizes based on the role
of the diffusion model, we categorize based on the recommendation task at hand.
The decision originates from the rationale that after all, the adoption of
diffusion models is to enhance the recommendation performance, not vice versa:
adapting the recommendation task to enable diffusion models. Nonetheless, we
offer a unique perspective for diffusion models in recommender systems
complementary to existing surveys. We present the foundational algorithms in
diffusion models and their applications in recommender systems to summarize the
rapid development in this field. Finally, we discuss open research directions
to prepare and encourage further efforts to advance the field. We compile the
relevant papers in a public GitHub repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rec<span class="highlight-title">GPT</span>: A Foundation Model for Sequential <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangqin Jiang, Xubin Ren, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses a fundamental barrier in recommender systems: the
inability to generalize across domains without extensive retraining.
Traditional ID-based approaches fail entirely in cold-start and cross-domain
scenarios where new users or items lack sufficient interaction history.
Inspired by foundation models' cross-domain success, we develop a foundation
model for sequential recommendation that achieves genuine zero-shot
generalization capabilities. Our approach fundamentally departs from existing
ID-based methods by deriving item representations exclusively from textual
features. This enables immediate embedding of any new item without model
retraining. We introduce unified item tokenization with Finite Scalar
Quantization that transforms heterogeneous textual descriptions into
standardized discrete tokens. This eliminates domain barriers that plague
existing systems. Additionally, the framework features hybrid
bidirectional-causal attention that captures both intra-item token coherence
and inter-item sequential dependencies. An efficient catalog-aware beam search
decoder enables real-time token-to-item mapping. Unlike conventional approaches
confined to their training domains, RecGPT naturally bridges diverse
recommendation contexts through its domain-invariant tokenization mechanism.
Comprehensive evaluations across six datasets and industrial scenarios
demonstrate consistent performance advantages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-Cure: <span class="highlight-title">LLM</span>-based Competitor User <span class="highlight-title">Review</span> Analysis for Feature
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maram Assi, Safwat Hassan, Ying Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of the mobile app market underscores the importance of
constant innovation and rapid response to user demands. As user satisfaction is
paramount to the success of a mobile application (app), developers typically
rely on user reviews, which represent user feedback that includes ratings and
comments to identify areas for improvement. However, the sheer volume of user
reviews poses challenges in manual analysis, necessitating automated
approaches. Existing automated approaches either analyze only the target apps
reviews, neglecting the comparison of similar features to competitors or fail
to provide suggestions for feature enhancement. To address these gaps, we
propose a Large Language Model (LLM)-based Competitive User Review Analysis for
Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically
generate suggestion s for mobile app feature improvements. More specifically,
LLM-Cure identifies and categorizes features within reviews by applying LLMs.
When provided with a complaint in a user review, LLM-Cure curates highly rated
(4 and 5 stars) reviews in competing apps related to the complaint and proposes
potential improvements tailored to the target application. We evaluate LLM-Cure
on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates
that LLM-Cure significantly outperforms the state-of-the-art approaches in
assigning features to reviews by up to 13% in F1-score, up to 16% in recall and
up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to
provide suggestions for resolving user complaints. We verify the suggestions
using the release notes that reflect the changes of features in the target
mobile app. LLM-Cure achieves a promising average of 73% of the implementation
of the provided suggestions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Causal Network Discovery of Alzheimer Disease Biomarkers
  via Scientific Literature-based Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Zhou, Liangjie Huang, Pinyang Cheng, Wenpen Yin, Rui Zhang, Wenrui Hao, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The causal relationships between biomarkers are essential for disease
diagnosis and medical treatment planning. One notable application is
Alzheimer's disease (AD) diagnosis, where certain biomarkers may influence the
presence of others, enabling early detection, precise disease staging, targeted
treatments, and improved monitoring of disease progression. However,
understanding these causal relationships is complex and requires extensive
research. Constructing a comprehensive causal network of biomarkers demands
significant effort from human experts, who must analyze a vast number of
research papers, and have bias in understanding diseases' biomarkers and their
relation. This raises an important question: Can advanced large language models
(LLMs), such as those utilizing retrieval-augmented generation (RAG), assist in
building causal networks of biomarkers for further medical analysis? To explore
this, we collected 200 AD-related research papers published over the past 25
years and then integrated scientific literature with RAG to extract AD
biomarkers and generate causal relations among them. Given the high-risk nature
of the medical diagnosis, we applied uncertainty estimation to assess the
reliability of the generated causal edges and examined the faithfulness and
scientificness of LLM reasoning using both automatic and human evaluation. We
find that RAG enhances the ability of LLMs to generate more accurate causal
networks from scientific papers. However, the overall performance of LLMs in
identifying causal relations of AD biomarkers is still limited. We hope this
study will inspire further foundational research on AI-driven analysis of AD
biomarkers causal network discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metadata practices for simulation workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Villamar, Matthias Kelbling, Heather L. More, Michael Denker, Tom Tetzlaff, Johanna Senk, Stephan Thober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer simulations are an essential pillar of knowledge generation in
science. Exploring, understanding, reproducing, and sharing the results of
simulations relies on tracking and organizing the metadata describing the
numerical experiments. The models used to understand real-world systems, and
the computational machinery required to simulate them, are typically complex,
and produce large amounts of heterogeneous metadata. Here, we present general
practices for acquiring and handling metadata that are agnostic to software and
hardware, and highly flexible for the user. These consist of two steps: 1)
recording and storing raw metadata, and 2) selecting and structuring metadata.
As a proof of concept, we develop the Archivist, a Python tool to help with the
second step, and use it to apply our practices to distinct high-performance
computing use cases from neuroscience and hydrology. Our practices and the
Archivist can readily be applied to existing workflows without the need for
substantial restructuring. They support sustainable numerical workflows,
fostering replicability, reproducibility, data exploration, and data sharing in
simulation-based research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context Is Not Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04907v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04907v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Pan, Mary-Anne Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominant way of judging Large Language Models (LLMs) has been to ask how
well they can recall explicit facts from very long inputs. While today's best
models achieve near perfect recall, this masks a harder skill: performing
multi-step reasoning and tracking intermediate state that never appears
verbatim. We introduce Verbose ListOps (VLO), a benchmark that embeds
deterministic ListOps computations inside narrative camouflage and, crucially,
allows step-level evaluation of every intermediate result. Experiments show
that models which solve raw ListOps with approximately 100% accuracy collapse
on VLO after only 10,000 tokens. By exposing where a model's reasoning chain
first diverges, VLO moves assessment beyond sheer context length and toward
genuine comprehension. VLO's generation pipeline is task-agnostic: it can weave
any deterministically verifiable reasoning schema -- arithmetic, symbolic,
abductive, inductive or defeasible -- into narrative form. This makes VLO a
reusable test-bed for the next wave of reasoning-centric model designs, not
merely those with step-explicit scaffolds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures, 4 tables; under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation
  Learning <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17811v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17811v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudhan Badrinath, Alex Yang, Kousik Rajesh, Prabhat Agarwal, Jaewon Yang, Haoyu Chen, Jiajing Xu, Charles Rosenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning, a task of learning latent vectors to represent
entities, is a key task in improving search and recommender systems in web
applications. Various representation learning methods have been developed,
including graph-based approaches for relationships among entities,
sequence-based methods for capturing the temporal evolution of user activities,
and content-based models for leveraging text and visual content. However, the
development of a unifying framework that integrates these diverse techniques to
support multiple applications remains a significant challenge.
  This paper presents OmniSage, a large-scale representation framework that
learns universal representations for a variety of applications at Pinterest.
OmniSage integrates graph neural networks with content-based models and user
sequence models by employing multiple contrastive learning tasks to effectively
process graph data, user sequence data, and content signals. To support the
training and inference of OmniSage, we developed an efficient infrastructure
capable of supporting Pinterest graphs with billions of nodes. The universal
representations generated by OmniSage have significantly enhanced user
experiences on Pinterest, leading to an approximate 2.5% increase in sitewide
repins (saves) across five applications. This paper highlights the impact of
unifying representation learning methods, and we make the model code publicly
available at https://github.com/pinterest/atg-research/tree/main/omnisage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of KDD 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jelly: a fast and convenient RDF serialization format 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Sowinski, Karolina Bogacka, Anastasiya Danilenka, Nikita Kozlov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing RDF serialization formats such as Turtle, N-Triples, and JSON-LD are
widely used for communication and storage in knowledge graph and Semantic Web
applications. However, they suffer from limitations in performance, compression
ratio, and lack of native support for RDF streams. To address these
shortcomings, we introduce Jelly, a fast and convenient binary serialization
format for RDF data that supports both batch and streaming use cases. Jelly is
designed to maximize serialization throughput, reduce file size with
lightweight streaming compression, and minimize compute resource usage. Built
on Protocol Buffers, Jelly is easy to integrate with modern programming
languages and RDF libraries. To maximize reusability, Jelly has an open
protocol specification, open-source implementations in Java and Python
integrated with popular RDF libraries, and a versatile command-line tool. To
illustrate its usefulness, we outline concrete use cases where Jelly can
provide tangible benefits. By combining practical usability with
state-of-the-art efficiency, Jelly is an important contribution to the Semantic
Web tool stack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OxO2 -- A SSSOM mapping browser for logically sound <span class="highlight-title">cross</span>walks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henriette Harmse, Haider Iqbal, Helen Parkinson, James McLaughlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EMBL-EBI created OxO to enable users to map between datasets that are
annotated with different ontologies. Mappings identified by the first version
of OxO were not necessarily logically sound, lacked important provenance
information such as author and reviewer, and could timeout or crash for certain
requests. In this paper we introduce OxO2 to address these concerns. Provenance
is addressed by implementing SSSOM, a mapping standard that defines provenance
for mappings. SSSOM defines the conditions under which logical sound mappings
can be derived and is implemented in OxO2 using Nemo, a Datalog rule engine. To
ensure reasoning is performant and memory efficient, Nemo implements a number
of strategies that ensures OxO2 will be stable for all requests. Due to these
changes, OxO2 users will be able to integrate between disparate datasets with
greater confidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures and 2 tables. Also submitted to FOIS
  Demonstration track and awaiting feedback</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VSAG: An Optimized Search Framework for Graph-based Approximate Nearest
  Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor search (ANNS) is a fundamental problem in vector
databases and AI infrastructures. Recent graph-based ANNS algorithms have
achieved high search accuracy with practical efficiency. Despite the
advancements, these algorithms still face performance bottlenecks in
production, due to the random memory access patterns of graph-based search and
the high computational overheads of vector distance. In addition, the
performance of a graph-based ANNS algorithm is highly sensitive to parameters,
while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning
requires repeatedly re-building the index.
  This paper introduces VSAG, an open-source framework that aims to enhance the
in production performance of graph-based ANNS algorithms. VSAG has been
deployed at scale in the services of Ant Group, and it incorporates three key
optimizations: (i) efficient memory access: it reduces L3 cache misses with
pre-fetching and cache-friendly vector organization; (ii) automated parameter
tuning: it automatically selects performance-optimal parameters without
requiring index rebuilding; (iii) efficient distance computation: it leverages
modern hardware, scalar quantization, and smartly switches to low-precision
representation to dramatically reduce the distance computation costs. We
evaluate VSAG on real-world datasets. The experimental results show that VSAG
achieves the state-of-the-art performance and provides up to 4x speedup over
HNSWlib (an industry-standard library) while ensuring the same accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the report of open-source library VSAG
  (https://github.com/antgroup/vsag)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-11T00:00:00Z">2025-06-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A quantum semantic framework for natural language processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Agostino, Quan Le Thien, Molly Apsel, Denizhan Pak, Elina Lesyk, Ashabari Majumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete Scale-invariant Metric Learning for Efficient Collaborative
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhang, Li Deng, Lixin Duan, Sami Azam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning has attracted extensive interest for its ability to provide
personalized recommendations based on the importance of observed user-item
interactions. Current metric learning methods aim to push negative items away
from the corresponding users and positive items by an absolute geometrical
distance margin. However, items may come from imbalanced categories with
different intra-class variations. Thus, the absolute distance margin may not be
ideal for estimating the difference between user preferences over imbalanced
items. To this end, we propose a new method, named discrete scale-invariant
metric learning (DSIML), by adding binary constraints to users and items, which
maps users and items into binary codes of a shared Hamming subspace to speed up
the online recommendation. Specifically, we firstly propose a scale-invariant
margin based on angles at the negative item points in the shared Hamming
subspace. Then, we derive a scale-invariant triple hinge loss based on the
margin. To capture more preference difference information, we integrate a
pairwise ranking loss into the scale-invariant loss in the proposed model. Due
to the difficulty of directly optimizing the mixed integer optimization problem
formulated with \textit{log-sum-exp} functions, we seek to optimize its
variational quadratic upper bound and learn hash codes with an alternating
optimization strategy. Experiments on benchmark datasets clearly show that our
proposed method is superior to competitive metric learning and hashing-based
baselines for recommender systems. The implementation code is available at
https://github.com/AnonyFeb/dsml.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digitization of Document and Information Extraction using OCR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasha Sinha, Rekha B S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving accurate details from documents is a crucial task, especially when
handling a combination of scanned images and native digital formats. This
document presents a combined framework for text extraction that merges Optical
Character Recognition (OCR) techniques with Large Language Models (LLMs) to
deliver structured outputs enriched by contextual understanding and confidence
indicators. Scanned files are processed using OCR engines, while digital files
are interpreted through layout-aware libraries. The extracted raw text is
subsequently analyzed by an LLM to identify key-value pairs and resolve
ambiguities. A comparative analysis of different OCR tools is presented to
evaluate their effectiveness concerning accuracy, layout recognition, and
processing speed. The approach demonstrates significant improvements over
traditional rule-based and template-based methods, offering enhanced
flexibility and semantic precision across different document categories
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjun Yao, Haoxuan Li, Zhiqiang Shen, Pan Li, Tongliang Liu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 28 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data
  Augmentation Strategies for Knowledge Graph Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiujun Zhou, Pingjian Zhang, Deyou Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Samantha Zhan, Crystina Zhang, Shengyao Zhuang, Xueguang Ma, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective video retrieval remains challenging due to the complexity of
integrating visual, auditory, and textual modalities. In this paper, we explore
unified retrieval methods using OmniEmbed, a powerful multimodal embedding
model from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task.
Evaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates
unified embeddings for text, images, audio, and video, enabling robust
multimodal retrieval. By finetuning OmniEmbed with the combined multimodal
data--visual frames, audio tracks, and textual descriptions provided in
MultiVENT 2.0, we achieve substantial improvements in complex, multilingual
video retrieval tasks. Our submission achieved the highest score on the MAGMaR
shared task leaderboard among public submissions as of May 20th, 2025,
highlighting the practical effectiveness of our unified multimodal retrieval
approach. Model checkpoint in this work is opensourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Anonymous User Interaction Relationships and Prediction of
  Advertising Feedback Based on Graph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Dai, Haoyang Feng, Yuan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While online advertising is highly dependent on implicit interaction networks
of anonymous users for engagement inference, and for the selection and
optimization of delivery strategies, existing graph models seldom can capture
the multi-scale temporal, semantic and higher-order dependency features of
these interaction networks, thus it's hard to describe the complicated patterns
of the anonymous behavior. In this paper, we propose Decoupled
Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main
contributions. Above all, we introduce temporal edge decomposition, which
divides each interaction into three types of channels: short-term burst,
diurnal cycle and long-range memory, and conducts feature extraction using the
convolution kernel of parallel dilated residuals; Furthermore, our model builds
a hierarchical heterogeneous aggregation, where user-user, user-advertisement,
advertisement-advertisement subgraphs are combined through the meta-path
conditional Transformer encoder, where the noise structure is dynamically
tamped down via the synergy of cross-channel self-attention and gating
relationship selector. Thirdly, the contrast regularity of feedback perception
is formulated, the consistency of various time slices is maximized, the entropy
of control exposure information with dual-view target is maximized, the global
prototype of dual-momentum queue distillation is presented, and the strategy
gradient layer with light weight is combined with delaying transformation
signal to fine-tune the node representation for benefit-oriented. The AUC of
DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in
comparison with the best baseline model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Driven Data Generation and a Novel Soft Metric for Evaluating
  Text-to-SQL in Aviation MRO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Sutanto, Jonathan Kenrick, Max Lorenz, Joan Santoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Large Language Models (LLMs) to text-to-SQL tasks promises
to democratize data access, particularly in critical industries like aviation
Maintenance, Repair, and Operation (MRO). However, progress is hindered by two
key challenges: the rigidity of conventional evaluation metrics such as
execution accuracy, which offer coarse, binary feedback, and the scarcity of
domain-specific evaluation datasets. This paper addresses these gaps. To enable
more nuanced assessment, we introduce a novel F1-score-based 'soft' metric that
quantifies the informational overlap between generated and ground-truth SQL
results. To address data scarcity, we propose an LLM-driven pipeline that
synthesizes realistic question-SQL pairs from database schemas. We demonstrate
our contributions through an empirical evaluation on an authentic MRO database.
Our experiments show that the proposed soft metric provides more insightful
performance analysis than strict accuracy, and our data generation technique is
effective in creating a domain-specific benchmark. Together, these
contributions offer a robust framework for evaluating and advancing text-to-SQL
systems in specialized environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Sequence: Do <span class="highlight-title">Large Language Model</span>s Understand Sequential
  <span class="highlight-title">Recommendation</span>? <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently emerged as promising tools for
recommendation thanks to their advanced textual understanding ability and
context-awareness. Despite the current practice of training and evaluating
LLM-based recommendation (LLM4Rec) models under a sequential recommendation
scenario, we found that whether these models understand the sequential
information inherent in users' item interaction sequences has been largely
overlooked. In this paper, we first demonstrate through a series of experiments
that existing LLM4Rec models do not fully capture sequential information both
during training and inference. Then, we propose a simple yet effective
LLM-based sequential recommender, called LLM-SRec, a method that enhances the
integration of sequential information into LLMs by distilling the user
representations extracted from a pre-trained CF-SRec model into LLMs. Our
extensive experiments show that LLM-SRec enhances LLMs' ability to understand
users' item interaction sequences, ultimately leading to improved
recommendation performance. Furthermore, unlike existing LLM4Rec models that
require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by
training only a few lightweight MLPs, highlighting its practicality in
real-world applications. Our code is available at
https://github.com/Sein-Kim/LLM-SRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented
  Generation with a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezzeldin Shereen, Dan Ristea, Shae McFadden, Burak Hasircioglu, Vasilios Mavroudis, Chris Hicks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal retrieval augmented generation (M-RAG) is instrumental for
inhibiting hallucinations in large multi-modal models (LMMs) through the use of
a factual knowledge base (KB). However, M-RAG introduces new attack vectors for
adversaries that aim to disrupt the system by injecting malicious entries into
the KB. In this paper, we present the first poisoning attack against M-RAG
targeting visual document retrieval applications where the KB contains images
of document pages. We propose two attacks, each of which require injecting only
a single adversarial image into the KB. Firstly, we propose a universal attack
that, for any potential user query, influences the response to cause a
denial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted
attack against one or a group of user queries, with the goal of spreading
targeted misinformation. For both attacks, we use a multi-objective
gradient-based adversarial approach to craft the injected image while
optimizing for both retrieval and generation. We evaluate our attacks against
several visual document retrieval datasets, a diverse set of state-of-the-art
retrievers (embedding models) and generators (LMMs), demonstrating the attack
effectiveness in both the universal and targeted settings. We additionally
present results including commonly used defenses, various attack
hyper-parameter settings, ablations, and attack transferability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Milgram's experiment in the knowledge space: Individual navigation
  strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manran Zhu, János Kertész
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data deluge characteristic for our times has led to information overload,
posing a significant challenge to effectively finding our way through the
digital landscape. Addressing this issue requires an in-depth understanding of
how we navigate through the abundance of information. Previous research has
discovered multiple patterns in how individuals navigate in the geographic,
social, and information spaces, yet individual differences in strategies for
navigation in the knowledge space has remained largely unexplored. To bridge
the gap, we conducted an online experiment where participants played a
navigation game on Wikipedia and completed questionnaires about their personal
information. Utilizing the hierarchical structure of the English Wikipedia and
a graph embedding trained on it, we identified two navigation strategies and
found that there are significant individual differences in the choices of them.
Older, white and female participants tend to adopt a proximity-driven strategy,
while younger participants prefer a hub-driven strategy. Our study connects
social navigation to knowledge navigation: individuals' differing tendencies to
use geographical and occupational information about the target person to
navigate in the social space can be understood as different choices between the
hub-driven and proximity-driven strategies in the knowledge space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17834v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17834v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokai Zhang, Shengtao Zhang, Zijian Cai, Heng Wang, Ruixuan Zhu, Zinan Zeng, Minnan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoilers in movie reviews are important on platforms like IMDb and Rotten
Tomatoes, offering benefits and drawbacks. They can guide some viewers' choices
but also affect those who prefer no plot details in advance, making effective
spoiler detection essential. Existing spoiler detection methods mainly analyze
review text, often overlooking the impact of movie genres and user bias,
limiting their effectiveness. To address this, we analyze movie review data,
finding genre-specific variations in spoiler rates and identifying that certain
users are more likely to post spoilers. Based on these findings, we introduce a
new spoiler detection framework called GUSD (The code is available at
https://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler
Detection), which incorporates genre-specific data and user behavior bias. User
bias is calculated through dynamic graph modeling of review history.
Additionally, the R2GFormer module combines RetGAT (Retentive Graph Attention
Network) for graph information and GenreFormer for genre-specific aggregation.
The GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to
specialized experts based on genre. Extensive testing on benchmark datasets
shows that GUSD achieves state-of-the-art results. This approach advances
spoiler detection by addressing genre and user-specific patterns, enhancing
user experience on movie review platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Multi-player Bandits for Explainable <span class="highlight-title">Recommendation</span>
  Diversification <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansen Zhang, Bowei He, Xiaokun Zhang, Haolun Wu, Zexu Sun, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing recommender systems tend to prioritize items closely aligned with
users' historical interactions, inevitably trapping users in the dilemma of
``filter bubble''. Recent efforts are dedicated to improving the diversity of
recommendations. However, they mainly suffer from two major issues: 1) a lack
of explainability, making it difficult for the system designers to understand
how diverse recommendations are generated, and 2) limitations to specific
metrics, with difficulty in enhancing non-differentiable diversity metrics. To
this end, we propose a \textbf{C}ounterfactual \textbf{M}ulti-player
\textbf{B}andits (CMB) method to deliver explainable recommendation
diversification across a wide range of diversity metrics. Leveraging a
counterfactual framework, our method identifies the factors influencing
diversity outcomes. Meanwhile, we adopt the multi-player bandits to optimize
the counterfactual optimization objective, making it adaptable to both
differentiable and non-differentiable diversity metrics. Extensive experiments
conducted on three real-world datasets demonstrate the applicability,
effectiveness, and explainability of the proposed CMB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECML PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causality-Inspired Fair Representation Learning for Multimodal
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17373v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17373v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Chen, Li Chen, Yongxin Ni, Yuhan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multimodal recommendations (MMR) have gained increasing attention
for alleviating the data sparsity problem of traditional recommender systems by
incorporating modality-based representations. Although MMR exhibits notable
improvement in recommendation accuracy, we empirically validate that an
increase in the quantity or variety of modalities leads to a higher degree of
users' sensitive information leakage due to entangled causal relationships,
risking fair representation learning. On the other hand, existing fair
representation learning approaches are mostly based on the assumption that
sensitive information is solely leaked from users' interaction data and do not
explicitly model the causal relationships introduced by multimodal data, which
limits their applicability in multimodal scenarios. To address this limitation,
we propose a novel fair multimodal recommendation approach (dubbed FMMRec)
through causality-inspired fairness-oriented modal disentanglement and
relation-aware fairness learning. Particularly, we disentangle biased and
filtered modal embeddings inspired by causal inference techniques, enabling the
mining of modality-based unfair and fair user-user relations, thereby enhancing
the fairness and informativeness of user representations. By addressing the
causal effects of sensitive attributes on user preferences, our approach aims
to achieve counterfactual fairness in multimodal recommendations. Experiments
on two public datasets demonstrate the superiority of our FMMRec relative to
the state-of-the-art baselines. Our source code is available at
https://github.com/WeixinChen98/FMMRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ACM Transactions on Information Systems (TOIS), 2025 (just
  accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Knowledge Organization Systems of Research Fields: Resources
  and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Salatino, Tanay Aggarwal, Andrea Mannocci, Francesco Osborne, Enrico Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Organization Systems (KOSs), such as term lists, thesauri,
taxonomies, and ontologies, play a fundamental role in categorising, managing,
and retrieving information. In the academic domain, KOSs are often adopted for
representing research areas and their relationships, primarily aiming to
classify research articles, academic courses, patents, books, scientific
venues, domain experts, grants, software, experiment materials, and several
other relevant products and agents. These structured representations of
research areas, widely embraced by many academic fields, have proven effective
in empowering AI-based systems to i) enhance retrievability of relevant
documents, ii) enable advanced analytic solutions to quantify the impact of
academic research, and iii) analyse and forecast research dynamics. This paper
aims to present a comprehensive survey of the current KOS for academic
disciplines. We analysed and compared 45 KOSs according to five main
dimensions: scope, structure, curation, usage, and links to other KOSs. Our
results reveal a very heterogeneous scenario in terms of scope, scale, quality,
and usage, highlighting the need for more integrated solutions for representing
research knowledge across academic fields. We conclude by discussing the main
challenges and the most promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Quantitative Science Studies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span>s for Scholarly Ontology Generation: An Extensive
  Analysis in the Engineering Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Aggarwal, Angelo Salatino, Francesco Osborne, Enrico Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontologies of research topics are crucial for structuring scientific
knowledge, enabling scientists to navigate vast amounts of research, and
forming the backbone of intelligent systems such as search engines and
recommendation systems. However, manual creation of these ontologies is
expensive, slow, and often results in outdated and overly general
representations. As a solution, researchers have been investigating ways to
automate or semi-automate the process of generating these ontologies. This
paper offers a comprehensive analysis of the ability of large language models
(LLMs) to identify semantic relationships between different research topics,
which is a critical step in the development of such ontologies. To this end, we
developed a gold standard based on the IEEE Thesaurus to evaluate the task of
identifying four types of relationships between pairs of topics: broader,
narrower, same-as, and other. Our study evaluates the performance of seventeen
LLMs, which differ in scale, accessibility (open vs. proprietary), and model
type (full vs. quantised), while also assessing four zero-shot reasoning
strategies. Several models have achieved outstanding results, including
Mixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,
0.920, and 0.967, respectively. Furthermore, our findings demonstrate that
smaller, quantised models, when optimised through prompt engineering, can
deliver performance comparable to much larger proprietary models, while
requiring significantly fewer computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Now accepted to Information Processing & Management. this is the
  camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.15629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.15629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsh Maheshwari, Srikanth Tenneti, Alwarappan Nakkiran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as a powerful application of
Large Language Models (LLMs), revolutionizing information search and
consumption. RAG systems combine traditional search capabilities with LLMs to
generate comprehensive answers to user queries, ideally with accurate
citations. However, in our experience of developing a RAG product, LLMs often
struggle with source attribution, aligning with other industry studies
reporting citation accuracy rates of only about 74% for popular generative
search engines. To address this, we present efficient post-processing
algorithms to improve citation accuracy in LLM-generated responses, with
minimal impact on latency and cost. Our approaches cross-check generated
citations against retrieved articles using methods including keyword + semantic
matching, fine tuned model with BERTScore, and a lightweight LLM-based
technique. Our experimental results demonstrate a relative improvement of
15.46% in the overall accuracy metrics of our RAG system. This significant
enhancement potentially enables a shift from our current larger language model
to a relatively smaller model that is approximately 12x more cost-effective and
3x faster in inference time, while maintaining comparable performance. This
research contributes to enhancing the reliability and trustworthiness of
AI-generated content in information retrieval and summarization tasks which is
critical to gain customer trust especially in commercial products.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoE-MLoRA for Multi-<span class="highlight-title">Domain</span> CTR Prediction: Efficient Adaptation with
  Expert Specialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken Yaggel, Eyal German, Aviel Ben Siman Tov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized recommendation systems must adapt to user interactions across
different domains. Traditional approaches like MLoRA apply a single adaptation
per domain but lack flexibility in handling diverse user behaviors. To address
this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is
first trained independently to specialize in its domain before a gating network
is trained to weight their contributions dynamically. We evaluate MoE-MLoRA
across eight CTR models on Movielens and Taobao, showing that it improves
performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)
but offers limited benefits in structured datasets with low domain diversity
and sparsity. Further analysis of the number of experts per domain reveals that
larger ensembles do not always improve performance, indicating the need for
model-aware tuning. Our findings highlight the potential of expert-based
architectures for multi-domain recommendation systems, demonstrating that
task-aware specialization and adaptive gating can enhance predictive accuracy
in complex environments. The implementation and code are available in our
GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World
  Geospatial Reasoning Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18470v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18470v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhou Yu, Riyang Bao, Ruiyu Ning, Jinghong Peng, Gengchen Mai, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering real-world geospatial questions--such as finding restaurants along
a travel route or amenities near a landmark--requires reasoning over both
geographic relationships and semantic user intent. However, existing large
language models (LLMs) lack spatial computing capabilities and access to
up-to-date, ubiquitous real-world geospatial data, while traditional geospatial
systems fall short in interpreting natural language. To bridge this gap, we
introduce Spatial-RAG, a Retrieval-Augmented Generation (RAG) framework
designed for geospatial question answering. Spatial-RAG integrates structured
spatial databases with LLMs via a hybrid spatial retriever that combines sparse
spatial filtering and dense semantic matching. It formulates the answering
process as a multi-objective optimization over spatial and semantic relevance,
identifying Pareto-optimal candidates and dynamically selecting the best
response based on user intent. Experiments across multiple tourism and
map-based QA datasets show that Spatial-RAG significantly improves accuracy,
precision, and ranking performance over strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMREC: <span class="highlight-title">LLM</span> Based <span class="highlight-title">Multi-Modal</span> Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Tian, Jinman Zhao, Zhenkai Wang, Zhicheng Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of recommender systems is growing rapidly due to the
exponential increase in the volume of content generated daily. This surge in
content presents unique challenges for designing effective recommender systems.
Key among these challenges is the need to effectively leverage the vast amounts
of natural language data and images that represent user preferences. This paper
presents a novel approach to enhancing recommender systems by leveraging Large
Language Models (LLMs) and deep learning techniques. The proposed framework
aims to improve the accuracy and relevance of recommendations by incorporating
multi-modal information processing and by the use of unified latent space
representation. The study explores the potential of LLMs to better understand
and utilize natural language data in recommendation contexts, addressing the
limitations of previous methods. The framework efficiently extracts and
integrates text and image information through LLMs, unifying diverse modalities
in a latent space to simplify the learning process for the ranking model.
Experimental results demonstrate the enhanced discriminative power of the model
when utilizing multi-modal information. This research contributes to the
evolving field of recommender systems by showcasing the potential of LLMs and
multi-modal data integration to create more personalized and contextually
relevant recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging <span class="highlight-title">LLM</span>s to Evaluate Usefulness of Document 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhu Wang, Erhan Zhang, Yiqun Chen, Jinghan Xuan, Yucheng Hou, Yitong Xu, Ying Nie, Shuaiqiang Wang, Dawei Yin, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional Cranfield paradigm struggles to effectively capture user
satisfaction due to its weak correlation between relevance and satisfaction,
alongside the high costs of relevance annotation in building test collections.
To tackle these issues, our research explores the potential of leveraging large
language models (LLMs) to generate multilevel usefulness labels for evaluation.
We introduce a new user-centric evaluation framework that integrates users'
search context and behavioral data into LLMs. This framework uses a cascading
judgment structure designed for multilevel usefulness assessments, drawing
inspiration from ordinal regression techniques. Our study demonstrates that
when well-guided with context and behavioral information, LLMs can accurately
evaluate usefulness, allowing our approach to surpass third-party labeling
methods. Furthermore, we conduct ablation studies to investigate the influence
of key components within the framework. We also apply the labels produced by
our method to predict user satisfaction, with real-world experiments indicating
that these labels substantially improve the performance of satisfaction
prediction models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unifying Algorithm for Hierarchical Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Abo Khamis, Jesse Comer, Phokion Kolaitis, Sudeepa Roy, Val Tannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The class of hierarchical queries is known to define the boundary of the
dichotomy between tractability and intractability for the following two
extensively studied problems about self-join free Boolean conjunctive queries
(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic
database; (ii) computing the Shapley value of a fact in a database on which a
SJF-BCQ evaluates to true. Here, we establish that hierarchical queries define
also the boundary of the dichotomy between tractability and intractability for
a different natural algorithmic problem, which we call the "bag-set
maximization" problem. The bag-set maximization problem associated with a
SJF-BCQ $Q$ asks: given a database $\cal D$, find the biggest value that $Q$
takes under bag semantics on a database $\cal D'$ obtained from $\cal D$ by
adding at most $\theta$ facts from another given database $\cal D^r$.
  For non-hierarchical queries, we show that the bag-set maximization problem
is an NP-complete optimization problem. More significantly, for hierarchical
queries, we show that all three aforementioned problems (probabilistic query
evaluation, Shapley value computation, and bag-set maximization) admit a single
unifying polynomial-time algorithm that operates on an abstract algebraic
structure, called a "2-monoid". Each of the three problems requires a different
instantiation of the 2-monoid tailored for the problem at hand.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU Acceleration of SQL Analytics on Compressed Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhou Huang, Krystian Sakowski, Hans Lehnert, Wei Cui, Carlo Curino, Matteo Interlandi, Marius Dumitru, Rathijit Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to
their massive compute parallelism and High Bandwidth Memory (HBM) -- when
datasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU
HBMs remain typically small when compared with lower-bandwidth CPU main memory.
Besides brute-force scaling across many GPUs, current solutions to accelerate
queries on large datasets include leveraging data partitioning and loading
smaller data batches in GPU HBM, and hybrid execution with a connected device
(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of
lower main memory and host-to-device interconnect bandwidths, introduce
additional I/O overheads, or incur higher costs. This is a substantial problem
when trying to scale adoption of GPUs on larger datasets. Data compression can
alleviate this bottleneck, but to avoid paying for costly
decompression/decoding, an ideal solution must include computation primitives
to operate directly on data in compressed form.
  This is the focus of our paper: a set of new methods for running queries
directly on light-weight compressed data using schemes such as Run-Length
Encoding (RLE), index encoding, bit-width reductions, and dictionary encoding.
Our novelty includes operating on multiple RLE columns without decompression,
handling heterogeneous column encodings, and leveraging PyTorch tensor
operations for portability across devices. Experimental evaluations show
speedups of an order of magnitude compared to state-of-the-art commercial
CPU-only analytics systems, for real-world queries on a production dataset that
would not fit into GPU memory uncompressed. This work paves the road for GPU
adoption in a much broader set of use cases, and it is complementary to most
other scale-out or fallback mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Microservices and Real-Time Processing in Retail IT: A <span class="highlight-title">Review</span> of
  Open-Source Toolchains and Deployment Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaditaa Vashisht, Rekha B S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid pace of digital transformation, the retail industry is
increasingly depending on real-time, scalable, and resilient systems to manage
financial transactions, analyze customer behavior, and streamline order
processing. This literature review explores how modern event-driven and
microservices-based architectures, particularly those leveraging Apache Kafka,
Spring Boot, MongoDB, and Kubernetes are transforming retail and financial
systems. By systematically reviewing academic publications, technical white
papers, and industry reports from recent years, this study synthesizes key
themes and implementation strategies. The analysis reveals that technologies
like Kafka and Spring Boot are instrumental in building low-latency,
event-driven applications that support real-time analytics and fraud detection,
while MongoDB, when deployed on Kubernetes, ensures fault tolerance and high
availability in inventory and transaction systems. Kubernetes itself plays a
crucial role in automating deployment and scaling of microservices. These
findings provide valuable insights for industry practitioners aiming to design
scalable infrastructures, identify research opportunities in hybrid deployment
models, and offer educators a foundation to integrate modern system
architectures into professional and technical communication training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HI-SQL: Optimizing Text-to-SQL Systems through Dynamic Hint Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Parab, Zishan Ahmad, Dagnachew Birru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL generation bridges the gap between natural language and
databases, enabling users to query data without requiring SQL expertise. While
large language models (LLMs) have significantly advanced the field, challenges
remain in handling complex queries that involve multi-table joins, nested
conditions, and intricate operations. Existing methods often rely on multi-step
pipelines that incur high computational costs, increase latency, and are prone
to error propagation. To address these limitations, we propose HI-SQL, a
pipeline that incorporates a novel hint generation mechanism utilizing
historical query logs to guide SQL generation. By analyzing prior queries, our
method generates contextual hints that focus on handling the complexities of
multi-table and nested operations. These hints are seamlessly integrated into
the SQL generation process, eliminating the need for costly multi-step
approaches and reducing reliance on human-crafted prompts. Experimental
evaluations on multiple benchmark datasets demonstrate that our approach
significantly improves query accuracy of LLM-generated queries while ensuring
efficiency in terms of LLM calls and latency, offering a robust and practical
solution for enhancing Text-to-SQL systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArcNeural: A <span class="highlight-title">Multi-Modal</span> Database for the Gen-AI Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wu Min, Qiao Yuncong, Yu Tan, Chenghu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ArcNeural introduces a novel multimodal database tailored for the demands of
Generative AI and Large Language Models, enabling efficient management of
diverse data types such as graphs, vectors, and documents. Its storage-compute
separated architecture integrates graph technology, advanced vector indexing,
and transaction processing to support real-time analytics and AI-driven
applications. Key features include a unified storage layer, adaptive edge
collection in MemEngine, and seamless integration of transaction and analytical
processing. Experimental evaluations demonstrate ArcNeural's superior
performance and scalability compared to state-of-the-art systems. This system
bridges structured and unstructured data management, offering a versatile
solution for enterprise-grade AI applications.
  ArcNeural's design addresses the challenges of multimodal data processing,
providing a robust framework for intelligent, data-driven solutions in the Gen
AI era.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Driven Data Generation and a Novel Soft Metric for Evaluating
  Text-to-SQL in Aviation MRO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Sutanto, Jonathan Kenrick, Max Lorenz, Joan Santoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Large Language Models (LLMs) to text-to-SQL tasks promises
to democratize data access, particularly in critical industries like aviation
Maintenance, Repair, and Operation (MRO). However, progress is hindered by two
key challenges: the rigidity of conventional evaluation metrics such as
execution accuracy, which offer coarse, binary feedback, and the scarcity of
domain-specific evaluation datasets. This paper addresses these gaps. To enable
more nuanced assessment, we introduce a novel F1-score-based 'soft' metric that
quantifies the informational overlap between generated and ground-truth SQL
results. To address data scarcity, we propose an LLM-driven pipeline that
synthesizes realistic question-SQL pairs from database schemas. We demonstrate
our contributions through an empirical evaluation on an authentic MRO database.
Our experiments show that the proposed soft metric provides more insightful
performance analysis than strict accuracy, and our data generation technique is
effective in creating a domain-specific benchmark. Together, these
contributions offer a robust framework for evaluating and advancing text-to-SQL
systems in specialized environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding a Fair Scoring Function for Top-$k$ Selection: From Hardness to
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangya Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting a subset of the $k$ "best" items from a dataset of $n$ items, based
on a scoring function, is a key task in decision-making. Given the rise of
automated decision-making software, it is important that the outcome of this
process, called top-$k$ selection, is fair. Here we consider the problem of
identifying a fair linear scoring function for top-$k$ selection. The function
computes a score for each item as a weighted sum of its (numerical) attribute
values, and must ensure that the selected subset includes adequate
representation of a minority or historically disadvantaged group. Existing
algorithms do not scale efficiently, particularly in higher dimensions. Our
hardness analysis shows that in more than two dimensions, no algorithm is
likely to achieve good scalability with respect to dataset size, and the
computational complexity is likely to increase rapidly with dimensionality.
However, the hardness results also provide key insights guiding algorithm
design, leading to our dual-algorithm solution: (1) For small values of $k$,
our hardness analysis reveals a gap in the hardness barrier. By addressing
various engineering challenges, including achieving efficient parallelism, we
turn this potential of efficiency into an optimized algorithm delivering
substantial practical performance gains. (2) For large values of $k$, where the
hardness is robust, we employ a practically efficient algorithm which, despite
being theoretically worse, achieves superior real-world performance.
Experimental evaluations on real-world datasets then explore scenarios where
worst-case behavior does not manifest, identifying areas critical to practical
performance. Our solution achieves speed-ups of up to several orders of
magnitude compared to SOTA, an efficiency made possible through a tight
integration of hardness analysis, algorithm design, practical engineering, and
empirical evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract shortened to meet Arxiv requirements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Long Context All You Need? Leveraging <span class="highlight-title">LLM</span>'s Extended Context for
  NL2SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12372v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12372v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities across
a range of natural language processing tasks. In particular, improvements in
reasoning abilities and the expansion of context windows have opened new
avenues for leveraging these powerful models. NL2SQL is challenging in that the
natural language question is inherently ambiguous, while the SQL generation
requires a precise understanding of complex data schema and semantics. One
approach to this semantic ambiguous problem is to provide more and sufficient
contextual information.
  In this work, we explore the performance and the latency trade-offs of the
extended context window (a.k.a., long context) offered by Google's
state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various
contextual information, including column example values, question and SQL query
pairs, user-provided hints, SQL documentation, and schema. To the best of our
knowledge, this is the first work to study how the extended context window and
extra contextual information can help NL2SQL generation with respect to both
accuracy and latency cost. We show that long context LLMs are robust and do not
get lost in the extended contextual information. Additionally, our long-context
NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve strong
performances on various benchmark datasets without finetuning and expensive
self-consistency based techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Griffin: Towards a Graph-Centric Relational Database Foundation Model <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Griffin, the first foundation model attemptation designed
specifically for Relational Databases (RDBs). Unlike previous smaller models
focused on single RDB tasks, Griffin unifies the data encoder and task decoder
to handle diverse tasks. Additionally, we enhance the architecture by
incorporating a cross-attention module and a novel aggregator. Griffin utilizes
pretraining on both single-table and RDB datasets, employing advanced encoders
for categorical, numerical, and metadata features, along with innovative
components such as cross-attention modules and enhanced message-passing neural
networks (MPNNs) to capture the complexities of relational data. Evaluated on
large-scale, heterogeneous, and temporal graphs extracted from RDBs across
various domains (spanning over 150 million nodes), Griffin demonstrates
superior or comparable performance to individually trained models, excels in
low-data scenarios, and shows strong transferability with similarity and
diversity in pretraining across new datasets and tasks, highlighting its
potential as a universally applicable foundation model for RDBs. Code available
at https://github.com/yanxwb/Griffin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Review</span>ing Uses of Regulatory Compliance Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10362v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10362v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finn Klessascheck, Luise Pufahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organizations need to manage numerous business processes for delivering their
services and products to customers. One important consideration thereby lies in
the adherence to regulations such as laws, guidelines, or industry standards.
In order to monitor adherence of their business processes to regulations -- in
other words, their regulatory compliance -- organizations make use of various
techniques that draw on process execution data of IT systems that support these
processes. Previous research has investigated conformance checking, an
operation of process mining, for the domains in which it is applied, its
operationalization of regulations, the techniques being used, and the
presentation of results produced. However, other techniques for regulatory
compliance monitoring, which we summarize as compliance checking techniques,
have not yet been investigated regarding these aspects in a structural manner.
To this end, this work presents a systematic literature review on uses of
regulatory compliance monitoring of business processes, thereby offering
insights into the various techniques being used, their application and the
results they generate. We highlight commonalities and differences between the
approaches and find that various steps are performed manually; we also provide
further impulses for research on compliance monitoring and its use in practice.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-10T00:00:00Z">2025-06-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Topic Modeling Analysis of Stigma Dimensions, Social, and Related
  Behavioral Circumstances in Clinical Notes Among Patients with HIV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Yiyang Liu, Mattia Prosperi, Krishna Vaddiparti, Robert L Cook, Jiang Bian, Yi Guo, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ThinkQE: Query Expansion via an Evolving Thinking Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Lei, Tao Shen, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Soprano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of online misinformation poses serious threats to democratic
societies. Traditionally, expert fact-checkers verify the truthfulness of
information through investigative processes. However, the volume and immediacy
of online content present major scalability challenges. Crowdsourcing offers a
promising alternative by leveraging non-expert judgments, but it introduces
concerns about bias, accuracy, and interpretability. This thesis investigates
how human intelligence can be harnessed to assess the truthfulness of online
information, focusing on three areas: misinformation assessment, cognitive
biases, and automated fact-checking systems. Through large-scale crowdsourcing
experiments and statistical modeling, it identifies key factors influencing
human judgments and introduces a model for the joint prediction and explanation
of truthfulness. The findings show that non-expert judgments often align with
expert assessments, particularly when factors such as timing and experience are
considered. By deepening our understanding of human judgment and bias in
truthfulness assessment, this thesis contributes to the development of more
transparent, trustworthy, and interpretable systems for combating
misinformation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis, University of Udine, defended May 2023, 458 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Graph Projections for Effective Complementary Product
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro Anghinoni, Pablo Zivic, Jorge Adrian Sanchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Representation Alignment for <span class="highlight-title">Cross</span>-modal Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Xu, Luis A. Leiva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paths to Causality: Finding Informative Subgraphs Within Knowledge
  Graphs for Knowledge-Based Causal Discovery <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuni Susanti, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD 2025 (full research paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging RDF Knowledge Graphs with Graph Neural Networks for
  Semantically-Rich Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Färber, David Lamprecht, Yuni Susanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at DASFAA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XGraphRAG: Interactive Visual Analysis for Graph-based
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Bo Pan, Yingchaojie Feng, Yuwei Wu, Jieyi Chen, Minfeng Zhu, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based Retrieval-Augmented Generation (RAG) has shown great capability
in enhancing Large Language Model (LLM)'s answer with an external knowledge
base. Compared to traditional RAG, it introduces a graph as an intermediate
representation to capture better structured relational knowledge in the corpus,
elevating the precision and comprehensiveness of generation results. However,
developers usually face challenges in analyzing the effectiveness of GraphRAG
on their dataset due to GraphRAG's complex information processing pipeline and
the overwhelming amount of LLM invocations involved during graph construction
and query, which limits GraphRAG interpretability and accessibility. This
research proposes a visual analysis framework that helps RAG developers
identify critical recalls of GraphRAG and trace these recalls through the
GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype
system incorporating a set of interactive visualizations to facilitate users'
analysis process, boosting failure cases collection and improvement
opportunities identification. Our evaluation demonstrates the effectiveness and
usability of our approach. Our work is open-sourced and available at
https://github.com/Gk0Wk/XGraphRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Pacific Visualization Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSRec: Enhancing Repeat-Aware <span class="highlight-title">Recommendation</span> from a Temporal-Sequential
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shigang Quan, Shui Liu, Zhenzhe Zheng, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Repeat consumption, such as repurchasing items and relistening songs, is a
common scenario in daily life. To model repeat consumption, the repeat-aware
recommendation has been proposed to predict which item will be re-interacted
based on the user-item interactions. In this paper, we investigate various
inherent characteristics to enhance the repeat-aware recommendation.
Specifically, we explore these characteristics from two aspects: one is from
the temporal aspect where we consider the time interval relationship in the
user behavior sequence; the other is from the sequential aspect where we
consider the sequential-level relationship in the user behavior sequence. And
our intuition is that both the temporal pattern and sequential pattern will
reflect users' intentions of repeat consumption. By utilizing these two
patterns, a novel model called Temporal and Sequential repeat-aware
Recommendation(TSRec for short) is proposed to enhance repeat-aware
recommendation. TSRec has three main components: 1) User-specific Temporal
Representation Module (UTRM), which encodes and extracts user historical repeat
temporal information. 2)Item-specific Temporal Representation Module (ITRM),
which incorporates item time interval information as side information to
alleviate the data sparsity problem of user repeat behavior sequence. 3)
Sequential Repeat-Aware Module (SRAM), which represents the similarity between
the user's current and the last repeat sequences. Extensive experimental
results on three public benchmarks demonstrate the superiority of TSRec over
state-of-the-art methods. The implementation code is available
https://anonymous.4open.science/r/TSRec-2306/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Context Selection for Long-Context QA: No Tuning, No
  Iteration, Just Adaptive-$k$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihiro Taguchi, Seiji Maekawa, Nikita Bhutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 16 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MERIT: A Merchant Incentive Ranking Model for Hotel Search & Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shigang Quan, Hailong Tan, Shui Liu, Zhenzhe zheng, Ruihao Zhu, Liangyue Li, Quan Lu, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Travel Platforms (OTPs) have been working on improving their hotel
Search & Ranking (S&R) systems that facilitate efficient matching between
consumers and hotels. Existing OTPs focus almost exclusively on improving
platform revenue. In this work, we take a first step in incorporating hotel
merchants' objectives into the design of hotel S&R systems to achieve an
incentive loop: the OTP tilts impressions and better-ranked positions to
merchants with high quality, and in return, the merchants provide better
service to consumers. Three critical design challenges need to be resolved to
achieve this incentive loop: Matthew Effect in the consumer feedback-loop,
unclear relation between hotel quality and performance, and conflicts between
short-term and long-term revenue. To address these challenges, we propose
MERIT, a MERchant IncenTive ranking model, which can simultaneously take the
interests of merchants and consumers into account. We define a new Merchant
Competitiveness Index (MCI) to represent hotel merchant quality and propose a
new Merchant Tower to model the relation between MCI and ranking scores. Also,
we design a monotonic structure for Merchant Tower to provide a clear relation
between hotel quality and performance. Finally, we propose a Multi-objective
Stratified Pairwise Loss, which can mitigate the conflicts between OTP's
short-term and long-term revenue. The offline experiment results indicate that
MERIT outperforms these methods in optimizing the demands of consumers and
merchants. Furthermore, we conduct an online A/B test and obtain an improvement
of 3.02% for the MCI score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NAM: A Normalization Attention Model for Personalized Product Search In
  Fliggy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shui Liu, Mingyuan Tao, Maofei Que, Pan Li, Dong Li, Shenghua Ni, Zhuoran Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized product search provides significant benefits to e-commerce
platforms by extracting more accurate user preferences from historical
behaviors. Previous studies largely focused on the user factors when
personalizing the search query, while ignoring the item perspective, which
leads to the following two challenges that we summarize in this paper: First,
previous approaches relying only on co-occurrence frequency tend to
overestimate the conversion rates for popular items and underestimate those for
long-tail items, resulting in inaccurate item similarities; Second, user
purchasing propensity is highly heterogeneous according to the popularity of
the target item: it is less correlated with the user's historical behavior for
a popular item and more correlated for a long-tail item. To address these
challenges, in this paper we propose NAM, a Normalization Attention Model,
which optimizes ''when to personalize'' by utilizing Inverse Item Frequency
(IIF) and employing a gating mechanism, as well as optimizes ''how to
personalize'' by normalizing the attention mechanism from a global perspective.
Through comprehensive experiments, we demonstrate that our proposed NAM model
significantly outperforms state-of-the-art baseline models. Furthermore, we
conducted an online A/B test at Fliggy, and obtained a significant improvement
of 0.8% over the latest production system in conversion rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Embeddings Should Capture Implicit Semantics, Not Just Surface
  Meaning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source
  Search in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Shi, Yiqing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can face factual limitations when responding to
time-sensitive queries about recent events that arise after their knowledge
thresholds in the training corpus. Existing search-augmented approaches fall
into two categories, each with distinct limitations: multi-agent search
frameworks incur substantial computational overhead by separating search
planning and response synthesis across multiple LLMs, while single-LLM
tool-calling methods restrict themselves to sequential planned, single-query
searches from sole search sources. We present Reasoning-Search (R-Search), a
single-LLM search framework that unifies multi-step planning, multi-source
search execution, and answer synthesis within one coherent inference process.
Innovatively, it structure the output into four explicitly defined components,
including reasoning steps that guide the search process (<think>), a
natural-language directed acyclic graph that represents the search plans with
respect to diverse sources (<search>), retrieved results from executing the
search plans (<result>), and synthesized final answers (<answer>). To enable
effective generation of these structured outputs, we propose a specialized
Reinforcement Fine-Tuning (ReFT) method based on GRPO, together with a
multi-component reward function that optimizes LLM's answer correctness,
structural validity of the generated DAG, and adherence to the defined output
format. Experimental evaluation on FinSearchBench-24, SearchExpertBench-25, and
seven Q and A benchmarks demonstrates that R-Search outperforms
state-of-the-art methods, while achieving substantial efficiency gains through
70% reduction in context token usage and approximately 50% decrease in
execution latency. Code is available at
https://github.com/wentao0429/Reasoning-search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rule-Assisted Attribute Embedding <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Zhao, Michael Bewong, Selasi Kwashie, Junwei Hu, Zaiwen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems often overlook the rich attribute information embedded
in property graphs, limiting their effectiveness. Existing graph convolutional
network (GCN) models either ignore attributes or rely on simplistic <user,
item, attribute> triples, failing to capture deeper semantic structures. We
propose RAE (Rule- Assisted Approach for Attribute Embedding), a novel method
that improves recommendations by mining semantic rules from property graphs to
guide attribute embedding. RAE performs rule-based random walks to generate
enriched attribute representations, which are integrated into GCNs. Experiments
on real-world datasets (BlogCatalog, Flickr) show that RAE outperforms
state-of-the-art baselines by 10.6% on average in Recall@20 and NDCG@20. RAE
also demonstrates greater robustness to sparse data and missing attributes,
highlighting the value of leveraging structured attribute information in
recommendation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML-PKDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization analysis of an unfolding network for analysis-based
  Compressed Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05582v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05582v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Kouni, Yannis Panagakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unfolding networks have shown promising results in the Compressed Sensing
(CS) field. Yet, the investigation of their generalization ability is still in
its infancy. In this paper, we perform a generalization analysis of a
state-of-the-art ADMM-based unfolding network, which jointly learns a decoder
for CS and a sparsifying redundant analysis operator. To this end, we first
impose a structural constraint on the learnable sparsifier, which parametrizes
the network's hypothesis class. For the latter, we estimate its Rademacher
complexity. With this estimate in hand, we deliver generalization error bounds
-- which scale like the square root of the number of layers -- for the examined
network. Finally, the validity of our theory is assessed and numerical
comparisons to a state-of-the-art unfolding network are made, on synthetic and
real-world datasets. Our experimental results demonstrate that our proposed
framework complies with our theoretical findings and outperforms the baseline,
consistently for all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kidist Amde Mekonnen, Yosef Worku Alemneh, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both sparse and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (excl. refs/appendix), 10 figures. Accepted to ACL 2025
  Findings. Kidist and Yosef contributed equally to this work. Public
  resources: https://github.com/kidist-amde/amharic-ir-benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL-based Query Rewriting with Distilled <span class="highlight-title">LLM</span> for online E-Commerce
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy A. Nguyen, Rishi Kesav Mohan, Van Yang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewriting (QR) is a critical technique in e-commerce search, addressing
the lexical gap between user queries and product descriptions to enhance search
performance. Existing QR approaches typically fall into two categories:
discriminative models and generative methods leveraging large language models
(LLMs). Discriminative models often struggle with natural language
understanding and offer limited flexibility in rewriting, while generative
LLMs, despite producing high-quality rewrites, face high inference latency and
cost in online settings. These limitations force offline deployment, making
them vulnerable to issues like information staleness and semantic drift. To
overcome these challenges, we propose a novel hybrid pipeline for QR that
balances efficiency and effectiveness. Our approach combines offline knowledge
distillation to create a lightweight but efficient student model with online
reinforcement learning (RL) to refine query rewriting dynamically using
real-time feedback. A key innovation is the use of LLMs as simulated human
feedback, enabling scalable reward signals and cost-effective evaluation
without manual annotations. Experimental results on Amazon ESCI dataset
demonstrate significant improvements in query relevance, diversity, and
adaptability, as well as positive feedback from the LLM simulation. This work
contributes to advancing LLM capabilities for domain-specific applications,
offering a robust solution for dynamic and complex e-commerce search
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are valued for their strong performance across
various tasks, but they also produce inaccurate or misleading outputs.
Uncertainty Estimation (UE) quantifies the model's confidence and helps users
assess response reliability. However, existing UE methods have not been
thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG),
where the input prompt includes non-parametric knowledge. This paper shows that
current UE methods cannot reliably assess correctness in the RAG setting. We
further propose an axiomatic framework to identify deficiencies in existing
methods and guide the development of improved approaches. Our framework
introduces five constraints that an effective UE method should meet after
incorporating retrieved documents into the LLM's prompt. Experimental results
reveal that no existing UE method fully satisfies all the axioms, explaining
their suboptimal performance in RAG. We further introduce a simple yet
effective calibration function based on our framework, which not only satisfies
more axioms than baseline methods but also improves the correlation between
uncertainty estimates and correctness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are AI Agents interacting with Online Ads? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Stöckl, Joel Nitu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI-driven agents become increasingly integrated into the digital
ecosystem, they reshape how online advertising is perceived and processed.
Particularly in the travel and hotel booking sector, these autonomous systems
influence the effectiveness of traditional advertising formats. While visual
cues and emotional appeals sway human users, AI agents prioritize structured
data such as price, availability, and specifications. This study examines how
different AI agents interact with online advertising, whether they incorporate
ads into their decision-making processes, and which ad formats prove most
effective. We analyze interaction patterns, click behavior, and decision-making
strategies through experiments with multimodal language models such as OpenAI
GPT-4o, Anthropic Claude, and Google Gemini 2.0 Flash. Our findings reveal that
AI agents neither ignore nor systematically avoid advertisements but instead
favor certain features-particularly keywords and structured data. These
insights have significant implications for the future design of advertising
strategies in AI-dominated digital environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Escalation of Source Bias in User, Data, and Recommender
  System Feedback Loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhou, Sunhao Dai, Liang Pang, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are essential for information access, allowing users to
present their content for recommendation. With the rise of large language
models (LLMs), AI-generated content (AIGC), primarily in the form of text, has
become a central part of the content ecosystem. As AIGC becomes increasingly
prevalent, it is important to understand how it affects the performance and
dynamics of recommender systems. To this end, we construct an environment that
incorporates AIGC to explore its short-term impact. The results from popular
sequential recommendation models reveal that AIGC are ranked higher in the
recommender system, reflecting the phenomenon of source bias. To further
explore the long-term impact of AIGC, we introduce a feedback loop with
realistic simulators. The results show that the model's preference for AIGC
increases as the user clicks on AIGC rises and the model trains on simulated
click data. This leads to two issues: In the short term, bias toward AIGC
encourages LLM-based content creation, increasing AIGC content, and causing
unfair traffic distribution. From a long-term perspective, our experiments also
show that when AIGC dominates the content ecosystem after a feedback loop, it
can lead to a decline in recommendation performance. To address these issues,
we propose a debiasing method based on L1-loss optimization to maintain
long-term content ecosystem balance. In a real-world environment with AIGC
generated by mainstream LLMs, our method ensures a balance between AIGC and
human-generated content in the ecosystem. The code and dataset are available at
https://github.com/Yuqi-Zhou/Rec_SourceBias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Length-Induced Embedding Collapse in PLM-based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings from PLM-based models enable a wide range of applications,
yet their performance often degrades on longer texts. In this paper, we
introduce a phenomenon we call Length Collapse, where embeddings of longer
texts tend to cluster together. This clustering results in a distributional
inconsistency between the embeddings of short and long texts. We further
investigate how these differences contribute to the performance decline
observed with longer texts across various downstream tasks. Through a rigorous
theoretical analysis of the self-attention mechanism, which acts as a low-pass
filter in PLM-based models, we demonstrate that as text length increases, the
strength of low-pass filtering intensifies, causing embeddings to retain more
low-frequency components. As a result, input token features become more
similar, leading to clustering and ultimately the collapse of embeddings for
longer texts. To address this issue, we propose a simple method, TempScale,
which mitigates the Length Collapse phenomenon. By narrowing the gap in
low-pass filtering rates between long and short texts, TempScale ensures more
consistent embeddings across different text lengths. This approach leads to
performance improvements of 0.94% on MTEB and 1.10% on LongEmbed, which focuses
specifically on long-context retrieval, providing strong evidence for the
validity of our analysis. The source code is available at
https://github.com/Yuqi-Zhou/Length_Collapse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Open-<span class="highlight-title">Domain</span> Task-Solving Capability of <span class="highlight-title">LLM</span>s via Autonomous
  Tool Integration from GitHub 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17294v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17294v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Lyu, Xin Cong, Heyang Yu, Pan Yang, Yujia Qin, Yining Ye, Yaxi Lu, Zhong Zhang, Yukun Yan, Yankai Lin, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in traditional natural language processing
tasks but struggle with problems that require complex domain-specific
calculations or simulations. While equipping LLMs with external tools to build
LLM-based agents can enhance their capabilities, existing approaches lack the
flexibility to address diverse and ever-evolving user queries in open domains.
Currently, there is also no existing dataset that evaluates LLMs on open-domain
knowledge that requires tools to solve. To this end, we introduce OpenAct
benchmark to evaluate the open-domain task-solving capability, which is built
on human expert consultation and repositories in GitHub. It comprises 339
questions spanning 7 diverse domains that need to be solved with
domain-specific methods. In our experiments, even state-of-the-art LLMs and
LLM-based agents demonstrate unsatisfactory success rates, underscoring the
need for a novel approach. Furthermore, we present OpenAgent, a novel LLM-based
agent system that can tackle evolving queries in open domains through
autonomously integrating specialized tools from GitHub. OpenAgent employs 1) a
hierarchical framework where specialized agents handle specific tasks and can
assign tasks to inferior agents, 2) a bi-level experience learning mechanism to
learn from both humans' and its own experiences to tackle tool flaws.
Experiments demonstrate its superior effectiveness and efficiency, which
significantly outperforms baselines. Our data and code are open-source at
https://github.com/OpenBMB/OpenAct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terabyte-Scale Analytics in the Blink of an Eye 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wu, Wei Cui, Carlo Curino, Matteo Interlandi, Rathijit Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the past two decades, the DB community has devoted substantial research
to take advantage of cheap clusters of machines for distributed data analytics
-- we believe that we are at the beginning of a paradigm shift. The scaling
laws and popularity of AI models lead to the deployment of incredibly powerful
GPU clusters in commercial data centers. Compared to CPU-only solutions, these
clusters deliver impressive improvements in per-node compute, memory bandwidth,
and inter-node interconnect performance. In this paper, we study the problem of
scaling analytical SQL queries on distributed clusters of GPUs, with the stated
goal of establishing an upper bound on the likely performance gains. To do so,
we build a prototype designed to maximize performance by leveraging ML/HPC best
practices, such as group communication primitives for cross-device data
movements. This allows us to conduct thorough performance experimentation to
point our community towards a massive performance opportunity of at least
60$\times$. To make these gains more relatable, before you can blink twice, our
system can run all 22 queries of TPC-H at a 1TB scale factor!
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not all those who drift are lost: Drift correction and calibration
  scheduling for the IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Hurst, Andrey V. Kalinichev, Klaus Koren, Daniel E. Lucani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensors provide a vital source of data that link digital systems with the
physical world. However, as sensors age, the relationship between what they
measure and what they output changes. This is known as sensor drift and poses a
significant challenge that, combined with limited opportunity for
re-calibration, can severely limit data quality over time. Previous approaches
to drift correction typically require large volumes of ground truth data and do
not consider measurement or prediction uncertainty. In this paper, we propose a
probabilistic sensor drift correction method that takes a fundamental approach
to modelling the sensor response using Gaussian Process Regression. Tested
using dissolved oxygen sensors, our method delivers mean squared error (MSE)
reductions of up to 90% and more than 20% on average. We also propose a novel
uncertainty-driven calibration schedule optimisation approach that builds on
top of drift correction and further reduces MSE by up to 15.7%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qymera: Simulating Quantum Circuits using RDBMS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Littau, Rihan Hai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum circuit simulation is crucial for quantum computing such as
validating quantum algorithms. We present Qymera, a system that repurposes
relational database management systems (RDBMSs) for simulation by translating
circuits into SQL queries, allowing quantum operations to run natively within
an RDBMS. Qymera supports a wide range of quantum circuits, offering a
graphical circuit builder and code-based interfaces to input circuits. With a
benchmarking framework, Qymera facilitates comparison of RDBMS-based simulation
against state-of-the-art simulation methods. Our demonstration showcases
Qymera's end-to-end SQL-based execution, seamless integration with classical
workflows, and its utility for development, benchmarking, and education in
quantum computing and data management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging RDF Knowledge Graphs with Graph Neural Networks for
  Semantically-Rich Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Färber, David Lamprecht, Yuni Susanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at DASFAA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and
  Design Choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Liu, Jiarui Ye, Mengshi Chen, Meng Li, Siqiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LSM-tree-based data stores are widely used in industry due to their
exceptional performance. However, as data volumes grow, efficiently querying
large-scale databases becomes increasingly challenging. To address this, recent
studies attempted to integrate learned indexes into LSM-trees to enhance lookup
performance, which has demonstrated promising improvements. Despite this, only
a limited range of learned index types has been considered, and the strengths
and weaknesses of different learned indexes remain unclear, making them
difficult for practical use. To fill this gap, we provide a comprehensive and
systematic benchmark to pursue an in-depth understanding of learned indexes in
LSM-tree systems. In this work, we summarize the workflow of 8 existing learned
indexes and analyze the associated theoretical cost. We also identify several
key factors that significantly influence the performance of learned indexes and
conclude them with a novel configuration space, including various index types,
boundary positions, and granularity. Moreover, we implement different learned
index designs on a unified platform to evaluate across various configurations.
Surprisingly, our experiments reveal several unexpected insights, such as the
marginal lookup enhancement when allocating a large memory budget to learned
indexes and modest retraining overhead of learned indexes. Besides, we also
offer practical guidelines to help developers intelligently select and tune
learned indexes for custom use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns
  from Subplan Hints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Sulimov, Claude Lehmann, Kurt Stockinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query optimization has become a research area where classical algorithms are
being challenged by machine learning algorithms. At the same time, recent
trends in learned query optimizers have shown that it is prudent to take
advantage of decades of database research and augment classical query
optimizers by shrinking the plan search space through different types of hints
(e.g. by specifying the join type, scan type or the order of joins) rather than
completely replacing the classical query optimizer with machine learning
models. It is especially relevant for cases when classical optimizers cannot
fully enumerate all logical and physical plans and, as an alternative, need to
rely on less robust approaches like genetic algorithms. However, even
symbiotically learned query optimizers are hampered by the need for vast
amounts of training data, slow plan generation during inference and unstable
results across various workload conditions. In this paper, we present GenJoin -
a novel learned query optimizer that considers the query optimization problem
as a generative task and is capable of learning from a random set of subplan
hints to produce query plans that outperform the classical optimizer. GenJoin
is the first learned query optimizer that significantly and consistently
outperforms PostgreSQL as well as state-of-the-art methods on two well-known
real-world benchmarks across a variety of workloads using rigorous machine
learning evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SQL-R1: Training Natural Language to SQL Reasoning Model By
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language to SQL (NL2SQL) enables intuitive interactions with
databases by transforming natural language queries into structured SQL
statements. Despite recent advancements in enhancing human-computer interaction
within database applications, significant challenges persist, particularly
regarding the inference performance in complex scenarios involving multi-table
joins and nested queries. Current methodologies primarily utilize supervised
fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and
interpretability in new environments (e.g., finance and healthcare). In order
to enhance the reasoning performance of the NL2SQL model in the above complex
situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the
reinforcement learning (RL) algorithms. We design a specialized RL-based reward
function tailored for NL2SQL tasks and discussed the impact of cold start on
the effectiveness of intensive training. In addition, we achieve competitive
accuracy using only a tiny amount of synthetic NL2SQL data for augmented
training and further explore data engineering for RL. In existing experiments,
SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider
and BIRD, respectively, only using the 7B base model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query Rewriting via <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sriram Dharwada, Himanshu Devrani, Jayant Haritsa, Harish Doraiswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When complex SQL queries suffer slow executions despite query optimization,
DBAs typically invoke automated query rewriting tools to recommend ``lean''
equivalents that are conducive to faster execution. The rewritings are usually
achieved via transformation rules, but these rules are limited in scope and
difficult to update in a production system. Recently, LLM-based techniques have
also been suggested, but they are prone to semantic and syntactic errors.
  We investigate here how the remarkable cognitive capabilities of LLMs can be
leveraged for performant query rewriting while incorporating safeguards and
optimizations to ensure correctness and efficiency. Our study shows that these
goals can be progressively achieved through incorporation of (a) an ensemble
suite of basic prompts, (b) database-sensitive prompts via redundancy removal
and selectivity-based rewriting rules, and (c) LLM token probability-guided
rewrite paths. Further, a suite of logic-based and statistical tools can be
used to check for semantic violations in the rewrites prior to DBA
consideration.
  We have implemented the above LLM-infused techniques in the LITHE system, and
evaluated complex analytic queries from standard benchmarks on contemporary
database platforms. The results show significant performance improvements for
slow queries, with regard to both abstract costing and actual execution, over
both SOTA techniques and the native query optimizer. For instance, with TPC-DS
on PostgreSQL, the geometric mean of the runtime speedups for slow queries was
as high as 13.2 over the native optimizer, whereas SOTA delivered 4.9 in
comparison.
  Overall, LITHE is a promising step toward viable LLM-based advisory tools for
ameliorating enterprise query performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-09T00:00:00Z">2025-06-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serendipitous <span class="highlight-title">Recommendation</span> with Multimodal <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoting Wang, Jianling Wang, Hao Li, Fangjun Yi, Mengyu Fu, Youwei Zhang, Yifan Liu, Liang Liu, Minmin Chen, Ed H. Chi, Lichan Hong, Haokai Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional recommendation systems succeed in identifying relevant content
but often fail to provide users with surprising or novel items. Multimodal
Large Language Models (MLLMs) possess the world knowledge and multimodal
understanding needed for serendipity, but their integration into
billion-item-scale platforms presents significant challenges. In this paper, we
propose a novel hierarchical framework where fine-tuned MLLMs provide
high-level guidance to conventional recommendation models, steering them
towards more serendipitous suggestions. This approach leverages MLLM strengths
in understanding multimodal content and user interests while retaining the
efficiency of traditional models for item-level recommendation. This mitigates
the complexity of applying MLLMs directly to vast action spaces. We also
demonstrate a chain-of-thought strategy enabling MLLMs to discover novel user
interests by first understanding video content and then identifying relevant
yet unexplored interest clusters. Through live experiments within a commercial
short-form video platform serving billions of users, we show that our
MLLM-powered approach significantly improves both recommendation serendipity
and user satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Stupid Questions: An Analysis of Question Query Generation for
  Citation <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian D. Zimmerman, Julien Aubert-Béduchaud, Florian Boudin, Akiko Aizawa, Olga Vechtomova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing techniques for citation recommendation are constrained by their
adherence to article contents and metadata. We leverage GPT-4o-mini's latent
expertise as an inquisitive assistant by instructing it to ask questions which,
when answered, could expose new insights about an excerpt from a scientific
article. We evaluate the utility of these questions as retrieval queries,
measuring their effectiveness in retrieving and ranking masked target
documents. In some cases, generated questions ended up being better queries
than extractive keyword queries generated by the same model. We additionally
propose MMR-RBO, a variation of Maximal Marginal Relevance (MMR) using
Rank-Biased Overlap (RBO) to identify which questions will perform
competitively with the keyword baseline. As all question queries yield unique
result sets, we contend that there are no stupid questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval <span class="chip">KDD '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdellah Ghassel, Ian Robinson, Gabriel Tanase, Hal Cooper, Bryan Thompson, Zhen Han, Vassilis N. Ioannidis, Soji Adeshina, Huzefa Rangwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Compression via Question Generation: Enhancing Multihop
  Document Retrieval without Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anvi Alex Eponon, Moein Shahiki-Tash, Ildar Batyrshin, Christian E. Maldonado-Sifuentes, Grigori Sidorov, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a question-based knowledge encoding approach that
improves retrieval-augmented generation (RAG) systems without requiring
fine-tuning or traditional chunking. We encode textual content using generated
questions that span the lexical and semantic space, creating targeted retrieval
cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a
Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We
also introduce "paper-cards", concise paper summaries under 300 characters,
which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified
technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with
LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking
and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency,
enables intuitive question-driven knowledge access, and decreases vector
storage demands by 80%, positioning it as a scalable and efficient RAG
alternative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of
  Legal Norms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hudson de Martim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific
  Research <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyong Lin, Lu Dai, Ruiqian Han, Yijie Sui, Ruilin Wang, Xingliang Sun, Qinglin Wu, Min Feng, Hao Liu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific researchers need intensive information about datasets to
effectively evaluate and develop theories and methodologies. The information
needs regarding datasets are implicitly embedded in particular research tasks,
rather than explicitly expressed in search queries. However, existing
scientific retrieval and question-answering (QA) datasets typically address
straightforward questions, which do not align with the distribution of
real-world research inquiries. To bridge this gap, we developed ScIRGen, a
dataset generation framework for scientific QA \& retrieval that more
accurately reflects the information needs of professional science researchers,
and uses it to create a large-scale scientific retrieval-augmented generation
(RAG) dataset with realistic queries, datasets and papers. Technically, we
designed a dataset-oriented information extraction method that leverages
academic papers to augment the dataset representation. We then proposed a
question generation framework by employing cognitive taxonomy to ensure the
quality of synthesized questions. We also design a method to automatically
filter synthetic answers based on the perplexity shift of LLMs, which is highly
aligned with human judgment of answers' validity. Collectively, these
methodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We
benchmarked representative methods on the ScIRGen-Geo dataset for their
question-answering and retrieval capabilities, finding out that current methods
still suffer from reasoning from complex questions. This work advances the
development of more sophisticated tools to support the intricate information
needs of the scientific community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Rostami, Vahid Rahimzadeh, Ali Adibi, Azadeh Shakery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset is available at https://doi.org/10.5281/zenodo.15616911</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Correlated Latent Exogenous Variables in Debiased Recommender
  Systems <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqiang Zhang, Yuchao Zhang, Jinkun Chen, Haochen Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 31st ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining V.2 (KDD '25), August 3--7, 2025, Toronto, ON,
  Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Historical and Current Interests for Continual Sequential
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuseok Lee, Hyunsik Yoo, Junyoung Hwang, SeongKu Kang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation models based on the Transformer architecture show
superior performance in harnessing long-range dependencies within user behavior
via self-attention. However, naively updating them on continuously arriving
non-stationary data streams incurs prohibitive computation costs or leads to
catastrophic forgetting. To address this, we propose Continual Sequential
Transformer for Recommendation (CSTRec) that effectively leverages
well-preserved historical user interests while capturing current interests. At
its core is Continual Sequential Attention (CSA), a linear attention mechanism
that retains past knowledge without direct access to old data. CSA integrates
two key components: (1) Cauchy-Schwarz Normalization that stabilizes training
under uneven interaction frequencies, and (2) Collaborative Interest Enrichment
that mitigates forgetting through shared, learnable interest pools. We further
introduce a technique that facilitates learning for cold-start users by
transferring historical knowledge from behaviorally similar existing users.
Extensive experiments on three real-world datasets indicate that CSTRec
outperforms state-of-the-art baselines in both knowledge retention and
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework
  for <span class="highlight-title">LLM</span>-Based Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Azizi, Fatemeh Koochaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLoSS: Generative Language Models with Semantic Search for Sequential
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Acharya, Aleksandr V. Petrov, Juba Ziani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Generative Low-rank language model with Semantic Search (GLoSS), a
generative recommendation framework that combines large language models with
dense retrieval for sequential recommendation. Unlike prior methods such as
GPT4Rec, which rely on lexical matching via BM25, GLoSS uses semantic search to
retrieve relevant items beyond lexical matching. For query generation, we
employ 4-bit quantized LlaMA-3 models fine-tuned with low-rank adaptation
(LoRA), enabling efficient training and inference on modest hardware. We
evaluate GLoSS on three real-world Amazon review datasets: Beauty, Toys, and
Sports, and find that it achieves state-of-the-art performance. Compared to
traditional ID-based baselines, GLoSS improves Recall@5 by 33.3%, 52.8%, and
15.2%, and NDCG@5 by 30.0%, 42.6%, and 16.1%, respectively. It also outperforms
LLM-based recommenders such as P5, GPT4Rec, LlamaRec and E4SRec with Recall@5
gains of 4.3%, 22.8%, and 29.5%. Additionally, user segment evaluations show
that GLoSS performs particularly well for cold-start users in the Amazon Toys
and Sports datasets, and benefits from longer user histories in Amazon Beauty
dataset, demonstrating robustness across different levels of interaction
lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and model checkpoints are publicly available
  at:https://github.com/krishnacharya/GLoSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-augmented systems can be dangerous medical communicators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patients have long sought health information online, and increasingly, they
are turning to generative AI to answer their health-related queries. Given the
high stakes of the medical domain, techniques like retrieval-augmented
generation and citation grounding have been widely promoted as methods to
reduce hallucinations and improve the accuracy of AI-generated responses and
have been widely adopted into search engines. This paper argues that even when
these methods produce literally accurate content drawn from source documents
sans hallucinations, they can still be highly misleading. Patients may derive
significantly different interpretations from AI-generated outputs than they
would from reading the original source material, let alone consulting a
knowledgeable clinician. Through a large-scale query analysis on topics
including disputed diagnoses and procedure safety, we support our argument with
quantitative and qualitative evidence of the suboptimal answers resulting from
current systems. In particular, we highlight how these models tend to
decontextualize facts, omit critical relevant sources, and reinforce patient
misconceptions or biases. We propose a series of recommendations -- such as the
incorporation of communication pragmatics and enhanced comprehension of source
documents -- that could help mitigate these issues and extend beyond the
medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Position paper in Proceedings of the 42 nd International Conference
  on Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span> Alignment as Retriever Optimization: An Information Retrieval
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized artificial intelligence with
capabilities in reasoning, coding, and communication, driving innovation across
industries. Their true potential depends on effective alignment to ensure
correct, trustworthy and ethical behavior, addressing challenges like
misinformation, hallucinations, bias and misuse. While existing Reinforcement
Learning (RL)-based alignment methods are notoriously complex, direct
optimization approaches offer a simpler alternative. In this work, we introduce
a novel direct optimization approach for LLM alignment by drawing on
established Information Retrieval (IR) principles. We present a systematic
framework that bridges LLM alignment and IR methodologies, mapping LLM
generation and reward models to IR's retriever-reranker paradigm. Building on
this foundation, we propose LLM Alignment as Retriever Preference Optimization
(LarPO), a new alignment method that enhances overall alignment quality.
Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %
averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work
opens new avenues for advancing LLM alignment by integrating IR foundations,
offering a promising direction for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Lukasik, Lin Chen, Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, Felix X. Yu, Sashank J. Reddi, Gang Fu, Mohammadhossein Bateni, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bipartite ranking is a fundamental supervised learning problem, with the goal
of learning a ranking over instances with maximal Area Under the ROC Curve
(AUC) against a single binary target label. However, one may often observe
multiple binary target labels, e.g., from distinct human annotators. How can
one synthesize such labels into a single coherent ranking? In this work, we
formally analyze two approaches to this problem -- loss aggregation and label
aggregation -- by characterizing their Bayes-optimal solutions. We show that
while both approaches can yield Pareto-optimal solutions, loss aggregation can
exhibit label dictatorship: one can inadvertently (and undesirably) favor one
label over others. This suggests that label aggregation can be preferable to
loss aggregation, which we empirically verify.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Universal User Representations Leveraging <span class="highlight-title">Cross</span>-<span class="highlight-title">domain</span> User
  Intent at Snapchat <span class="chip">SIGIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.21838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.21838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clark Mingxuan Ju, Leonardo Neves, Bhuvesh Kumar, Liam Collins, Tong Zhao, Yuwei Qiu, Qing Dou, Yang Zhou, Sohail Nizam, Rengim Ozturk, Yvette Liu, Sen Yang, Manish Malik, Neil Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of powerful user representations is a key factor in the
success of recommender systems (RecSys). Online platforms employ a range of
RecSys techniques to personalize user experience across diverse in-app
surfaces. User representations are often learned individually through user's
historical interactions within each surface and user representations across
different surfaces can be shared post-hoc as auxiliary features or additional
retrieval sources. While effective, such schemes cannot directly encode
collaborative filtering signals across different surfaces, hindering its
capacity to discover complex relationships between user behaviors and
preferences across the whole platform. To bridge this gap at Snapchat, we seek
to conduct universal user modeling (UUM) across different in-app surfaces,
learning general-purpose user representations which encode behaviors across
surfaces. Instead of replacing domain-specific representations, UUM
representations capture cross-domain trends, enriching existing representations
with complementary information. This work discusses our efforts in developing
initial UUM versions, practical challenges, technical choices and modeling and
research directions with promising offline performance. Following successful
A/B testing, UUM representations have been launched in production, powering
multiple use cases and demonstrating their value. UUM embedding has been
incorporated into (i) Long-form Video embedding-based retrieval, leading to
2.78% increase in Long-form Video Open Rate, (ii) Long-form Video L2 ranking,
with 19.2% increase in Long-form Video View Time sum, (iii) Lens L2 ranking,
leading to 1.76% increase in Lens play time, and (iv) Notification L2 ranking,
with 0.87% increase in Notification Open Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the industrial track of SIGIR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introspective Growth: Automatically Advancing <span class="highlight-title">LLM</span> Expertise in
  Technology Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that compared
to placebo scientific information, prompting LLMs to generate and answer their
own questions - targeting the background knowledge required for the task -
significantly improves performance. These self-generated questions and answers
activate otherwise underutilized internal knowledge. Allowing LLMs to retrieve
answers from external scientific texts further enhances performance, suggesting
that model knowledge is compressed and lacks the full richness of the training
data. We also find that chain-of-thought prompting and self-questioning
converge, though self-questioning remains more effective for improving
understanding of technical concepts. Notably, we uncover an asymmetry in
prompting: smaller models often generate more fundamental, more open-ended,
better-aligned questions for mid-sized models than large models do, revealing a
new strategy for cross-model collaboration. Altogether, our findings establish
self-questioning as both a practical mechanism for automatically improving LLM
comprehension, especially in domains with sparse and underrepresented
knowledge, and a diagnostic probe of how internal and external knowledge are
organized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our patent dataset at
  https://huggingface.co/datasets/UchiKlab/patent_understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmark Granularity and Model Robustness for Image-Text Retrieval <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15239v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15239v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Hendriksen, Shuo Zhang, Ridho Reinanda, Mohamed Yahya, Edgar Meij, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-Text Retrieval (ITR) systems are central to multimodal information
access, with Vision-Language Models (VLMs) showing strong performance on
standard benchmarks. However, these benchmarks predominantly rely on
coarse-grained annotations, limiting their ability to reveal how models perform
under real-world conditions, where query granularity varies. Motivated by this
gap, we examine how dataset granularity and query perturbations affect
retrieval performance and robustness across four architecturally diverse VLMs
(ALIGN, AltCLIP, CLIP, and GroupViT). Using both standard benchmarks (MS-COCO,
Flickr30k) and their fine-grained variants, we show that richer captions
consistently enhance retrieval, especially in text-to-image tasks, where we
observe an average improvement of 16.23%, compared to 6.44% in image-to-text.
To assess robustness, we introduce a taxonomy of perturbations and conduct
extensive experiments, revealing that while perturbations typically degrade
performance, they can also unexpectedly improve retrieval, exposing nuanced
model behaviors. Notably, word order emerges as a critical factor --
contradicting prior assumptions of model insensitivity to it. Our results
highlight variation in model robustness and a dataset-dependent relationship
between caption granularity and perturbation sensitivity and emphasize the
necessity of evaluating models on datasets of varying granularity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenTCM: A GraphRAG-Empowered <span class="highlight-title">LLM</span>-based System for Traditional Chinese
  Medicine Knowledge Retrieval and Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglin He, Yunqi Guo, Lai Kwan Lam, Waikei Leung, Lixing He, Yuanan Jiang, Chi Chiu Wang, Guoliang Xing, Hongkai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we integrate OpenTCM with this knowledge graph, enabling high-fidelity
ingredient knowledge retrieval and diagnostic question-answering without model
fine-tuning. Experimental evaluations demonstrate that OpenTCM achieves mean
expert scores (MES) of 4.378 in ingredient information retrieval and 4.045 in
diagnostic question-answering tasks, outperforming state-of-the-art solutions
in real-world TCM use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coherence-guided Preference Disentanglement for <span class="highlight-title">Cross</span>-<span class="highlight-title">domain</span>
  <span class="highlight-title">Recommendation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyi Xiang, Yan Zhang, Lixin Duan, Hongzhi Yin, Ivor W. Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering user preferences across different domains is pivotal in
cross-domain recommendation systems, particularly when platforms lack
comprehensive user-item interactive data. The limited presence of shared users
often hampers the effective modeling of common preferences. While leveraging
shared items' attributes, such as category and popularity, can enhance
cross-domain recommendation performance, the scarcity of shared items between
domains has limited research in this area. To address this, we propose a
Coherence-guided Preference Disentanglement (CoPD) method aimed at improving
cross-domain recommendation by i) explicitly extracting shared item attributes
to guide the learning of shared user preferences and ii) disentangling these
preferences to identify specific user interests transferred between domains.
CoPD introduces coherence constraints on item embeddings of shared and specific
domains, aiding in extracting shared attributes. Moreover, it utilizes these
attributes to guide the disentanglement of user preferences into separate
embeddings for interest and conformity through a popularity-weighted loss.
Experiments conducted on real-world datasets demonstrate the superior
performance of our proposed CoPD over existing competitive baselines,
highlighting its effectiveness in enhancing cross-domain recommendation
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified
  Theory and Risk Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance
  <span class="highlight-title">LLM</span> <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Zhao, Fengli Xu, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by advances in Large Language Models (LLMs), integrating them into
recommendation tasks has gained interest due to their strong semantic
understanding and prompt flexibility. Prior work encoded user-item interactions
or metadata into prompts for recommendations. In parallel, LLM reasoning,
boosted by test-time scaling and reinforcement learning, has excelled in fields
like mathematics and code, where reasoning traces and correctness signals are
clear, enabling high performance and interpretability. However, directly
applying these reasoning methods to recommendation is ineffective because user
feedback is implicit and lacks reasoning supervision. To address this, we
propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that
samples interaction chains from the user-item graph and converts them into
structured interaction-of-thoughts via a progressive masked prompting strategy,
with each thought representing stepwise reasoning grounded in interaction
context. This allows LLMs to simulate step-by-step decision-making based on
implicit patterns. We design a two-stage training pipeline: supervised
fine-tuning teaches basic reasoning from high-quality traces, and reinforcement
learning refines reasoning via reward signals, alleviating sparse explicit
supervision. Experiments on three real-world datasets show R2Rec outperforms
classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement
in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore,
the explicit reasoning chains enhance interpretability by revealing the
decision process. Our code is available at:
https://anonymous.4open.science/r/R2Rec-7C5D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliciting In-context Retrieval and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-View Adaptive Contrastive Learning for Information Retrieval Based
  Fault Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunying Zhou, Xiaoyuan Xie, Gong Chen, Peng He, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most studies focused on information retrieval-based techniques for fault
localization, which built representations for bug reports and source code files
and matched their semantic vectors through similarity measurement. However,
such approaches often ignore some useful information that might help improve
localization performance, such as 1) the interaction relationship between bug
reports and source code files; 2) the similarity relationship between bug
reports; and 3) the co-citation relationship between source code files. In this
paper, we propose a novel approach named Multi-View Adaptive Contrastive
Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the
above-mentioned relationships for software fault localization. Specifically, we
first generate data augmentations from report-code interaction view,
report-report similarity view and code-code co-citation view separately, and
adopt graph neural network to aggregate the information of bug reports or
source code files from the three views in the embedding process. Moreover, we
perform contrastive learning across these views. Our design of contrastive
learning task will force the bug report representations to encode information
shared by report-report and report-code views,and the source code file
representations shared by code-code and report-code views, thereby alleviating
the noise from auxiliary information. Finally, to evaluate the performance of
our approach, we conduct extensive experiments on five open-source Java
projects. The results show that our model can improve over the best baseline up
to 28.93%, 25.57% and 20.35% on Accuracy@1, MAP and MRR, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Mitigate Information Loss in Knowledge Graphs for GraphRAG:
  Leveraging Triple Context Restoration and Query-Driven Feedback <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manzong Huang, Chenyang Bu, Yi He, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently
propelled significant advances in complex reasoning tasks, thanks to their
broad domain knowledge and contextual awareness. Unfortunately, current methods
often assume KGs to be complete, which is impractical given the inherent
limitations of KG construction and the potential loss of contextual cues when
converting unstructured text into entity-relation triples. In response, this
paper proposes the Triple Context Restoration and Query-driven Feedback
(TCR-QF) framework, which reconstructs the textual context underlying each
triple to mitigate information loss, while dynamically refining the KG
structure by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the
effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%
improvement in Exact Match and a 15.5% improvement in F1 over its
state-of-the-art GraphRAG competitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEANN: A Low-Storage Vector Index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Wang, Shu Liu, Zhifei Li, Yongji Wu, Ziming Mao, Yilong Zhao, Xiao Yan, Zhiying Xu, Yang Zhou, Ion Stoica, Sewon Min, Matei Zaharia, Joseph E. Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RADAR: Benchmarking Language Models on Imperfect Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken Gu, Zhihan Zhang, Kate Lin, Yuwei Zhang, Akshay Paruchuri, Hong Yu, Mehran Kazemi, Kumar Ayush, A. Ali Heydari, Maxwell A. Xu, Girish Narayanswamy, Yun Liu, Ming-Zher Poh, Yuzhe Yang, Mark Malhotra, Shwetak Patel, Hamid Palangi, Xuhai Xu, Daniel McDuff, Tim Althoff, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUITE: A Query Rewrite System Beyond Rules with <span class="highlight-title">LLM</span> Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Information-Theoretical Size Bounds for Conjunctive Queries with
  Functional Dependencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valter Uotila, Jiaheng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving formulations for computing and estimating tight worst-case size
increases for conjunctive queries with various constraints has been at the core
of theoretical database research. If the problem has no constraints or only one
constraint, such as functional dependencies or degree constraints, tight
worst-case size bounds have been proven, and they are even practically
computable. If the problem has more than one constraint, computing tight bounds
can be difficult in practice and may even require an infinite number of linear
inequalities in its optimization formulation. While these challenges have been
addressed with varying methods, no prior research has employed quantum
information theory to address this problem. In this work, we establish a
connection between earlier work on estimating size bounds for conjunctive
queries with classical information theory and the field of quantum information
theory. We propose replacing the classical Shannon entropy formulation with the
quantum R\'enyi entropy. Whereas classical Shannon entropy requires infinitely
many inequalities to characterize the optimization space, R\'enyi entropy
requires only one type of inequality, which is non-negativity. Although this is
a promising modification, optimization with respect to the quantum states
instead of classical distributions creates a new set of challenges that prevent
us from finding a practically computable, tight worst-case size bound. In this
line, we propose a quantum version to derive worst-case size bounds. The
previous tight classical worst-case size bound can be viewed as a special limit
of this quantum bound. We also provide a comprehensive background on prior
research and discuss the future possibilities of quantum information theory in
theoretical database research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIFBench: An Extensive Benchmark for Fatigue Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Gautam, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fatigue-induced crack growth is a leading cause of structural failure across
critical industries such as aerospace, civil engineering, automotive, and
energy. Accurate prediction of stress intensity factors (SIFs) -- the key
parameters governing crack propagation in linear elastic fracture mechanics --
is essential for assessing fatigue life and ensuring structural integrity.
While machine learning (ML) has shown great promise in SIF prediction, its
advancement has been severely limited by the lack of rich, transparent,
well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale
benchmark database designed to support ML-based SIF prediction. SIFBench
contains over 5 million different crack and component geometries derived from
high-fidelity finite element simulations across 37 distinct scenarios, and
provides a unified Python interface for seamless data access and customization.
We report baseline results using a range of popular ML models -- including
random forests, support vector machines, feedforward neural networks, and
Fourier neural operators -- alongside comprehensive evaluation metrics and
template code for model training, validation, and assessment. By offering a
standardized and scalable resource, SIFBench substantially lowers the entry
barrier and fosters the development and application of ML methods in damage
tolerance design and predictive maintenance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BVLSM: Write-Efficient LSM-Tree Storage via WAL-Time Key-Value
  Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Wendi Cheng, Jiahe Wei, Xueqiang Shan, Weikai Liu, Xiaonan Zhao, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern data-intensive applications increasingly store and process big-value
items, such as multimedia objects and machine learning embeddings, which
exacerbate storage inefficiencies in Log-Structured Merge-Tree (LSM)-based
key-value stores. This paper presents BVLSM, a Write-Ahead Log (WAL)-time
key-value separation mechanism designed to address three key challenges in
LSM-Tree storage systems: write amplification, poor memory utilization, and I/O
jitter under big-value workloads. Unlike state-of-the-art approaches that delay
key-value separation until the flush stage, leading to redundant data in
MemTables and repeated writes. BVLSM proactively decouples keys and values
during the WAL phase. The MemTable stores only lightweight metadata, allowing
multi-queue parallel store for big value. The benchmark results show that BVLSM
significantly outperforms both RocksDB and BlobDB under 64KB random write
workloads. In asynchronous WAL mode, it achieves throughput improvements of
7.6x over RocksDB and 1.9x over BlobDB.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-08T00:00:00Z">2025-06-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HotelMatch-<span class="highlight-title">LLM</span>: Joint <span class="highlight-title">Multi-Task</span> Training of Small and Large Language
  Models for Efficient Multimodal Hotel Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arian Askari, Emmanouil Stergiadis, Ilya Gusev, Moran Beladev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025, Main track. 13 Pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Knowledge Graphs: the Shifting Paradigm of Scholarly
  Information Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthäus Zloch, Danilo Dessì, Jennifer D'Souza, Leyla Jael Castro, Benjamin Zapilko, Saurav Karmakar, Brigitte Mathiak, Markus Stocker, Wolfgang Otto, Sören Auer, Stefan Dietze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharing and reusing research artifacts, such as datasets, publications, or
methods is a fundamental part of scientific activity, where heterogeneity of
resources and metadata and the common practice of capturing information in
unstructured publications pose crucial challenges. Reproducibility of research
and finding state-of-the-art methods or data have become increasingly
challenging. In this context, the concept of Research Knowledge Graphs (RKGs)
has emerged, aiming at providing an easy to use and machine-actionable
representation of research artifacts and their relations. That is facilitated
through the use of established principles for data representation, the
consistent adoption of globally unique persistent identifiers and the reuse and
linking of vocabularies and data. This paper provides the first
conceptualisation of the RKG vision, a categorisation of in-use RKGs together
with a description of RKG building blocks and principles. We also survey
real-world RKG implementations differing with respect to scale, schema, data,
used vocabulary, and reliability of the contained data. We also characterise
different RKG construction methodologies and provide a forward-looking
perspective on the diverse applications, opportunities, and challenges
associated with the RKG vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Semantic Web Conference 2025, In-use track, 10 pages, 1
  figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RADAR: Recall Augmentation through Deferred Asynchronous Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Jaspal, Qian Dang, Ajantha Ramineni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational
  Agents: A Framework for Evaluation (CAFE) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine Bauer, Li Chen, Nicola Ferro, Norbert Fuhr, Avishek Anand, Timo Breuer, Guglielmo Faggioli, Ophir Frieder, Hideo Joho, Jussi Karlgren, Johannes Kiesel, Bart P. Knijnenburg, Aldo Lipani, Lien Michiels, Andrea Papenmeier, Maria Soledad Pera, Mark Sanderson, Scott Sanner, Benno Stein, Johanne R. Trippas, Karin Verspoor, Martijn C Willemsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the workshop, we deeply discussed what CONversational Information
ACcess (CONIAC) is and its unique features, proposing a world model abstracting
it, and defined the Conversational Agents Framework for Evaluation (CAFE) for
the evaluation of CONIAC systems, consisting of six major components: 1) goals
of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)
aspects of the users carrying out the tasks, 4) evaluation criteria to be
considered, 5) evaluation methodology to be applied, and 6) measures for the
quantitative criteria chosen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages; 10 figures; Dagstuhl manifesto</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Swath to Full-Disc: Advancing Precipitation Retrieval with
  Multimodal Knowledge Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Wang, Kai Ying, Bin Xu, Chunjiao Wang, Cong Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate near-real-time precipitation retrieval has been enhanced by
satellite-based technologies. However, infrared-based algorithms have low
accuracy due to weak relations with surface precipitation, whereas passive
microwave and radar-based methods are more accurate but limited in range. This
challenge motivates the Precipitation Retrieval Expansion (PRE) task, which
aims to enable accurate, infrared-based full-disc precipitation retrievals
beyond the scanning swath. We introduce Multimodal Knowledge Expansion, a
two-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling
stage, PRE-Net transfers knowledge from a multimodal data integration model to
an infrared-based model within the scanning swath via Coordinated Masking and
Wavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune
refines predictions across the full disc by balancing multimodal and full-disc
infrared knowledge. Experiments on the introduced PRE benchmark demonstrate
that PRE-Net significantly advanced precipitation retrieval performance,
outperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code
will be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correcting for Position Bias in Learning to Rank: A Control Function
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Aminul Islam, Kathryn Vasilaky, Elena Zheleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Interest Needle in Popularity Haystack: Improving Retrieval by
  Modeling Item Exposure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Agarwal, Amit Jaspal, Saurabh Gupta, Omkar Vichare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems operate in closed feedback loops, where user interactions
reinforce popularity bias, leading to over-recommendation of already popular
items while under-exposing niche or novel content. Existing bias mitigation
methods, such as Inverse Propensity Scoring (IPS) and Off-Policy Correction
(OPC), primarily operate at the ranking stage or during training, lacking
explicit real-time control over exposure dynamics. In this work, we introduce
an exposure-aware retrieval scoring approach, which explicitly models item
exposure probability and adjusts retrieval-stage ranking at inference time.
Unlike prior work, this method decouples exposure effects from engagement
likelihood, enabling controlled trade-offs between fairness and engagement in
large-scale recommendation platforms. We validate our approach through online
A/B experiments in a real-world video recommendation system, demonstrating a
25% increase in uniquely retrieved items and a 40% reduction in the dominance
of over-popular content, all while maintaining overall user engagement levels.
Our results establish a scalable, deployable solution for mitigating popularity
bias at the retrieval stage, offering a new paradigm for bias-aware
personalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages. UMAP '25: 33rd ACM Conference on User Modeling, Adaptation
  and Personalization, New York City, USA, June 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepRAG: Thinking to Retrieve Step by Step for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable reasoning capabilities,
while their practical applications are limited by severe factual hallucinations
due to limitations in the timeliness, accuracy, and comprehensiveness of their
parametric knowledge. Meanwhile, enhancing retrieval-augmented generation (RAG)
with reasoning remains challenging due to ineffective task decomposition and
redundant retrieval, which can introduce noise and degrade response quality. In
this paper, we propose DeepRAG, a framework that models retrieval-augmented
reasoning as a Markov Decision Process (MDP), enabling reasonable and adaptive
retrieval. By iteratively decomposing queries, DeepRAG dynamically determines
whether to retrieve external knowledge or rely on parametric reasoning at each
step. Experiments show that DeepRAG improves retrieval efficiency and boosts
answer accuracy by 26.4%, demonstrating its effectiveness in enhancing
retrieval-augmented reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gumbel Reranking: Differentiable End-to-End Reranker Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG systems rely on rerankers to identify relevant documents. However,
fine-tuning these models remains challenging due to the scarcity of annotated
query-document pairs. Existing distillation-based approaches suffer from
training-inference misalignment and fail to capture interdependencies among
candidate documents. To overcome these limitations, we reframe the reranking
process as an attention-mask problem and propose Gumbel Reranking, an
end-to-end training framework for rerankers aimed at minimizing the
training-inference gap. In our approach, reranker optimization is reformulated
as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel
Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end
optimization by minimizing the overall language loss. Experiments across
various settings consistently demonstrate performance gains, including a 10.4\%
improvement in recall on HotpotQA for distinguishing indirectly relevant
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMTEB: Massive Multilingual Text Embedding Benchmark <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings are typically evaluated on a limited set of tasks, which are
constrained by language, domain, and task diversity. To address these
limitations and provide a more comprehensive evaluation, we introduce the
Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,
community-driven expansion of MTEB, covering over 500 quality-controlled
evaluation tasks across 250+ languages. MMTEB includes a diverse set of
challenging, novel tasks such as instruction following, long-document
retrieval, and code retrieval, representing the largest multilingual collection
of evaluation tasks for embedding models to date. Using this collection, we
develop several highly multilingual benchmarks, which we use to evaluate a
representative set of models. We find that while large language models (LLMs)
with billions of parameters can achieve state-of-the-art performance on certain
language subsets and task categories, the best-performing publicly available
model is multilingual-e5-large-instruct with only 560 million parameters. To
facilitate accessibility and reduce computational cost, we introduce a novel
downsampling method based on inter-task correlation, ensuring a diverse
selection while preserving relative model rankings. Furthermore, we optimize
tasks such as retrieval by sampling hard negatives, creating smaller but
effective splits. These optimizations allow us to introduce benchmarks that
drastically reduce computational demands. For instance, our newly introduced
zero-shot English benchmark maintains a ranking order similar to the full-scale
version but at a fraction of the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Training and Inference Scaling Laws in Generative Retrieval <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Cai, Yongqi Li, Ruifeng Yuan, Wenjie Wang, Zhen Zhang, Wenjie Li, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval reformulates retrieval as an autoregressive generation
task, where large language models (LLMs) generate target documents directly
from a query. As a novel paradigm, the mechanisms that underpin its performance
and scalability remain largely unexplored. We systematically investigate
training and inference scaling laws in generative retrieval, exploring how
model size, training data scale, and inference-time compute jointly influence
performance. We propose a novel evaluation metric inspired by contrastive
entropy and generation loss, providing a continuous performance signal that
enables robust comparisons across diverse generative retrieval methods. Our
experiments show that n-gram-based methods align strongly with training and
inference scaling laws. We find that increasing model size, training data
scale, and inference-time compute all contribute to improved performance,
highlighting the complementary roles of these factors in enhancing generative
retrieval. Across these settings, LLaMA models consistently outperform T5
models, suggesting a particular advantage for larger decoder-only models in
generative retrieval. Our findings underscore that model sizes, data
availability, and inference computation interact to unlock the full potential
of generative retrieval, offering new insights for designing and optimizing
future systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can the Rookies Cut the Tough Cookie? Exploring the Use of <span class="highlight-title">LLM</span>s for SQL
  Equivalence Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajat Singh, Srikanta Bedathur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivalence checking of SQL queries is an intractable problem often
encountered in settings ranging from grading SQL submissions to debugging query
optimizers. Despite recent work toward developing practical solutions, only
simple queries written using a small subset of SQL are supported, leaving the
equivalence checking of sophisticated SQL queries at the mercy of intensive,
potentially error-prone, manual analysis. In this paper, we explore how LLMs
can be used to reason with SQL queries to address this challenging problem.
Towards this, we introduce a novel, realistic, and sufficiently complex
benchmark called SQLEquiQuest for SQL query equivalence checking that reflects
real-world settings. We establish strong baselines for SQL equivalence checking
by leveraging the ability of LLMs to reason with SQL queries. We conduct a
detailed evaluation of several state-of-the-art LLMs using various prompting
strategies and carefully constructed in-context learning examples, including
logical plans generated by SQL query processors. Our empirical evaluation shows
that LLMs go well beyond the current capabilities of formal models for SQL
equivalence, going from a mere 30% supported query pairs to full coverage,
achieving up to 82% accuracy on Spider+DIN. However, a critical limitation of
LLMs revealed by our analysis is that they exhibit a strong bias for
equivalence predictions, with consistently poor performance over non-equivalent
pairs, opening a new direction for potential future research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-07T00:00:00Z">2025-06-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneSug: The Unified End-to-End Generative Framework for E-commerce Query
  Suggestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Guo, Ben Chen, Siyuan Wang, Ying Yang, Chenyi Lei, Yuqing Ding, Han Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query suggestion plays a crucial role in enhancing user experience in
e-commerce search systems by providing relevant query recommendations that
align with users' initial input. This module helps users navigate towards
personalized preference needs and reduces typing effort, thereby improving
search experience. Traditional query suggestion modules usually adopt
multi-stage cascading architectures, for making a well trade-off between system
response time and business conversion. But they often suffer from
inefficiencies and suboptimal performance due to inconsistent optimization
objectives across stages. To address these, we propose OneSug, the first
end-to-end generative framework for e-commerce query suggestion. OneSug
incorporates a prefix2query representation enhancement module to enrich
prefixes using semantically and interactively related queries to bridge content
and business characteristics, an encoder-decoder generative model that unifies
the query suggestion process, and a reward-weighted ranking strategy with
behavior-level weights to capture fine-grained user preferences. Extensive
evaluations on large-scale industry datasets demonstrate OneSug's ability for
effective and efficient query suggestion. Furthermore, OneSug has been
successfully deployed for the entire traffic on the e-commerce search engine in
Kuaishou platform for over 1 month, with statistically significant improvements
in user top click position (-9.33%), CTR (+2.01%), Order (+2.04%), and Revenue
(+1.69%) over the online multi-stage strategy, showing great potential in
e-commercial conversion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The State-of-the-Art in Lifelog Retrieval: A <span class="highlight-title">Review</span> of Progress at the
  ACM Lifelog Search Challenge Workshop 2022-24 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allie Tran, Werner Bailer, Duc-Tien Dang-Nguyen, Graham Healy, Steve Hodges, Björn Þór Jónsson, Luca Rossetto, Klaus Schoeffmann, Minh-Triet Tran, Lucia Vadicamo, Cathal Gurrin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ACM Lifelog Search Challenge (LSC) is a venue that welcomes and compares
systems that support the exploration of lifelog data, and in particular the
retrieval of specific information, through an interactive competition format.
This paper reviews the recent advances in interactive lifelog retrieval as
demonstrated at the ACM LSC from 2022 to 2024. Through a detailed comparative
analysis, we highlight key improvements across three main retrieval tasks:
known-item search, question answering, and ad-hoc search. Our analysis
identifies trends such as the widespread adoption of embedding-based retrieval
methods (e.g., CLIP, BLIP), increased integration of large language models
(LLMs) for conversational retrieval, and continued innovation in multimodal and
collaborative search interfaces. We further discuss how specific retrieval
techniques and user interface (UI) designs have impacted system performance,
emphasizing the importance of balancing retrieval complexity with usability.
Our findings indicate that embedding-driven approaches combined with LLMs show
promise for lifelog retrieval systems. Likewise, improving UI design can
enhance usability and efficiency. Additionally, we recommend reconsidering
multi-instance system evaluations within the expert track to better manage
variability in user familiarity and configuration effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic and Parametric Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Qingyao Ai, Jingtao Zhan, Qian Dong, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-based RAG Enhancement via Global Query Disambiguation and
  Dependency-Aware Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyuan Li, Junrui Liu, Yi Shan, Minghui Huang, Tong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary graph-based retrieval-augmented generation (RAG) methods
typically begin by extracting entities from user queries and then leverage
pre-constructed knowledge graphs to retrieve related relationships and
metadata. However, this pipeline's exclusive reliance on entity-level
extraction can lead to the misinterpretation or omission of latent yet critical
information and relations. As a result, retrieved content may be irrelevant or
contradictory, and essential knowledge may be excluded, exacerbating
hallucination risks and degrading the fidelity of generated responses. To
address these limitations, we introduce PankRAG, a framework that combines a
globally aware, hierarchical query-resolution strategy with a novel
dependency-aware reranking mechanism. PankRAG first constructs a multi-level
resolution path that captures both parallel and sequential interdependencies
within a query, guiding large language models (LLMs) through structured
reasoning. It then applies its dependency-aware reranker to exploit the
dependency structure among resolved sub-questions, enriching and validating
retrieval results for subsequent sub-questions. Empirical evaluations
demonstrate that PankRAG consistently outperforms state-of-the-art approaches
across multiple benchmarks, underscoring its robustness and generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Total Recall: Enhancing FAIRness through AI-Driven Metadata
  Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowmya S Sundaram, Rafael S. Gonçalves, Mark A Musen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific metadata often suffer from incompleteness, inconsistency, and
formatting errors, which hinder effective discovery and reuse of the associated
datasets. We present a method that combines GPT-4 with structured metadata
templates from the CEDAR knowledge base to automatically standardize metadata
and to ensure compliance with established standards. A CEDAR template specifies
the expected fields of a metadata submission and their permissible values. Our
standardization process involves using CEDAR templates to guide GPT-4 in
accurately correcting and refining metadata entries in bulk, resulting in
significant improvements in metadata retrieval performance, especially in
recall -- the proportion of relevant datasets retrieved from the total relevant
datasets available. Using the BioSample and GEO repositories maintained by the
National Center for Biotechnology Information (NCBI), we demonstrate that
retrieval of datasets whose metadata are altered by GPT-4 when provided with
CEDAR templates (GPT-4+CEDAR) is substantially better than retrieval of
datasets whose metadata are in their original state and that of datasets whose
metadata are altered using GPT-4 with only data-dictionary guidance (GPT-4+DD).
The average recall increases dramatically, from 17.65\% with baseline raw
metadata to 62.87\% with GPT-4+CEDAR. Furthermore, we evaluate the robustness
of our approach by comparing GPT-4 against other large language models,
including LLaMA-3 and MedLLaMA2, demonstrating consistent performance
advantages for GPT-4+CEDAR. These results underscore the transformative
potential of combining advanced language models with symbolic models of
standardized metadata structures for more effective and reliable data
retrieval, thus accelerating scientific discoveries and data-driven research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Sequential <span class="highlight-title">Recommendation</span> with Vanilla <span class="highlight-title">Cross</span>-Entropy Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fan, Yanrong Hu, Kai Fang, Qingyang Liu, Hongjiu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) systems model user preferences by analyzing
interaction histories. Although transferable multi-modal SR architectures
demonstrate superior performance compared to traditional ID-based approaches,
current methods incur substantial fine-tuning costs when adapting to new
domains due to complex optimization requirements and negative transfer effects
- a significant deployment bottleneck that hinders engineers from efficiently
repurposing pre-trained models for novel application scenarios with minimal
tuning overhead. We propose MMM4Rec (Multi-Modal Mamba for Sequential
Recommendation), a novel multi-modal SR framework that incorporates a dedicated
algebraic constraint mechanism for efficient transfer learning. By combining
State Space Duality (SSD)'s temporal decay properties with a time-aware
modeling design, our model dynamically prioritizes key modality information,
overcoming limitations of Transformer-based approaches. The framework
implements a constrained two-stage process: (1) sequence-level cross-modal
alignment via shared projection matrices, followed by (2) temporal fusion using
our newly designed Cross-SSD module and dual-channel Fourier adaptive
filtering. This architecture maintains semantic consistency while suppressing
noise propagation.MMM4Rec achieves rapid fine-tuning convergence with simple
cross-entropy loss, significantly improving multi-modal recommendation accuracy
while maintaining strong transferability. Extensive experiments demonstrate
MMM4Rec's state-of-the-art performance, achieving the maximum 31.78% NDCG@10
improvement over existing models and exhibiting 10 times faster average
convergence speed when transferring to large-scale downstream datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>4DSR: Leveraging <span class="highlight-title">Large Language Model</span> for Denoising Sequential
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08208v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08208v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Wang, Feng Liu, Changwang Zhang, Jiawei Chen, Yudi Wu, Sheng Zhou, Xingyu Lou, Jun Wang, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommenders generate recommendations based on users' historical
interaction sequences. However, in practice, these collected sequences are
often contaminated by noisy interactions, which significantly impairs
recommendation performance. Accurately identifying such noisy interactions
without additional information is particularly challenging due to the absence
of explicit supervisory signals indicating noise. Large Language Models (LLMs),
equipped with extensive open knowledge and semantic reasoning abilities, offer
a promising avenue to bridge this information gap. However, employing LLMs for
denoising in sequential recommendation presents notable challenges: 1) Direct
application of pretrained LLMs may not be competent for the denoising task,
frequently generating nonsensical responses; 2) Even after fine-tuning, the
reliability of LLM outputs remains questionable, especially given the
complexity of the denoising task and the inherent hallucinatory issue of LLMs.
  To tackle these challenges, we propose LLM4DSR, a tailored approach for
denoising sequential recommendation using LLMs. We constructed a
self-supervised fine-tuning task to activate LLMs' capabilities to identify
noisy items and suggest replacements. Furthermore, we developed an uncertainty
estimation module that ensures only high-confidence responses are utilized for
sequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected
sequences to be flexibly applied across various recommendation models.
Extensive experiments validate the superiority of LLM4DSR over existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSL: Not All Tokens Are What You Need for Tuning <span class="highlight-title">LLM</span> as a Recommender <span class="chip">SIGIR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04178v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04178v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Wang, Feng Liu, Jiawei Chen, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), known for their comprehension capabilities and
extensive knowledge, have been increasingly applied to recommendation systems
(RS). Given the fundamental gap between the mechanism of LLMs and the
requirement of RS, researchers have focused on fine-tuning LLMs with
recommendation-specific data to enhance their performance. Language Modeling
Loss (LML), originally designed for language generation tasks, is commonly
adopted. However, we identify two critical limitations of LML: 1) it exhibits
significant divergence from the recommendation objective; 2) it erroneously
treats all fictitious item descriptions as negative samples, introducing
misleading training signals.
  To address these limitations, we propose a novel Masked Softmax Loss (MSL)
tailored for fine-tuning LLMs on recommendation. MSL improves LML by
identifying and masking invalid tokens that could lead to fictitious item
descriptions during loss computation. This strategy can effectively avoid the
interference from erroneous negative signals and ensure well alignment with the
recommendation objective supported by theoretical guarantees. During
implementation, we identify a potential challenge related to gradient vanishing
of MSL. To overcome this, we further introduce the temperature coefficient and
propose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the
temperature without requiring extensive hyperparameter tuning. Extensive
experiments conducted on four public datasets further validate the
effectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.
The code is available at https://github.com/WANGBohaO-jpg/MSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-06T00:00:00Z">2025-06-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infinity Search: Approximate Vector Search with Projections on q-Metric
  Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Pariente, Ignacio Hounie, Santiago Segarra, Alejandro Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing
  Text Selection in High School Literature through Knowledge Graph-Based
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirmal Gelal, Chloe Snow, Ambyr Rios, Hande Küçük McGinty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The implementation of transformational pedagogy in secondary education
classrooms requires a broad multiliteracy approach. Due to limited planning
time and resources, high school English Literature teachers often struggle to
curate diverse, thematically aligned literature text sets. This study addresses
the critical need for a tool that provides scaffolds for novice educators in
selecting literature texts that are diverse -- in terms of genre, theme,
subtheme, and author -- yet similar in context and pedagogical merits. We have
developed a recommendation system, Teaching Text Expansion for Teacher
Scaffolding (T-TExTS), that suggests high school English Literature books based
on pedagogical merits, genre, and thematic relevance using a knowledge graph.
We constructed a domain-specific ontology using the KNowledge Acquisition and
Representation Methodology (KNARM), transformed into a knowledge graph, which
was then embedded using DeepWalk, biased random walk, and a hybrid of both
approaches. The system was evaluated using link prediction and recommendation
performance metrics, including Area Under the Curve (AUC), Mean Reciprocal Rank
(MRR), Hits@K, and normalized Discounted Cumulative Gain (nDCG). DeepWalk
outperformed in most ranking metrics, with the highest AUC (0.9431), whereas
the hybrid model offered balanced performance. These findings demonstrate the
importance of semantic, ontology-driven approaches in recommendation systems
and suggest that T-TExTS can significantly ease the burden of English
Literature text selection for high school educators, promoting more informed
and inclusive curricular decisions. The source code for T-TExTS is available
at: https://github.com/koncordantlab/TTExTS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Recall or Relevance? A <span class="highlight-title">Multi-Task</span> Multi-Head Approach for
  Item-to-Item Retrieval in <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Zhang, Sumit Kumar, Wei Chang, Yubo Wang, Feng Zhang, Weize Mao, Hanchao Yu, Aashu Singh, Min Li, Qifan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of item-to-item (I2I) retrieval is to identify a set of relevant and
highly engaging items based on a given trigger item. It is a crucial component
in modern recommendation systems, where users' previously engaged items serve
as trigger items to retrieve relevant content for future engagement. However,
existing I2I retrieval models in industry are primarily built on co-engagement
data and optimized using the recall measure, which overly emphasizes
co-engagement patterns while failing to capture semantic relevance. This often
leads to overfitting short-term co-engagement trends at the expense of
long-term benefits such as discovering novel interests and promoting content
diversity. To address this challenge, we propose MTMH, a Multi-Task and
Multi-Head I2I retrieval model that achieves both high recall and semantic
relevance. Our model consists of two key components: 1) a multi-task learning
loss for formally optimizing the trade-off between recall and semantic
relevance, and 2) a multi-head I2I retrieval architecture for retrieving both
highly co-engaged and semantically relevant items. We evaluate MTMH using
proprietary data from a commercial platform serving billions of users and
demonstrate that it can improve recall by up to 14.4% and semantic relevance by
up to 56.6% compared with prior state-of-the-art models. We also conduct live
experiments to verify that MTMH can enhance both short-term consumption metrics
and long-term user-experience-related metrics. Our work provides a principled
approach for jointly optimizing I2I recall and semantic relevance, which has
significant implications for improving the overall performance of
recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommender systems, stigmergy, and the tyranny of popularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zackary Okun Dunivin, Paul E. Smaldino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific recommender systems, such as Google Scholar and Web of Science,
are essential tools for discovery. Search algorithms that power work through
stigmergy, a collective intelligence mechanism that surfaces useful paths
through repeated engagement. While generally effective, this
``rich-get-richer'' dynamic results in a small number of high-profile papers
that dominate visibility. This essay argues argue that these algorithm
over-reliance on popularity fosters intellectual homogeneity and exacerbates
structural inequities, stifling innovative and diverse perspectives critical
for scientific progress. We propose an overhaul of search platforms to
incorporate user-specific calibration, allowing researchers to manually adjust
the weights of factors like popularity, recency, and relevance. We also advise
platform developers on how word embeddings and LLMs could be implemented in
ways that increase user autonomy. While our suggestions are particularly
pertinent to aligning recommender systems with scientific values, these ideas
are broadly applicable to information access systems in general. Designing
platforms that increase user autonomy is an important step toward more robust
and dynamic information
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wan, Han Wang, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online video web content is richly multimodal: a single video blends vision,
speech, ambient audio, and on-screen text. Retrieval systems typically treat
these modalities as independent retrieval sources, which can lead to noisy and
subpar retrieval. We explore multimodal video content retrieval, where
relevance can be scored from one particular modality or jointly across multiple
modalities simultaneously. Consequently, an effective retriever must
dynamically choose which modality (or set of modalities) best addresses the
query. We introduce CLaMR, a multimodal, late-interaction retriever that
jointly indexes 4 modalities: video frames, transcribed speech, on-screen text,
and metadata. CLaMR jointly encodes all modalities with a unified multimodal
backbone for improved contextualization and is trained to enhance dynamic
modality selection via two key innovations. First, given the lack of training
data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale
synthetic training dataset built on MultiVENT 2.0 (event-centric videos in
various languages paired with queries) with modality-targeted queries. Next, we
propose a modality-aware loss that jointly trains according to a standard
contrastive objective alongside an objective for learning correct modality
usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation
strategies, such as averaging similarities for baseline retrievers, degrade
performance by introducing noise from irrelevant modalities. In contrast, CLaMR
consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR
improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4
over the best multi-modality retriever. We illustrate CLaMR's downstream
utility on long-video QA, retrieving relevant frames and obtaining a 3.50%
boost over LanguageBind on Video-MME and 1.42% over dense sampling on
LongVideoBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages. Code and data: https://github.com/meetdavidwan/clamr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phonetically-Augmented Discriminative Rescoring for Voice Search Error
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Van Gysel, Maggie Wu, Lyan Verwimp, Caglar Tirkaz, Marco Bertola, Zhihong Lei, Youssef Oualil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using
paired audio-text samples that are expensive to obtain, since high-quality
ground-truth data requires human annotators. Voice search applications, such as
digital media players, leverage ASR to allow users to search by voice as
opposed to an on-screen keyboard. However, recent or infrequent movie titles
may not be sufficiently represented in the E2E ASR system's training data, and
hence, may suffer poor recognition.
  In this paper, we propose a phonetic correction system that consists of (a) a
phonetic search based on the ASR model's output that generates phonetic
alternatives that may not be considered by the E2E system, and (b) a rescorer
component that combines the ASR model recognition and the phonetic
alternatives, and select a final system output.
  We find that our approach improves word error rate between 4.4 and 7.6%
relative on benchmarks of popular movie titles over a series of competitive
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at Interspeech '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel, Human-in-the-Loop Computational Grounded Theory Framework for
  Big Social Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lama Alqazlan, Zheng Fang, Michael Castelle, Rob Procter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of big data has significantly influenced the possibilities
and methodological choices for conducting large-scale behavioural and social
science research. In the context of qualitative data analysis, a major
challenge is that conventional methods require intensive manual labour and are
often impractical to apply to large datasets. One effective way to address this
issue is by integrating emerging computational methods to overcome scalability
limitations. However, a critical concern for researchers is the trustworthiness
of results when Machine Learning (ML) and Natural Language Processing (NLP)
tools are used to analyse such data. We argue that confidence in the
credibility and robustness of results depends on adopting a 'human-in-the-loop'
methodology that is able to provide researchers with control over the
analytical process, while retaining the benefits of using ML and NLP. With this
in mind, we propose a novel methodological framework for Computational Grounded
Theory (CGT) that supports the analysis of large qualitative datasets, while
maintaining the rigour of established Grounded Theory (GT) methodologies. To
illustrate the framework's value, we present the results of testing it on a
dataset collected from Reddit in a study aimed at understanding tutors'
experiences in the gig economy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Merits of <span class="highlight-title">LLM</span>-Based Corpus Enrichment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gal Zur, Tommy Mordo, Moshe Tennenholtz, Oren Kurland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (genAI) technologies -- specifically, large language models
(LLMs) -- and search have evolving relations. We argue for a novel perspective:
using genAI to enrich a document corpus so as to improve query-based retrieval
effectiveness. The enrichment is based on modifying existing documents or
generating new ones. As an empirical proof of concept, we use LLMs to generate
documents relevant to a topic which are more retrievable than existing ones. In
addition, we demonstrate the potential merits of using corpus enrichment for
retrieval augmented generation (RAG) and answer attribution in question
answering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Respecting Temporal-Causal Consistency: Entity-Event Knowledge Graphs
  for Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Yu Zhang, Zitao Li, Yaliang Li, Bolin Ding, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) based on large language models often
falters on narrative documents with inherent temporal structures. Standard
unstructured RAG methods rely solely on embedding-similarity matching and lack
any general mechanism to encode or exploit chronological information, while
knowledge graph RAG (KG-RAG) frameworks collapse every mention of an entity
into a single node, erasing the evolving context that drives many queries. To
formalize this challenge and draw the community's attention, we construct
ChronoQA, a robust and discriminative QA benchmark that measures temporal,
causal, and character consistency understanding in narrative documents (e.g.,
novels) under the RAG setting. We then introduce Entity-Event RAG (E^2RAG), a
dual-graph framework that keeps separate entity and event subgraphs linked by a
bipartite mapping, thereby preserving the temporal and causal facets needed for
fine-grained reasoning. Across ChronoQA, our approach outperforms
state-of-the-art unstructured and KG-based RAG baselines, with notable gains on
causal and character consistency queries. E^2RAG therefore offers a practical
path to more context-aware retrieval for tasks that require precise answers
grounded in chronological information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NetMob25 Dataset: A High-resolution Multi-layered View of Individual
  Mobility in Greater Paris Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Chasse, Anne J. Kouam, Aline C. Viana, Razvan Stanica, Wellington V. Lobato, Geymerson Ramos, Geoffrey Deperle, Abdelmounaim Bouroudi, Suzanne Bussod, Fernando Molano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality mobility data remains scarce despite growing interest from
researchers and urban stakeholders in understanding individual-level movement
patterns. The Netmob25 Data Challenge addresses this gap by releasing a unique
GPS-based mobility dataset derived from the EMG 2023 GNSS-based mobility survey
conducted in the Ile-de-France region (Greater Paris area), France. This
dataset captures detailed daily mobility over a full week for 3,337 volunteer
residents aged 16 to 80, collected between October 2022 and May 2023. Each
participant was equipped with a dedicated GPS tracking device configured to
record location points every 2-3 seconds and was asked to maintain a digital or
paper logbook of their trips. All inferred mobility traces were algorithmically
processed and validated through follow-up phone interviews.
  The dataset includes three components: (i) an Individuals database describing
demographic, socioeconomic, and household characteristics; (ii) a Trips
database with over 80,000 annotated displacements including timestamps,
transport modes, and trip purposes; and (iii) a Raw GPS Traces database
comprising about 500 million high-frequency points. A statistical weighting
mechanism is provided to support population-level estimates. An extensive
anonymization pipeline was applied to the GPS traces to ensure GDPR compliance
while preserving analytical value. Access to the dataset requires acceptance of
the challenge's Terms and Conditions and signing a Non-Disclosure Agreement.
This paper describes the survey design, collection protocol, processing
methodology, and characteristics of the released dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Personalized Financial Product <span class="highlight-title">Recommendation</span> by Integrating
  <span class="highlight-title">Large Language Model</span>s and Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushang Zhao, Yike Peng, Dannier Li, Yuxin Yang, Chengrui Zhou, Jing Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of fintech, personalized financial product
recommendations have become increasingly important. Traditional methods like
collaborative filtering or content-based models often fail to capture users'
latent preferences and complex relationships. We propose a hybrid framework
integrating large language models (LLMs) and graph neural networks (GNNs). A
pre-trained LLM encodes text data (e.g., user reviews) into rich feature
vectors, while a heterogeneous user-product graph models interactions and
social ties. Through a tailored message-passing mechanism, text and graph
information are fused within the GNN to jointly optimize embeddings.
Experiments on public and real-world financial datasets show our model
outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong
interpretability. This work offers new insights for personalized financial
recommendations and cross-modal fusion in broader recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Long Semantic IDs in Parallel for <span class="highlight-title">Recommendation</span> <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Hou, Jiacheng Li, Ashley Shin, Jinsung Jeon, Abhishek Santhanam, Wei Shao, Kaveh Hassani, Ning Yao, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic ID-based recommendation models tokenize each item into a small
number of discrete tokens that preserve specific semantics, leading to better
performance, scalability, and memory efficiency. While recent models adopt a
generative approach, they often suffer from inefficient inference due to the
reliance on resource-intensive beam search and multiple forward passes through
the neural sequence model. As a result, the length of semantic IDs is typically
restricted (e.g. to just 4 tokens), limiting their expressiveness. To address
these challenges, we propose RPG, a lightweight framework for semantic ID-based
recommendation. The key idea is to produce unordered, long semantic IDs,
allowing the model to predict all tokens in parallel. We train the model to
predict each token independently using a multi-token prediction loss, directly
integrating semantics into the learning objective. During inference, we
construct a graph connecting similar semantic IDs and guide decoding to avoid
generating invalid IDs. Experiments show that scaling up semantic ID length to
64 enables RPG to outperform generative baselines by an average of 12.6% on the
NDCG@10, while also improving inference efficiency. Code is available at:
https://github.com/facebookresearch/RPG_KDD2025.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NGA: Non-autoregressive Generative Auction with Global Externalities for
  Advertising Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuowu Zheng, Ze Wang, Fan Yang, Wenqing Ye, Weihua Huang, Wenqiang He, Teng Zhang, Xingxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online advertising auctions are fundamental to internet commerce, demanding
solutions that not only maximize revenue but also ensure incentive
compatibility, high-quality user experience, and real-time efficiency. While
recent learning-based auction frameworks have improved context modeling by
capturing intra-list dependencies among ads, they remain limited in addressing
global externalities and often suffer from inefficiencies caused by sequential
processing. In this work, we introduce the Non-autoregressive Generative
Auction with global externalities (NGA), a novel end-to-end framework designed
for industrial online advertising. NGA explicitly models global externalities
by jointly capturing the relationships among ads as well as the effects of
adjacent organic content. To further enhance efficiency, NGA utilizes a
non-autoregressive, constraint-based decoding strategy and a parallel
multi-tower evaluator for unified list-wise reward and payment computation.
Extensive offline experiments and large-scale online A/B testing on commercial
advertising platforms demonstrate that NGA consistently outperforms existing
methods in both effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bioptic B1: A Target-Agnostic Potency-Based Small Molecules Search
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14572v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14572v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Vinogradov, Ivan Izmailov, Simon Steshin, Kong T. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent successes in virtual screening have been made possible by large models
and extensive chemical libraries. However, combining these elements is
challenging: the larger the model, the more expensive it is to run, making
ultra-large libraries unfeasible. To address this, we developed a
target-agnostic, efficacy-based molecule search model, which allows us to find
structurally dissimilar molecules with similar biological activities. We used
the best practices to design fast retrieval system, based on
processor-optimized SIMD instructions, enabling us to screen the ultra-large
40B Enamine REAL library with 100\% recall rate. We extensively benchmarked our
model and several state-of-the-art models for both speed performance and
retrieval quality of novel molecules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNAnatomy: Rethinking Model-Level Explanations for Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiao-Ying Lu, Yiran Li, Ujwal Pratap Krishna Kaluvakolanu Thyagarajan, Kwan-Liu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) achieve outstanding performance across
graph-based tasks but remain difficult to interpret. In this paper, we revisit
foundational assumptions underlying model-level explanation methods for GNNs,
namely: (1) maximizing classification confidence yields representative
explanations, (2) a single explanation suffices for an entire class of graphs,
and (3) explanations are inherently trustworthy. We identify pitfalls resulting
from these assumptions: methods that optimize for classification confidence may
overlook partially learned patterns; topological diversity across graph subsets
within the same class is often underrepresented; and explanations alone offer
limited support for building user trust when applied to new datasets or models.
This paper introduces GNNAnatomy, a distillation-based method designed to
generate explanations while avoiding these pitfalls. GNNAnatomy first
characterizes graph topology using graphlets, a set of fundamental
substructures. We then train a transparent multilayer perceptron surrogate to
directly approximate GNN predictions based on the graphlet representations. By
analyzing the weights assigned to each graphlet, we identify the most
discriminative topologies, which serve as GNN explanations. To account for
structural diversity within a class, GNNAnatomy generates explanations at the
required granularity through an interface that supports human-AI teaming. This
interface helps users identify subsets of graphs where distinct critical
substructures drive class differentiation, enabling multi-grained explanations.
Additionally, by enabling exploration and linking explanations back to input
graphs, the interface fosters greater transparency and trust. We evaluate
GNNAnatomy on both synthetic and real-world datasets through quantitative
metrics and qualitative comparisons with state-of-the-art model-level
explainable GNN methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quake: Adaptive Indexing for Vector Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Mohoney, Devesh Sarda, Mengze Tang, Shihabur Rahman Chowdhury, Anil Pacaci, Ihab F. Ilyas, Theodoros Rekatsinas, Shivaram Venkataraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector search, the task of finding the k-nearest neighbors of a query vector
against a database of high-dimensional vectors, underpins many machine learning
applications, including retrieval-augmented generation, recommendation systems,
and information retrieval. However, existing approximate nearest neighbor (ANN)
methods perform poorly under dynamic and skewed workloads where data
distributions evolve. We introduce Quake, an adaptive indexing system that
maintains low latency and high recall in such environments. Quake employs a
multi-level partitioning scheme that adjusts to updates and changing access
patterns, guided by a cost model that predicts query latency based on partition
sizes and access frequencies. Quake also dynamically sets query execution
parameters to meet recall targets using a novel recall estimation model.
Furthermore, Quake utilizes NUMA-aware intra-query parallelism for improved
memory bandwidth utilization during search. To evaluate Quake, we prepare a
Wikipedia vector search workload and develop a workload generator to create
vector search workloads with configurable access patterns. Our evaluation shows
that on dynamic workloads, Quake achieves query latency reductions of 1.5-38x
and update latency reductions of 4.5-126x compared to state-of-the-art indexes
such as SVS, DiskANN, HNSW, and SCANN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In a Few Words: Comparing Weak Supervision and <span class="highlight-title">LLM</span>s for Short Query
  Intent Classification <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.21398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.21398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Alexander, Arjen P. de Vries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User intent classification is an important task in information retrieval.
Previously, user intents were classified manually and automatically; the latter
helped to avoid hand labelling of large datasets. Recent studies explored
whether LLMs can reliably determine user intent. However, researchers have
recognized the limitations of using generative LLMs for classification tasks.
In this study, we empirically compare user intent classification into
informational, navigational, and transactional categories, using weak
supervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and
LLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for
fine-tuning, comparing their performance to an established baseline classifier
trained using weak supervision (ORCAS-I). Our results indicate that while LLMs
outperform weak supervision in recall, they continue to struggle with
precision, which shows the need for improved methods to balance both metrics
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at International ACM SIGIR Conference on Research and
  Development in Information Retrieval (SIGIR '25), July 13--18, 2025, Padua,
  Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ User Altruism in <span class="highlight-title">Recommendation</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Fedorova, Madeline Kitch, Chara Podimata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users of social media platforms based on recommendation systems (RecSys)
(e.g. TikTok, X, YouTube) strategically interact with platform content to
influence future recommendations. On some such platforms, users have been
documented to form large-scale grassroots movements encouraging others to
purposefully interact with algorithmically suppressed content in order to
"boost" its recommendation; we term this behavior user altruism. To capture
this behavior, we study a game between users and a RecSys, where users provide
the RecSys (potentially manipulated) preferences over the contents available to
them, and the RecSys -- limited by data and computation constraints -- creates
a low-rank approximation preference matrix, and ultimately provides each user
her (approximately) most-preferred item. We compare the users' social welfare
under truthful preference reporting and under a class of strategies capturing
user altruism. In our theoretical analysis, we provide sufficient conditions to
ensure strict increases in user social welfare under user altruism, and provide
an algorithm to find an effective altruistic strategy. Interestingly, we show
that for commonly assumed recommender utility functions, effectively altruistic
strategies also improve the utility of the RecSys! We show that our results are
robust to several model misspecifications, thus strengthening our conclusions.
Our theoretical analysis is complemented by empirical results of effective
altruistic strategies on the GoodReads dataset, and an online survey on how
real-world users behave altruistically in RecSys. Overall, our findings serve
as a proof-of-concept of the reasons why traditional RecSys may incentivize
users to form collectives and/or follow altruistic strategies when interacting
with them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context is Gold to find the Gold Passage: Evaluating and Training
  Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Conti, Manuel Faysse, Gautier Viaud, Antoine Bosselut, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A limitation of modern document retrieval embedding methods is that they
typically encode passages (chunks) from the same documents independently, often
overlooking crucial contextual information from the rest of the document that
could greatly improve individual chunk representations.
  In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a
benchmark designed to evaluate retrieval models on their ability to leverage
document-wide context. Our results show that state-of-the-art embedding models
struggle in retrieval scenarios where context is required. To address this
limitation, we propose InSeNT (In-sequence Negative Training), a novel
contrastive post-training approach which combined with late chunking pooling
enhances contextual representation learning while preserving computational
efficiency. Our method significantly improves retrieval quality on ConTEB
without sacrificing base model performance. We further find chunks embedded
with our method are more robust to suboptimal chunking strategies and larger
retrieval corpus sizes. We open-source all artifacts at
https://github.com/illuin-tech/contextual-embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Structure of Financial Equity Research Reports -- Identification of
  the Most Frequently Asked Questions in Financial Analyst Reports to Automate
  Equity Research Using Llama 3 and <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adria Pop, Jan Spörer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research dissects financial equity research reports (ERRs) by mapping
their content into categories. There is insufficient empirical analysis of the
questions answered in ERRs. In particular, it is not understood how frequently
certain information appears, what information is considered essential, and what
information requires human judgment to distill into an ERR. The study analyzes
72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique
question archetypes. We did not predefine the questions but derived them solely
from the statements in the ERRs. This approach provides an unbiased view of the
content of the observed ERRs. Subsequently, we used public corporate reports to
classify the questions' potential for automation. Answers were labeled
"text-extractable" if the answers to the question were accessible in corporate
reports. 78.7% of the questions in ERRs can be automated. Those automatable
question consist of 48.2% text-extractable (suited to processing by large
language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of
questions require human judgment to answer. We empirically validate using
Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language
generation and information extraction enable the automation of approximately
80% of the statements in ERRs. Surprisingly, the models complement each other's
strengths and weaknesses well. The research confirms that the current writing
process of ERRs can likely benefit from additional automation, improving
quality and efficiency. The research thus allows us to quantify the potential
impacts of introducing large language models in the ERR writing process. The
full question list, including the archetypes and their frequency, will be made
available online after peer review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JEL classes: C45; G11; G12; G14</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECoRAG: Evidentiality-guided Compression for Long Context RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonseok Jeong, Jinsu Kim, Dohyeon Lee, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable performance in Open-Domain
Question Answering (ODQA) by leveraging external documents through
Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer
context, context compression is necessary. However, prior compression methods
do not focus on filtering out non-evidential information, which limit the
performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or
ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved
documents based on evidentiality, ensuring whether answer generation is
supported by the correct evidence. As an additional step, ECoRAG reflects
whether the compressed content provides sufficient evidence, and if not,
retrieves more until sufficient. Experiments show that ECoRAG improves LLM
performance on ODQA tasks, outperforming existing compression methods.
Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency
but also minimizes token usage by retaining only the necessary information to
generate the correct answer. Code is available at
https://github.com/ldilab/ECoRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGE: A Framework of Precise Retrieval for RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Guoliang Li, Jinyang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has demonstrated significant proficiency
in conducting question-answering (QA) tasks within a specified corpus.
Nonetheless, numerous failure instances of RAG in QA still exist. These
failures are not solely attributable to the limitations of Large Language
Models (LLMs); instead, they predominantly arise from the retrieval of
inaccurate information for LLMs due to two limitations: (1) Current RAG methods
segment the corpus without considering semantics, making it difficult to find
relevant context due to impaired correlation between questions and the
segments. (2) There is a trade-off between missing essential context with fewer
context retrieved and getting irrelevant context with more context retrieved.
  In this paper, we introduce a RAG framework (SAGE), to overcome these
limitations. First, to address the segmentation issue without considering
semantics, we propose to train a semantic segmentation model. This model is
trained to segment the corpus into semantically complete chunks. Second, to
ensure that only the most relevant chunks are retrieved while the irrelevant
ones are ignored, we design a chunk selection algorithm to dynamically select
chunks based on the decreasing speed of the relevance score, leading to a more
relevant selection. Third, to further ensure the precision of the retrieved
chunks, we propose letting LLMs assess whether retrieved chunks are excessive
or lacking and then adjust the amount of context accordingly. Experiments show
that SAGE outperforms baselines by 61.25% in the quality of QA on average.
Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the
tokens consumed in LLM inference and achieves a 49.41% enhancement in cost
efficiency on average. Additionally, our work offers valuable insights for
boosting RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActionPiece: Contextually Tokenizing Action Sequences for Generative
  <span class="highlight-title">Recommendation</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation (GR) is an emerging paradigm where user actions are
tokenized into discrete token patterns and autoregressively generated as
predictions. However, existing GR models tokenize each action independently,
assigning the same fixed tokens to identical actions across all sequences
without considering contextual relationships. This lack of context-awareness
can lead to suboptimal performance, as the same action may hold different
meanings depending on its surrounding context. To address this issue, we
propose ActionPiece to explicitly incorporate context when tokenizing action
sequences. In ActionPiece, each action is represented as a set of item
features. Given the action sequence corpora, we construct the vocabulary by
merging feature patterns as new tokens, based on their co-occurrence frequency
both within individual sets and across adjacent sets. Considering the unordered
nature of feature sets, we further introduce set permutation regularization,
which produces multiple segmentations of action sequences with the same
semantics. Our code is available at:
https://github.com/google-deepmind/action_piece.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinSage: A Multi-aspect RAG System for Financial Filings Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Jijun Chi, Zhenghan Tai, Tung Sum Thomas Kwok, Muzhi Li, Zhuhong Li, Hailin He, Yuchen Hua, Peng Lu, Suyuchen Wang, Yihong Wu, Jerry Huang, Jingrui Tian, Fengran Mo, Yufei Cui, Ling Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models in real-world settings often entails a need
to utilize domain-specific data and tools in order to follow the complex
regulations that need to be followed for acceptable use. Within financial
sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation
(RAG) systems to address complex compliance requirements in financial document
workflows. However, existing solutions struggle to account for the inherent
heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of
regulatory standards used in financial filings, leading to compromised accuracy
in critical information extraction. We propose the FinSage framework as a
solution, utilizing a multi-aspect RAG framework tailored for regulatory
compliance analysis in multi-modal financial documents. FinSage introduces
three innovative components: (1) a multi-modal pre-processing pipeline that
unifies diverse data formats and generates chunk-level metadata summaries, (2)
a multi-path sparse-dense retrieval system augmented with query expansion
(HyDE) and metadata-aware semantic search, and (3) a domain-specialized
re-ranking module fine-tuned via Direct Preference Optimization (DPO) to
prioritize compliance-critical content. Extensive experiments demonstrate that
FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions
derived from surpasses the best baseline method on the FinanceBench question
answering datasets by 24.06% in accuracy. Moreover, FinSage has been
successfully deployed as financial question-answering agent in online meetings,
where it has already served more than 1,200 people.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoIR: A Comprehensive Benchmark for Code Information Retrieval Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02883v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02883v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the substantial success of Information Retrieval (IR) in various NLP
tasks, most IR systems predominantly handle queries and corpora in natural
language, neglecting the domain of code retrieval. Code retrieval is critically
important yet remains under-explored, with existing methods and benchmarks
inadequately representing the diversity of code in various domains and tasks.
Addressing this gap, we present COIR (Code Information Retrieval Benchmark), a
robust and comprehensive benchmark specifically designed to assess code
retrieval capabilities. COIR comprises ten meticulously curated code datasets,
spanning eight distinctive retrieval tasks across seven diverse domains. We
first discuss the construction of COIR and its diverse dataset composition.
Further, we evaluate nine widely used retrieval models using COIR, uncovering
significant difficulties in performing code retrieval tasks even with
state-of-the-art systems. To facilitate easy adoption and integration within
existing research workflows, COIR has been developed as a user-friendly Python
framework, readily installable via pip. It shares same data schema as other
popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark
evaluations. Through COIR, we aim to invigorate research in the code retrieval
domain, providing a versatile benchmarking tool that encourages further
development and exploration of code retrieval systems.
https://github.com/CoIR-team/coir.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-based relevance assessment still can't replace human relevance
  assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles L. A. Clarke, Laura Dietz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of large language models (LLMs) for relevance assessment in
information retrieval has gained significant attention, with recent studies
suggesting that LLM-based judgments provide comparable evaluations to human
judgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim
that LLM-based relevance assessments, such as those generated by the UMBRELA
system, can fully replace traditional human relevance assessments in TREC-style
evaluations. This paper critically examines this claim, highlighting practical
and theoretical limitations that undermine the validity of this conclusion.
First, we question whether the evidence provided by Upadhyay et al. really
supports their claim, particularly if a test collection is used asa benchmark
for future improvements. Second, through a submission deliberately intended to
do so, we demonstrate the ease with which automatic evaluation metrics can be
subverted, showing that systems designed to exploit these evaluations can
achieve artificially high scores. Theoretical challenges -- such as the
inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and
the potential degradation of future LLM performance -- must be addressed before
LLM-based relevance assessments can be considered a viable replacement for
human judgments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in "11th International Workshop on Evaluating Information
  Access (EVIA 2025)"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo Relevance Feedback is Enough to Close the Gap Between Small and
  Large Dense Retrieval Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Li, Xiao Wang, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling dense retrievers to larger large language model (LLM) backbones has
been a dominant strategy for improving their retrieval effectiveness. However,
this has substantial cost implications: larger backbones require more expensive
hardware (e.g. GPUs with more memory) and lead to higher indexing and querying
costs (latency, energy consumption). In this paper, we challenge this paradigm
by introducing PromptPRF, a feature-based pseudo-relevance feedback (PRF)
framework that enables small LLM-based dense retrievers to achieve
effectiveness comparable to much larger models.
  PromptPRF uses LLMs to extract query-independent, structured and unstructured
features (e.g., entities, summaries, chain-of-thought keywords, essay) from
top-ranked documents. These features are generated offline and integrated into
dense query representations via prompting, enabling efficient retrieval without
additional training. Unlike prior methods such as GRF, which rely on online,
query-specific generation and sparse retrieval, PromptPRF decouples feedback
generation from query processing and supports dense retrievers in a fully
zero-shot setting.
  Experiments on TREC DL and BEIR benchmarks demonstrate that PromptPRF
consistently improves retrieval effectiveness and offers favourable
cost-effectiveness trade-offs. We further present ablation studies to
understand the role of positional feedback and analyse the interplay between
feature extractor size, PRF depth, and model performance. Our findings
demonstrate that with effective PRF design, scaling the retriever is not always
necessary, narrowing the gap between small and large models while reducing
inference cost.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over
  Data Lakes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugenie Lai, Gerardo Vitagliano, Ziyu Zhang, Sivaprasad Sudhir, Om Chabra, Anna Zeng, Anton A. Zabreyko, Chenning Li, Ferdi Kossmann, Jialin Ding, Jun Chen, Markos Markakis, Matthew Russo, Weiyang Wang, Ziniu Wu, Michael J. Cafarella, Lei Cao, Samuel Madden, Tim Kraska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stream DaQ: Stream-First Data Quality Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Papastergios, Anastasios Gounaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data quality is fundamental to modern data science workflows, where data
continuously flows as unbounded streams feeding critical downstream tasks, from
elementary analytics to advanced artificial intelligence models. Existing data
quality approaches either focus exclusively on static data or treat streaming
as an extension of batch processing, lacking the temporal granularity and
contextual awareness required for true streaming applications. In this paper,
we present a novel data quality monitoring model specifically designed for
unbounded data streams. Our model introduces stream-first concepts, such as
configurable windowing mechanisms, dynamic constraint adaptation, and
continuous assessment that produces quality meta-streams for real-time pipeline
awareness. To demonstrate practical applicability, we developed Stream DaQ, an
open-source Python framework that implements our theoretical model. Stream DaQ
unifies and adapts over 30 quality checks fragmented across existing static
tools into a comprehensive streaming suite, enabling practitioners to define
sophisticated, context-aware quality constraints through compositional
expressiveness. Our evaluation demonstrates that the model's implementation
significantly outperforms a production-grade alternative in both execution time
and throughput while offering richer functionality via native streaming
capabilities compared to other choices. Through its Python-native design,
Stream DaQ seamlessly integrates with modern data science workflows, making
continuous quality monitoring accessible to the broader data science community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Explanations for Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Gilad, Tova Milo, Kathy Razmadze, Ron Zadicario
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dire need to protect sensitive data has led to various flavors of privacy
definitions. Among these, Differential privacy (DP) is considered one of the
most rigorous and secure notions of privacy, enabling data analysis while
preserving the privacy of data contributors. One of the fundamental tasks of
data analysis is clustering , which is meant to unravel hidden patterns within
complex datasets. However, interpreting clustering results poses significant
challenges, and often necessitates an extensive analytical process.
Interpreting clustering results under DP is even more challenging, as analysts
are provided with noisy responses to queries, and longer, manual exploration
sessions require additional noise to meet privacy constraints. While increasing
attention has been given to clustering explanation frameworks that aim at
assisting analysts by automatically uncovering the characteristics of each
cluster, such frameworks may also disclose sensitive information within the
dataset, leading to a breach in privacy. To address these challenges, we
present DPClustX, a framework that provides explanations for black-box
clustering results while satisfying DP. DPClustX takes as input the sensitive
dataset alongside privately computed clustering labels, and outputs a global
explanation, emphasizing prominent characteristics of each cluster while
guaranteeing DP. We perform an extensive experimental analysis of DPClustX on
real data, showing that it provides insightful and accurate explanations even
under tight privacy constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Query Optimization via <span class="highlight-title">LLM</span>-Based Plan Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Vasilenko, Alexander Demin, Vladimir Boorlakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) embeddings offer a promising new avenue for
database query optimization. In this paper, we explore how pre-trained
execution plan embeddings can guide SQL query execution without the need for
additional model training. We introduce LLM-PM (LLM-based Plan Mapping), a
framework that embeds the default execution plan of a query, finds its k
nearest neighbors among previously executed plans, and recommends database
hintsets based on neighborhood voting. A lightweight consistency check
validates the selected hint, while a fallback mechanism searches the full hint
space when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM
achieves an average speed-up of 21% query latency reduction. This work
highlights the potential of LLM-powered embeddings to deliver practical
improvements in query performance and opens new directions for training-free,
embedding-based optimizer guidance systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RelGNN: Composite Message Passing for Relational Deep Learning <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlang Chen, Charilaos Kanatsoulis, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive tasks on relational databases are critical in real-world
applications spanning e-commerce, healthcare, and social media. To address
these tasks effectively, Relational Deep Learning (RDL) encodes relational data
as graphs, enabling Graph Neural Networks (GNNs) to exploit relational
structures for improved predictions. However, existing RDL methods often
overlook the intrinsic structural properties of the graphs built from
relational databases, leading to modeling inefficiencies, particularly in
handling many-to-many relationships. Here we introduce RelGNN, a novel GNN
framework specifically designed to leverage the unique structural
characteristics of the graphs built from relational databases. At the core of
our approach is the introduction of atomic routes, which are simple paths that
enable direct single-hop interactions between the source and destination nodes.
Building upon these atomic routes, RelGNN designs new composite message passing
and graph attention mechanisms that reduce redundancy, highlight key signals,
and enhance predictive accuracy. RelGNN is evaluated on 30 diverse real-world
tasks from Relbench (Fey et al., 2024), and achieves state-of-the-art
performance on the vast majority of tasks, with improvements of up to 25%. Code
is available at https://github.com/snap-stanford/RelGNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures, 7 tables. ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PathFinder: A unified approach for handling paths in graph query
  languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02194v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02194v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamín Farías, Wim Martens, Carlos Rojas, Domagoj Vrgoč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path queries are a core feature of modern graph query languages such as
Cypher, SQL/PGQ, and GQL. These languages provide a rich set of features for
matching paths, such as restricting to certain path modes (shortest, simple,
trail) and constraining the edge labels along the path by a regular expression.
In this paper we present PathFinder, a unifying approach for dealing with path
queries in all these query languages. PathFinder leverages a compact
representation of the (potentially exponential number of) paths that can match
a given query, extends it with pipelined execution, and supports all commonly
used path modes. In the paper we describe the algorithmic backbone of
PathFinder, provide a reference implementation, and test it over a large set of
real-world queries and datasets. Our results show that PathFinder exhibits very
stable behavior, even on large data and complex queries, and its performance is
an order of magnitude better than that of many modern graph engines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking OWL Expressi<span class="highlight-title">vit</span>y: Semantic Units for FAIR and Cognitively
  Interoperable Knowledge Graphs Why OWLs don't have to understand everything
  they say 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic knowledge graphs are foundational to implementing the FAIR
Principles, yet RDF/OWL representations often lack the semantic flexibility and
cognitive interoperability required in scientific domains. We present a novel
framework for semantic modularization based on semantic units (i.e., modular,
semantically coherent subgraphs enhancing expressivity, reusability, and
interpretability), combined with four new representational resource types
(some-instance, most-instances, every-instance, all-instances) for modelling
assertional, contingent, prototypical, and universal statements. The framework
enables the integration of knowledge modelled using different logical
frameworks (e.g., OWL, First-Order Logic, or none), provided each semantic unit
is internally consistent and annotated with its logic base. This allows, for
example, querying all OWL 2.0-compliant units for reasoning purposes while
preserving the full graph for broader knowledge discovery. Our framework
addresses twelve core limitations of OWL/RDF modeling, including negation,
cardinality, complex class axioms, conditional and directive statements, and
logical arguments, while improving cognitive accessibility for domain experts.
We provide schemata and translation patterns to demonstrate semantic
interoperability and reasoning potential, establishing a scalable foundation
for constructing FAIR-aligned, semantically rich knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2301.01227</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic
  Interpretability Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12730v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12730v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abir Harrasse, Philip Quirke, Clement Neo, Dhruv Nathawani, Luke Marks, Amir Abdullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanistic interpretability research faces a gap between analyzing simple
circuits in toy tasks and discovering features in large models. To bridge this
gap, we propose text-to-SQL generation as an ideal task to study, as it
combines the formal structure of toy tasks with real-world complexity. We
introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL
operations, and train models ranging from 33M to 1B parameters to establish a
comprehensive testbed for interpretability. We apply multiple complementary
interpretability techniques, including Edge Attribution Patching and Sparse
Autoencoders, to identify minimal circuits and components supporting SQL
generation. We compare circuits for different SQL subskills, evaluating their
minimality, reliability, and identifiability. Finally, we conduct a layerwise
logit lens analysis to reveal how models compose SQL queries across layers:
from intent recognition to schema resolution to structured generation. Our work
provides a robust framework for probing and comparing interpretability methods
in a structured, progressively complex setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 19 figures, 7 tables, 18 trained models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGE: A Framework of Precise Retrieval for RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Guoliang Li, Jinyang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has demonstrated significant proficiency
in conducting question-answering (QA) tasks within a specified corpus.
Nonetheless, numerous failure instances of RAG in QA still exist. These
failures are not solely attributable to the limitations of Large Language
Models (LLMs); instead, they predominantly arise from the retrieval of
inaccurate information for LLMs due to two limitations: (1) Current RAG methods
segment the corpus without considering semantics, making it difficult to find
relevant context due to impaired correlation between questions and the
segments. (2) There is a trade-off between missing essential context with fewer
context retrieved and getting irrelevant context with more context retrieved.
  In this paper, we introduce a RAG framework (SAGE), to overcome these
limitations. First, to address the segmentation issue without considering
semantics, we propose to train a semantic segmentation model. This model is
trained to segment the corpus into semantically complete chunks. Second, to
ensure that only the most relevant chunks are retrieved while the irrelevant
ones are ignored, we design a chunk selection algorithm to dynamically select
chunks based on the decreasing speed of the relevance score, leading to a more
relevant selection. Third, to further ensure the precision of the retrieved
chunks, we propose letting LLMs assess whether retrieved chunks are excessive
or lacking and then adjust the amount of context accordingly. Experiments show
that SAGE outperforms baselines by 61.25% in the quality of QA on average.
Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the
tokens consumed in LLM inference and achieves a 49.41% enhancement in cost
efficiency on average. Additionally, our work offers valuable insights for
boosting RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Common Data Format (CDF): A Standardized Format for Match-Data in
  Football (Soccer) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Anzer, Kilian Arnsmeyer, Pascal Bauer, Joris Bekkers, Ulf Brefeld, Jesse Davis, Nicolas Evans, Matthias Kempe, Samuel J Robertson, Joshua Wyatt Smith, Jan Van Haaren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During football matches, a variety of different parties (e.g., companies)
each collect (possibly overlapping) data about the match ranging from basic
information (e.g., starting players) to detailed positional data. This data is
provided to clubs, federations, and other organizations who are increasingly
interested in leveraging this data to inform their decision making.
Unfortunately, analyzing such data pose significant barriers because each
provider may (1) collect different data, (2) use different specifications even
within the same category of data, (3) represent the data differently, and (4)
delivers the data in a different manner (e.g., file format, protocol).
Consequently, working with these data requires a significant investment of time
and money. The goal of this work is to propose a uniform and standardized
format for football data called the Common Data Format (CDF). The CDF specifies
a minimal schema for five types of match data: match sheet data, video footage,
event data, tracking data, and match meta data. It aims to ensure that the
provided data is clear, sufficiently contextualized (e.g., its provenance is
clear), and complete such that it enables common downstream analysis tasks.
Concretely, this paper will detail the technical specifications of the CDF, the
representational choices that were made to help ensure the clarity of the
provided data, and a concrete approach for delivering data in the CDF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided <span class="highlight-title">Large Language Model</span> for SQL Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Hao Chen, Junnan Dong, Shengyuan Chen, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown promise in
bridging the gap between natural language queries and database management
systems, enabling users to interact with databases without the background of
SQL. However, LLMs often struggle to comprehend complex database structures and
accurately interpret user intentions. Decomposition-based methods have been
proposed to enhance the performance of LLMs on complex tasks, but decomposing
SQL generation into subtasks is non-trivial due to the declarative structure of
SQL syntax and the intricate connections between query concepts and database
elements. In this paper, we propose a novel Structure GUided text-to-SQL
framework~(SGU-SQL) that incorporates syntax-based prompting to enhance the SQL
generation capabilities of LLMs. Specifically, SGU-SQL establishes
structure-aware links between user queries and database schema and decomposes
the complex generation task using syntax-based prompting to enable more
accurate LLM-based SQL generation. Extensive experiments on two benchmark
datasets demonstrate that SGU-SQL consistently outperforms state-of-the-art
text-to-SQL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 42nd International Conference on Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-05T00:00:00Z">2025-06-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Sequel-Aware Graph Neural Networks for Sequential Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anushka Tiwari, Haimonti Dutta, Shahrzad Khanizadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based recommendation systems use higher-order user and item embeddings
for next-item predictions. Dynamically adding collaborative signals from
neighbors helps to use similar users' preferences during learning. While
item-item correlations and their impact on recommendations have been studied,
the efficacy of temporal item sequences for recommendations is much less
explored. In this paper, we examine temporal item sequence (sequel-aware)
embeddings along with higher-order user embeddings and show that sequel-aware
Graph Neural Networks have better (or comparable) recommendation performance
than graph-based recommendation systems that do not consider sequel
information. Extensive empirical results comparing Heterogeneous Sequel-aware
Graph Neural Networks (HSAL-GNNs) to other algorithms for sequential learning
(such as transformers, graph neural networks, auto-encoders) are presented on
three synthetic and three real-world datasets. Our results indicate that the
incorporation of sequence information from items greatly enhances
recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search Arena: Analyzing Search-Augmented <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code: https://github.com/lmarena/search-arena. Dataset:
  https://huggingface.co/datasets/lmarena-ai/search-arena-24k</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Comprehensibility of Multi-structured Financial Documents using
  <span class="highlight-title">LLM</span>s and Pre-processing Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivani Upadhyay, Messiah Ataey, Shariyar Murtuza, Yifan Nie, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of complex structured data in hybrid sources, such as PDF
documents and web pages, presents unique challenges for current Large Language
Models (LLMs) and Multi-modal Large Language Models (MLLMs) in providing
accurate answers. Despite the recent advancements of MLLMs, they still often
falter when interpreting intricately structured information, such as nested
tables and multi-dimensional plots, leading to hallucinations and erroneous
outputs. This paper explores the capabilities of LLMs and MLLMs in
understanding and answering questions from complex data structures found in PDF
documents by leveraging industrial and open-source tools as part of a
pre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,
achieves an accuracy of 56% on multi-structured documents when fed documents
directly, and that integrating pre-processing tools raises the accuracy of LLMs
to 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is
publicly available at https://github.com/OGCDS/FinancialQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledgeable-r1: Policy Optimization for Knowledge Exploration in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Lin, Yilin Wen, Du Su, Fei Sun, Muhan Chen, Chenfu Bao, Zhonghou Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a mainstream method for improving
performance on knowledge-intensive tasks. However,current RAG systems often
place too much emphasis on retrieved contexts. This can lead to reliance on
inaccurate sources and overlook the model's inherent knowledge, especially when
dealing with misleading or excessive information. To resolve this imbalance, we
propose Knowledgeable-r1 that using joint sampling and define multi policy
distributions in knowledge capability exploration to stimulate large language
models'self-integrated utilization of parametric and contextual knowledge.
Experiments show that Knowledgeable-r1 significantly enhances robustness and
reasoning accuracy in both parameters and contextual conflict tasks and general
RAG tasks, especially outperforming baselines by 17.07% in counterfactual
scenarios and demonstrating consistent gains across RAG tasks. Our code are
available at https://github.com/lcy80366872/ knowledgeable-r1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Contrastive Learning in Session-based <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokun Zhang, Bo Xu, Fenglong Ma, Zhizheng Wang, Liang Yang, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation aims to predict intents of anonymous users based
on limited behaviors. With the ability in alleviating data sparsity,
contrastive learning is prevailing in the task. However, we spot that existing
contrastive learning based methods still suffer from three obstacles: (1) they
overlook item-level sparsity and primarily focus on session-level sparsity; (2)
they typically augment sessions using item IDs like crop, mask and reorder,
failing to ensure the semantic consistency of augmented views; (3) they treat
all positive-negative signals equally, without considering their varying
utility. To this end, we propose a novel multi-modal adaptive contrastive
learning framework called MACL for session-based recommendation. In MACL, a
multi-modal augmentation is devised to generate semantically consistent views
at both item and session levels by leveraging item multi-modal features.
Besides, we present an adaptive contrastive loss that distinguishes varying
contributions of positive-negative signals to improve self-supervised learning.
Extensive experiments on three real-world datasets demonstrate the superiority
of MACL over state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Storage-Efficient Visual Document Retrieval: An Empirical Study
  on Reducing Patch-Level Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Ma, Jinsong Li, Yuhang Zang, Xiaobao Wu, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Jiaqi Wang, Yixin Cao, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the strong performance of ColPali/ColQwen2 in Visualized Document
Retrieval (VDR), it encodes each page into multiple patch-level embeddings and
leads to excessive memory usage. This empirical study investigates methods to
reduce patch embeddings per page at minimum performance degradation. We
evaluate two token-reduction strategies: token pruning and token merging.
Regarding token pruning, we surprisingly observe that a simple random strategy
outperforms other sophisticated pruning methods, though still far from
satisfactory. Further analysis reveals that pruning is inherently unsuitable
for VDR as it requires removing certain page embeddings without query-specific
information. Turning to token merging (more suitable for VDR), we search for
the optimal combinations of merging strategy across three dimensions and
develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance
with only 11.8% of original memory usage, and preserves 94.6% effectiveness at
2.8% memory footprint. We expect our empirical findings and resulting
Light-ColPali/ColQwen2 offer valuable insights and establish a competitive
baseline for future research towards efficient VDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff
  Table 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Matsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor search (ANNS) is an essential building block for
applications like RAG but can sometimes yield results that are overly similar
to each other. In certain scenarios, search results should be similar to the
query and yet diverse. We propose LotusFilter, a post-processing module to
diversify ANNS results. We precompute a cutoff table summarizing vectors that
are close to each other. During the filtering, LotusFilter greedily looks up
the table to delete redundant vectors from the candidates. We demonstrated that
the LotusFilter operates fast (0.02 [ms/query]) in settings resembling
real-world RAG applications, utilizing features such as OpenAI embeddings. Our
code is publicly available at https://github.com/matsui528/lotf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. GitHub: https://github.com/matsui528/lotf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner
  for Query Expansion in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyuan Liu, Mengxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using
  <span class="highlight-title">Large Language Model</span>-based Query Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyuan Liu, Mengxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PUB: An <span class="highlight-title">LLM</span>-Enhanced Personality-Driven User Behaviour Simulator for
  Recommender System Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Ma, Ziqi Xu, Yongli Ren, Danula Hettiachchi, Jeffrey Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional offline evaluation methods for recommender systems struggle to
capture the complexity of modern platforms due to sparse behavioural signals,
noisy data, and limited modelling of user personality traits. While simulation
frameworks can generate synthetic data to address these gaps, existing methods
fail to replicate behavioural diversity, limiting their effectiveness. To
overcome these challenges, we propose the Personality-driven User Behaviour
Simulator (PUB), an LLM-based simulation framework that integrates the Big Five
personality traits to model personalised user behaviour. PUB dynamically infers
user personality from behavioural logs (e.g., ratings, reviews) and item
metadata, then generates synthetic interactions that preserve statistical
fidelity to real-world data. Experiments on the Amazon review datasets show
that logs generated by PUB closely align with real user behaviour and reveal
meaningful associations between personality traits and recommendation outcomes.
These results highlight the potential of the personality-driven simulator to
advance recommender system evaluation, offering scalable, controllable,
high-fidelity alternatives to resource-intensive real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebTrust: An AI-Driven Data Scoring System for Reliable Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joydeep Chandra, Aleksandr Algazinov, Satyam Kumar Navneet, Rim El Filali, Matt Laing, Andrew Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As access to information becomes more open and widespread, people are
increasingly using AI tools for assistance. However, many of these tools
struggle to estimate the trustworthiness of the information. Although today's
search engines include AI features, they often fail to offer clear indicators
of data reliability. To address this gap, we introduce WebTrust, a system
designed to simplify the process of finding and judging credible information
online. Built on a fine-tuned version of IBM's Granite-1B model and trained on
a custom dataset, WebTrust works by assigning a reliability score (from 0.1 to
1) to each statement it processes. In addition, it offers a clear justification
for why a piece of information received that score. Evaluated using prompt
engineering, WebTrust consistently achieves superior performance compared to
other small-scale LLMs and rule-based approaches, outperforming them across all
experiments on MAE, RMSE, and R2. User testing showed that when reliability
scores are displayed alongside search results, people feel more confident and
satisfied with the information they find. With its accuracy, transparency, and
ease of use, WebTrust offers a practical solution to help combat misinformation
and make trustworthy information more accessible to everyone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GENIUS: A Generative Framework for Universal Multimodal Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungyeon Kim, Xinliang Zhu, Xiaofan Lin, Muhammet Bastan, Douglas Gray, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval is an emerging approach in information retrieval that
generates identifiers (IDs) of target data based on a query, providing an
efficient alternative to traditional embedding-based retrieval methods.
However, existing models are task-specific and fall short of embedding-based
retrieval in performance. This paper proposes GENIUS, a universal generative
retrieval framework supporting diverse tasks across multiple modalities and
domains. At its core, GENIUS introduces modality-decoupled semantic
quantization, transforming multimodal data into discrete IDs encoding both
modality and semantics. Moreover, to enhance generalization, we propose a query
augmentation that interpolates between a query and its target, allowing GENIUS
to adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses
prior generative methods by a clear margin. Unlike embedding-based retrieval,
GENIUS consistently maintains high retrieval speed across database size, with
competitive performance across multiple benchmarks. With additional re-ranking,
GENIUS often achieves results close to those of embedding-based methods while
preserving efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Head RAG: Solving Multi-Aspect Problems with <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Ales Kubicek, Robert Gerstenberger, Marcin Chrapek, Roman Niggli, Patrik Okanovic, Yi Zhu, Patrick Iff, Michal Podstawski, Lucas Weitzendorf, Mingyuan Chi, Joanna Gajda, Piotr Nyczyk, Jürgen Müller, Hubert Niewiadomski, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving observation is that different attention
heads learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, multi-aspect datasets, and real-world
use cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages
over 18 RAG baselines, empirical improvements of up to 20% in retrieval success
ratios, and benefits for downstream LLM generation. MRAG can be seamlessly
integrated with existing RAG frameworks and benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMTU: A Massive <span class="highlight-title">Multi-Task</span> Table Understanding and Reasoning Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Interaction with Databases on Edge Devices in the
  Internet of Battlefield Things 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher D. Molek, Roberto Fronteddu, K. Brent Venable, Niranjan Suri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Hierarchy Design for Caching Middleware in the Age of NVM <span class="chip">ICDE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahram Ghandeharizadeh, Sandy Irani, Jenny Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in storage technology have introduced Non-Volatile Memory, NVM, as a
new storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid
State Disk (SSD), and Disk present a system designer with a wide array of
options in designing caching middleware. Moreover, design decisions to
replicate a data item in more than one level of a caching memory hierarchy may
enhance the overall system performance with a faster recovery time in the event
of a memory failure. Given a fixed budget, the key configuration questions are:
Which storage media should constitute the memory hierarchy? What is the storage
capacity of each hierarchy? Should data be replicated or partitioned across the
different levels of the hierarchy? We model these cache configuration questions
as an instance of the Multiple Choice Knapsack Problem (MCKP). This model is
guided by the specification of each type of memory along with an application's
database characteristics and its workload. Although MCKP is NP-complete, its
linear programming relaxation is efficiently solvable and can be used to
closely approximate the optimal solution. We use the resulting simple algorithm
to evaluate design tradeoffs in the context of a memory hierarchy for a
Key-Value Store (e.g., memcached) as well as a host-side cache (e.g.,
Flashcache). The results show selective replication is appropriate with certain
failure rates and workload characteristics. With a slim failure rate and
frequent data updates, tiering of data across the different storage media that
constitute the cache is superior to replication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version appeared in the IEEE 34th International Conference
  on Data Engineering (ICDE), Paris, France, 2018, pp. 1380-1383, doi:
  10.1109/ICDE.2018.00155</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PandasBench: A Benchmark for the Pandas API 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Broihier, Stefanos Baziotis, Daniel Kang, Charith Mendis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Pandas API has been central to the success of pandas and its
alternatives. Despite its importance, there is no benchmark for it, and we
argue that we cannot repurpose existing benchmarks (from other domains) for the
Pandas API.
  In this paper, we introduce requirements that are necessary for a Pandas API
enchmark, and present the first benchmark that fulfills them: PandasBench. We
argue that it should evaluate the real-world coverage of a technique. Yet,
real-world coverage is not sufficient for a useful benchmark, and so we also:
cleaned it from irrelevant code, adapted it for benchmark usage, and introduced
input scaling. We claim that uniform scaling used in other benchmarks (e.g.,
TPC-H) is too coarse-grained for PandasBench, and use a non-uniform scaling
scheme. PandasBench is the largest Pandas API benchmark to date, with 102
notebooks and 3,721 cells.
  We used PandasBench to evaluate Modin, Dask, Koalas, and Dias. This is the
largest-scale evaluation of all these techniques to date. Prior works report
significant speedups using constrained benchmarks, but we show that on a larger
benchmark with real-world code, the most notebooks that got a speedup were
8/102 (~8%) for Modin, and 0 for both Koalas and Dask. Dias showed speedups in
up to 55 notebooks (~54%), but it rewrites code incorrectly in certain cases,
which had not been observed in prior work. Second, we identified many failures:
Modin runs only 72/102 (~70%) notebooks, Dask 4 (~4%), Koalas 10 (~10%), and
Dias 97 (95%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Bang For Your Buck(et): Fast and Space-efficient
  Hardware-accelerated Coarse-granular Indexing on GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justus Henneberg, Felix Schuhknecht, Rosina Kharal, Trevor Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent work, we have shown that NVIDIA's raytracing cores on RTX video
cards can be exploited to realize hardware-accelerated lookups for GPU-resident
database indexes. On a high level, the concept materializes all keys as
triangles in a 3D scene and indexes them. Lookups are performed by firing rays
into the scene and utilizing the index structure to detect hits in a
hardware-accelerated fashion. While this approach called RTIndeX (or short RX)
is indeed promising, it currently suffers from three limitations: (1)
significant memory overhead per key, (2) slow range-lookups, and (3) poor
updateability. In this work, we show that all three problems can be tackled by
a single design change: Generalizing RX to become a coarse-granular index cgRX.
Instead of indexing individual keys, cgRX indexes buckets of keys which are
post-filtered after retrieval. This drastically reduces the memory overhead,
leads to the generation of a smaller and more efficient index structure, and
enables fast range-lookups as well as updates. We will see that representing
the buckets in the 3D space such that the lookup of a key is performed both
correctly and efficiently requires the careful orchestration of firing rays in
a specific sequence. Our experimental evaluation shows that cgRX offers the
most bang for the buck(et) by providing a throughput in relation to the memory
footprint that is 1.5-3x higher than for the comparable range-lookup supporting
baselines. At the same time, cgRX improves the range-lookup performance over RX
by up to 2x and offers practical updateability that is up to 5.6x faster than
rebuilding from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hermes: High-Performance Homomorphically Encrypted Vector Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully Homomorphic Encryption (FHE) has long promised the ability to compute
over encrypted data without revealing sensitive contents -- a foundational goal
for secure cloud analytics. Yet despite decades of cryptographic advances,
practical integration of FHE into real-world relational databases remains
elusive. This paper presents \textbf{Hermes}, the first system to enable
FHE-native vector query processing inside a standard SQL engine. By leveraging
the multi-slot capabilities of modern schemes, Hermes introduces a novel data
model that packs multiple records per ciphertext and embeds encrypted auxiliary
statistics (e.g., local sums) to support in-place updates and aggregation. To
reconcile ciphertext immutability with record-level mutability, we develop new
homomorphic algorithms based on slot masking, shifting, and rewriting. Hermes
is implemented as native C++ loadable functions in MySQL using OpenFHE v1.2.4,
comprising over 3,500 lines of code. Experiments on real-world datasets show up
to 1{,}600$\times$ throughput gain in encryption and over 30$\times$ speedup in
insertion compared to per-tuple baselines. Hermes brings FHE from cryptographic
promise to practical reality -- realizing a long-standing vision at the
intersection of databases and secure computation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-04T00:00:00Z">2025-06-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I'm Sorry Dave, I'm Afraid I Can't Return That: On YouTube Search API
  Use in Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Efstratiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  YouTube is among the most widely-used platforms worldwide, and has seen a lot
of recent academic attention. Despite its popularity and the number of studies
conducted on it, much less is understood about the way in which YouTube's Data
API, and especially the Search endpoint, operates. In this paper, we analyze
the API's behavior by running identical queries across a period of 12 weeks.
Our findings suggest that the search endpoint returns highly inconsistent
results between queries in ways that are not officially documented.
Specifically, the API seems to randomize returned videos based on the relative
popularity of the respective topic during the query period, making it nearly
impossible to obtain representative historical video samples, especially during
non-peak topical periods. Our results also suggest that the API may prioritize
shorter, more popular videos, although the role of channel popularity is not as
clear. We conclude with suggested strategies for researchers using the API for
data collection, as well as future research directions on expanding the API's
use-cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Query Fairness Under Unawareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Jaenich, Alejandro Moreo, Alessandro Fabris, Graham McDonald, Andrea Esuli, Iadh Ounis, Fabrizio Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional ranking algorithms are designed to retrieve the most relevant
items for a user's query, but they often inherit biases from data that can
unfairly disadvantage vulnerable groups. Fairness in information access systems
(IAS) is typically assessed by comparing the distribution of groups in a
ranking to a target distribution, such as the overall group distribution in the
dataset. These fairness metrics depend on knowing the true group labels for
each item. However, when groups are defined by demographic or sensitive
attributes, these labels are often unknown, leading to a setting known as
"fairness under unawareness". To address this, group membership can be inferred
using machine-learned classifiers, and group prevalence is estimated by
counting the predicted labels. Unfortunately, such an estimation is known to be
unreliable under dataset shift, compromising the accuracy of fairness
evaluations. In this paper, we introduce a robust fairness estimator based on
quantification that effectively handles multiple sensitive attributes beyond
binary classifications. Our method outperforms existing baselines across
various sensitive attributes and, to the best of our knowledge, is the first to
establish a reliable protocol for measuring fairness under unawareness across
multiple queries and groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeanExplore: A search engine for Lean 4 declarations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expanding Lean 4 ecosystem poses challenges for navigating its vast
libraries. This paper introduces LeanExplore, a search engine for Lean 4
declarations. LeanExplore enables users to semantically search for statements,
both formally and informally, across select Lean 4 packages (including
Batteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is
powered by a hybrid ranking strategy, integrating scores from a multi-source
semantic embedding model (capturing conceptual meaning from formal Lean code,
docstrings, AI-generated informal translations, and declaration titles), BM25+
for keyword-based lexical relevance, and a PageRank-based score reflecting
declaration importance and interconnectedness. The search engine is accessible
via a dedicated website (https://www.leanexplore.com/) and a Python API
(https://github.com/justincasher/lean-explore). Furthermore, the database can
be downloaded, allowing users to self-host the service. LeanExplore integrates
easily with LLMs via the model context protocol (MCP), enabling users to chat
with an AI assistant about Lean declarations or utilize the search engine for
building theorem-proving agents. This work details LeanExplore's architecture,
data processing, functionalities, and its potential to enhance Lean 4 workflows
and AI-driven mathematical research
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure. Project website: https://www.leanexplore.com/ ,
  Code: https://github.com/justincasher/lean-explore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T$^2$-RAGBench: Text-and-Table Benchmark for Evaluating
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Strich, Enes Kutay Isgorur, Maximilian Trescher, Chris Biemann, Martin Semmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most financial documents contain a combination of textual and tabular
information, robust Retrieval-Augmented Generation (RAG) systems are essential
for effectively accessing and reasoning over such content to perform complex
numerical tasks. This paper introduces T$^2$-RAGBench, a benchmark comprising
32,908 question-context-answer triples, designed to evaluate RAG methods on
real-world financial data. Unlike typical QA datasets that operate under
Oracle-context settings, where the relevant context is explicitly provided,
T$^2$-RAGBench challenges models to first retrieve the correct context before
conducting numerical reasoning. Existing QA datasets involving text and tables
typically contain context-dependent questions, which may yield multiple correct
answers depending on the provided context. To address this, we transform these
datasets into a context-independent format, enabling reliable RAG evaluation.
We conduct a comprehensive evaluation of popular RAG methods. Our analysis
identifies Hybrid BM25, a technique that combines dense and sparse vectors, as
the most effective approach for text-and-table data. However, results
demonstrate that T$^2$-RAGBench remains challenging even for SOTA LLMs and RAG
methods. Further ablation studies examine the impact of embedding models and
corpus size on retrieval performance. T$^2$-RAGBench provides a realistic and
rigorous benchmark for existing RAG methods on text-and-table data. Code and
dataset are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generative Adaptive Replay Continual Learning Model for Temporal
  Knowledge Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhang, Wei Chen, Youfang Lin, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning
(TKGR) methods focus on significantly reducing computational cost and
mitigating catastrophic forgetting caused by fine-tuning models with new data.
However, existing CL-based TKGR methods still face two key limitations: (1)
They usually one-sidedly reorganize individual historical facts, while
overlooking the historical context essential for accurately understanding the
historical semantics of these facts; (2) They preserve historical knowledge by
simply replaying historical facts, while ignoring the potential conflicts
between historical and emerging facts. In this paper, we propose a Deep
Generative Adaptive Replay (DGAR) method, which can generate and adaptively
replay historical entity distribution representations from the whole historical
context. To address the first challenge, historical context prompts as sampling
units are built to preserve the whole historical context information. To
overcome the second challenge, a pre-trained diffusion model is adopted to
generate the historical distribution. During the generation process, the common
features between the historical and current distributions are enhanced under
the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay
mechanism is designed to effectively integrate historical and current
distributions. Experimental results demonstrate that DGAR significantly
outperforms baselines in reasoning and mitigating forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GORACS: Group-level Optimal Transport-guided Coreset Selection for
  <span class="highlight-title">LLM</span>-based Recommender Systems <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiehua Mei, Hengrui Chen, Peng Yu, Jiaqing Liang, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have shown great potential in
recommender systems, the prohibitive computational costs for fine-tuning LLMs
on entire datasets hinder their successful deployment in real-world scenarios.
To develop affordable and effective LLM-based recommender systems, we focus on
the task of coreset selection which identifies a small subset of fine-tuning
data to optimize the test loss, thereby facilitating efficient LLMs'
fine-tuning. Although there exist some intuitive solutions of subset selection,
including distribution-based and importance-based approaches, they often lead
to suboptimal performance due to the misalignment with downstream fine-tuning
objectives or weak generalization ability caused by individual-level sample
selection. To overcome these challenges, we propose GORACS, which is a novel
Group-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based
recommender systems. GORACS is designed based on two key principles for coreset
selection: 1) selecting the subsets that minimize the test loss to align with
fine-tuning objectives, and 2) enhancing model generalization through
group-level data selection. Corresponding to these two principles, GORACS has
two key components: 1) a Proxy Optimization Objective (POO) leveraging optimal
transport and gradient information to bound the intractable test loss, thus
reducing computational costs by avoiding repeated LLM retraining, and 2) a
two-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient
group-level selection. Our extensive experiments across diverse recommendation
datasets and tasks validate that GORACS significantly reduces fine-tuning costs
of LLMs while achieving superior performance over the state-of-the-art
baselines and full data training. The source code of GORACS are available at
https://github.com/Mithas-114/GORACS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Embedding Empowered Entity Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma J. Gerritse, Faegheh Hasibi, Arjen P. de Vries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we investigate methods for entity retrieval using graph
embeddings. While various methods have been proposed over the years, most
utilize a single graph embedding and entity linking approach. This hinders our
understanding of how different graph embedding and entity linking methods
impact entity retrieval. To address this gap, we investigate the effects of
three different categories of graph embedding techniques and five different
entity linking methods. We perform a reranking of entities using the distance
between the embeddings of annotated entities and the entities we wish to
rerank. We conclude that the selection of both graph embeddings and entity
linkers significantly impacts the effectiveness of entity retrieval. For graph
embeddings, methods that incorporate both graph structure and textual
descriptions of entities are the most effective. For entity linking, both
precision and recall concerning concepts are important for optimal retrieval
performance. Additionally, it is essential for the graph to encompass as many
entities as possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2005.02843</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-objective Aligned Bidword Generation Model for E-commerce Search
  Advertising <span class="chip">SIGIR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhui Liu, Chunyuan Yuan, Ming Pang, Zheng Fang, Li Yuan, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo, Jingping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Karl, Ansgar Scherp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publication databases rely on accurate metadata extraction from diverse web
sources, yet variations in web layouts and data formats present challenges for
metadata providers. This paper introduces CRAWLDoc, a new method for contextual
ranking of linked web documents. Starting with a publication's URL, such as a
digital object identifier, CRAWLDoc retrieves the landing page and all linked
web resources, including PDFs, ORCID profiles, and supplementary materials. It
embeds these resources, along with anchor texts and the URLs, into a unified
representation. For evaluating CRAWLDoc, we have created a new, manually
labeled dataset of 600 publications from six top publishers in computer
science. Our method CRAWLDoc demonstrates a robust and layout-independent
ranking of relevant documents across publishers and data formats. It lays the
foundation for improved metadata extraction from web documents with various
layouts and formats. Our source code and dataset can be accessed at
https://github.com/FKarl/CRAWLDoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SCOLIA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Mental Models of Generative Conversational Search and The
  Effect of Interface Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chadha Degachi, Samuel Kernan Freire, Evangelos Niforatos, Gerd Kortuem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The experience and adoption of conversational search is tied to the accuracy
and completeness of users' mental models -- their internal frameworks for
understanding and predicting system behaviour. Thus, understanding these models
can reveal areas for design interventions. Transparency is one such
intervention which can improve system interpretability and enable mental model
alignment. While past research has explored mental models of search engines,
those of generative conversational search remain underexplored, even while the
popularity of these systems soars. To address this, we conducted a study with
16 participants, who performed 4 search tasks using 4 conversational interfaces
of varying transparency levels. Our analysis revealed that most user mental
models were too abstract to support users in explaining individual search
instances. These results suggest that 1) mental models may pose a barrier to
appropriate trust in conversational search, and 2) hybrid web-conversational
search is a promising novel direction for future search interface design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">Transformer</span>s for Discriminative <span class="highlight-title">Recommendation</span> via Generative
  Pretraining <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen, Bing Wang, Xiaoyi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminative recommendation tasks, such as CTR (click-through rate) and CVR
(conversion rate) prediction, play critical roles in the ranking stage of
large-scale industrial recommender systems. However, training a discriminative
model encounters a significant overfitting issue induced by data sparsity.
Moreover, this overfitting issue worsens with larger models, causing them to
underperform smaller ones. To address the overfitting issue and enhance model
scalability, we propose a framework named GPSD (\textbf{G}enerative
\textbf{P}retraining for \textbf{S}calable \textbf{D}iscriminative
Recommendation), drawing inspiration from generative training, which exhibits
no evident signs of overfitting. GPSD leverages the parameters learned from a
pretrained generative model to initialize a discriminative model, and
subsequently applies a sparse parameter freezing strategy. Extensive
experiments conducted on both industrial-scale and publicly available datasets
demonstrate the superior performance of GPSD. Moreover, it delivers remarkable
improvements in online A/B tests. GPSD offers two primary advantages: 1) it
substantially narrows the generalization gap in model training, resulting in
better test performance; and 2) it leverages the scalability of Transformers,
delivering consistent performance gains as models are scaled up. Specifically,
we observe consistent performance improvements as the model dense parameters
scale from 13K to 0.3B, closely adhering to power laws. These findings pave the
way for unifying the architectures of recommendation models and language
models, enabling the direct application of techniques well-established in large
language models to recommendation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithms for estimating linear function in data mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hoang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main goal of this topic is to showcase several studied algorithms for
estimating the linear utility function to predict the users preferences. For
example, if a user comes to buy a car that has several attributes including
speed, color, age, etc in a linear function, the algorithms that we present in
this paper help with estimating this linear function to filter out a small
subset that would be of best interest to the user among a million tuples in a
very large database. In addition, the estimating linear function could also be
applicable in getting to know what the data can do or predicting the future
based on the data that is used in data science, which is demonstrated by the
GNN, PLOD algorithms. In the ever-evolving field of data science, deriving
valuable insights from large datasets is critical for informed decision-making,
particularly in predictive applications. Data analysts often identify
high-quality datasets without missing values, duplicates, or inconsistencies
before merging diverse attributes for analysis. Taking housing price prediction
as a case study, various attributes must be considered, including location
factors (proximity to urban centers, crime rates), property features (size,
style, modernity), and regional policies (tax implications). Experts in the
field typically rank these attributes to establish a predictive utility
function, which machine learning models use to forecast outcomes like housing
prices. Several data discovery algorithms, including those that address the
challenges of predefined utility functions and human input for attribute
ranking, which often result in a time-consuming iterative process, that the
work of cannot overcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProRank: Prompt Warmup via Reinforcement Learning for Small Language
  Models Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianming Li, Aamir Shakir, Rui Huang, Julius Lipp, Jing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking is fundamental to information retrieval and retrieval-augmented
generation, with recent Large Language Models (LLMs) significantly advancing
reranking quality. While recent advances with LLMs have significantly improved
document reranking quality, current approaches primarily rely on large-scale
LLMs (>7B parameters) through zero-shot prompting, presenting high
computational costs. Small Language Models (SLMs) offer a promising alternative
because of their efficiency, but our preliminary quantitative analysis reveals
they struggle with understanding task prompts without fine-tuning. This limits
their effectiveness for document reranking tasks. To address this issue, we
introduce a novel two-stage training approach, ProRank, for SLM-based document
reranking. First, we propose a prompt warmup stage using reinforcement learning
GRPO to steer SLMs to understand task prompts and generate more accurate
coarse-grained binary relevance scores for document reranking. Then, we
continuously fine-tune the SLMs with a fine-grained score learning stage
without introducing additional layers to further improve the reranking quality.
Comprehensive experimental results demonstrate that the proposed ProRank
consistently outperforms both the most advanced open-source and proprietary
reranking models. Notably, our lightweight ProRank-0.5B model even surpasses
the powerful 32B LLM reranking model on the BEIR benchmark, establishing that
properly trained SLMs can achieve superior document reranking performance while
maintaining computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized
  <span class="highlight-title">Domain</span> Question-Answering <span class="chip">NeurIPS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) struggles with domain-specific
enterprise datasets, often isolated behind firewalls and rich in complex,
specialized terminology unseen by LLMs during pre-training. Semantic
variability across domains like medicine, networking, or law hampers RAG's
context precision, while fine-tuning solutions are costly, slow, and lack
generalization as new data emerges. Achieving zero-shot precision with
retrievers without fine-tuning still remains a key challenge. We introduce
'MetaGen Blended RAG', a novel enterprise search approach that enhances
semantic retrievers through a metadata generation pipeline and hybrid query
indexes using dense and sparse vectors. By leveraging key concepts, topics, and
acronyms, our method creates metadata-enriched semantic indexes and boosted
hybrid queries, delivering robust, scalable performance without fine-tuning. On
the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks
and even rivaling fine-tuned models on that dataset, while also excelling on
datasets like SQuAD and NQ. This approach redefines enterprise search using a
new approach to building semantic retrievers with unmatched generalization
across specialized domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Paper Submitted for NeurIPS 2025- The Thirty-Ninth Annual
  Conference on Neural Information Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MammAlps: A multi-view video behavior monitoring dataset of wild mammals
  in the Swiss Alps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer Sumbül, Alexander Mathis, Devis Tuia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring wildlife is essential for ecology and ethology, especially in
light of the increasing human impact on ecosystems. Camera traps have emerged
as habitat-centric sensors enabling the study of wildlife populations at scale
with minimal disturbance. However, the lack of annotated video datasets limits
the development of powerful video understanding models needed to process the
vast amount of fieldwork data collected. To advance research in wild animal
behavior monitoring we present MammAlps, a multimodal and multi-view dataset of
wildlife behavior monitoring from 9 camera-traps in the Swiss National Park.
MammAlps contains over 14 hours of video with audio, 2D segmentation maps and
8.5 hours of individual tracks densely labeled for species and behavior. Based
on 6135 single animal clips, we propose the first hierarchical and multimodal
animal behavior recognition benchmark using audio, video and reference scene
segmentation maps as inputs. Furthermore, we also propose a second
ecology-oriented benchmark aiming at identifying activities, species, number of
individuals and meteorological conditions from 397 multi-view and long-term
ecological events, including false positive triggers. We advocate that both
tasks are complementary and contribute to bridging the gap between machine
learning and ecology. Code and data are available at:
https://github.com/eceo-epfl/MammAlps
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; Benchmark and code at:
  https://github.com/eceo-epfl/MammAlps. After submission of v1, we noticed
  that a few audio files were not correctly aligned with the corresponding
  video. We fixed the issue, which had little to no impact on performance. We
  also now report results for three runs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Cascade Ranking as One Network <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09492v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09492v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunli Wang, Zhen Zhang, Zhiqiang Wang, Zixuan Yang, Yu Li, Jian Yang, Shiyang Wen, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cascade Ranking is a prevalent architecture in large-scale top-k selection
systems like recommendation and advertising platforms. Traditional training
methods focus on single-stage optimization, neglecting interactions between
stages. Recent advances have introduced interaction-aware training paradigms,
but still struggle to 1) align training objectives with the goal of the entire
cascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn
effective collaboration patterns for different stages. To address these
challenges, we propose LCRON, which introduces a novel surrogate loss function
derived from the lower bound probability that ground truth items are selected
by cascade ranking, ensuring alignment with the overall objective of the
system. According to the properties of the derived bound, we further design an
auxiliary loss for each stage to drive the reduction of this bound, leading to
a more robust and effective top-k selection. LCRON enables end-to-end training
of the entire cascade ranking system as a unified network. Experimental results
demonstrate that LCRON achieves significant improvement over existing methods
on public benchmarks and industrial applications, addressing key limitations in
cascade ranking training and significantly enhancing system performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model
  Generation Using <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Cai Chen, Yi Pin Xu, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In electronic design, engineers often manually search through extensive
documents to retrieve component parameters required for constructing SPICE
models, a process that is both labor-intensive and time-consuming. To address
this challenge, we present an automated framework called D2S-FLOW that
leverages large language models (LLMs) to extract electrical parameters from
datasheets and generate SPICE models with high precision and efficiency,
significantly reducing the need for manual intervention. Unlike traditional RAG
systems, D2S-FLOW employs a workflow to enhance precision in handling
unstructured documents and inconsistent naming conventions through three
innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical
Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity
Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER
utilizes document structure for precise parameter localization, and HNEN
standardizes terminology via semantic inference. Experimental results
demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1
score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the
strongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it
reduces API token consumption by 38% and minimizes the irrelevant information
ratio to 4%, showcasing substantial improvements in resource efficiency. This
research provides an effective automated solution for circuit design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LARES: Latent Reasoning for Sequential <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Liu, Bowen Zheng, Xiaolei Wang, Wayne Xin Zhao, Jinpeng Wang, Sheng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems have become increasingly important in
real-world applications that model user behavior sequences to predict their
preferences. However, existing sequential recommendation methods predominantly
rely on non-reasoning paradigms, which may limit the model's computational
capacity and result in suboptimal recommendation performance. To address these
limitations, we present LARES, a novel and scalable LAtent REasoning framework
for Sequential recommendation that enhances model's representation capabilities
through increasing the computation density of parameters by depth-recurrent
latent reasoning. Our proposed approach employs a recurrent architecture that
allows flexible expansion of reasoning depth without increasing parameter
complexity, thereby effectively capturing dynamic and intricate user interest
patterns. A key difference of LARES lies in refining all input tokens at each
implicit reasoning step to improve the computation utilization. To fully unlock
the model's reasoning potential, we design a two-phase training strategy: (1)
Self-supervised pre-training (SPT) with dual alignment objectives; (2)
Reinforcement post-training (RPT). During the first phase, we introduce
trajectory-level alignment and step-level alignment objectives, which enable
the model to learn recommendation-oriented latent reasoning patterns without
requiring supplementary annotated data. The subsequent phase utilizes
reinforcement learning (RL) to harness the model's exploratory ability, further
refining its reasoning capabilities. Comprehensive experiments on real-world
benchmarks demonstrate our framework's superior performance. Notably, LARES
exhibits seamless compatibility with existing advanced models, further
improving their recommendation performance. Our code is available at
https://anonymous.4open.science/r/LARES-E458/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypothetical Documents or Knowledge Leakage? Rethinking <span class="highlight-title">LLM</span>-based Query
  Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyze whether the generated documents contain information
entailed by ground-truth evidence and assess their impact on performance. Our
findings indicate that, on average, performance improvements consistently
occurred for claims whose generated documents included sentences entailed by
gold evidence. This suggests that knowledge leakage may be present in
fact-verification benchmarks, potentially inflating the perceived performance
of LLM-based query expansion methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Recommender with End-to-End Learnable Item Tokenization <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05546v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05546v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Liu, Bowen Zheng, Cheng Ling, Lantao Hu, Han Li, Wayne Xin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation systems have gained increasing attention as an
innovative approach that directly generates item identifiers for recommendation
tasks. Despite their potential, a major challenge is the effective construction
of item identifiers that align well with recommender systems. Current
approaches often treat item tokenization and generative recommendation training
as separate processes, which can lead to suboptimal performance. To overcome
this issue, we introduce ETEGRec, a novel End-To-End Generative Recommender
that unifies item tokenization and generative recommendation into a cohesive
framework. Built on a dual encoder-decoder architecture, ETEGRec consists of an
item tokenizer and a generative recommender. To enable synergistic interaction
between these components, we propose a recommendation-oriented alignment
strategy, which includes two key optimization objectives: sequence-item
alignment and preference-semantic alignment. These objectives tightly couple
the learning processes of the item tokenizer and the generative recommender,
fostering mutual enhancement. Additionally, we develop an alternating
optimization technique to ensure stable and efficient end-to-end training of
the entire framework. Extensive experiments demonstrate the superior
performance of our approach compared to traditional sequential recommendation
models and existing generative recommendation baselines. Our code is available
at https://github.com/RUCAIBox/ETEGRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROGRAG: A Robustly Optimized GraphRAG Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) commonly struggle with specialized or emerging
topics which are rarely seen in the training corpus. Graph-based
retrieval-augmented generation (GraphRAG) addresses this by structuring domain
knowledge as a graph for dynamic retrieval. However, existing pipelines involve
complex engineering workflows, making it difficult to isolate the impact of
individual components. It is also challenging to evaluate the retrieval
effectiveness due to the overlap between the pretraining and evaluation
datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG
framework. Specifically, we propose a multi-stage retrieval mechanism that
integrates dual-level with logic form retrieval methods to improve retrieval
robustness without increasing computational cost. To further refine the system,
we incorporate various result verification methods and adopt an incremental
database construction approach. Through extensive ablation experiments, we
rigorously assess the effectiveness of each component. Our implementation
includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct
initially underperformed. ROGRAG significantly improves the score from 60.0% to
75.0% and outperforms mainstream methods. Experiments on domain-specific
datasets reveal that dual-level retrieval enhances fuzzy matching, while logic
form retrieval improves structured reasoning, highlighting the importance of
multi-stage retrieval.ROGRAG is released as an open-source resource and
supports installation with pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2025 demo track, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Offline Reinforcement Learning Algorithm Customized for <span class="highlight-title">Multi-Task</span>
  Fusion in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17589v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17589v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Cong Xu, Ming Zhao, Jiawei Zhu, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for
combining multiple scores outputted by Multi-Task Learning (MTL) into a final
score to maximize user satisfaction, which determines the ultimate
recommendation results. Recently, to optimize long-term user satisfaction
within a recommendation session, Reinforcement Learning (RL) is used for MTF in
the industry. However, the offline RL algorithms used for MTF so far have the
following severe problems: 1) to avoid out-of-distribution (OOD) problem, their
constraints are overly strict, which seriously damage their performance; 2)
they are unaware of the exploration policy used for producing training data and
never interact with real environment, so only suboptimal policy can be learned;
3) the traditional exploration policies are inefficient and hurt user
experience. To solve the above problems, we propose a novel method named
IntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF
integrates offline RL model with our online exploration policy to relax
overstrict and complicated constraints, which significantly improves its
performance. We also design an extremely efficient exploration policy, which
eliminates low-value exploration space and focuses on exploring potential
high-value state-action pairs. Moreover, we adopt progressive training mode to
further enhance our model's performance with the help of our exploration
policy. We conduct extensive offline and online experiments in the short video
channel of Tencent News. The results demonstrate that our model outperforms
other models remarkably. IntegratedRL-MTF has been fully deployed in our RS and
other large-scale RSs in Tencent, which have achieved significant improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiang Yao, Weizhao Jin, Srivatsan Ravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The entity resolution problem requires finding pairs across datasets that
belong to different owners but refer to the same entity in the real world. To
train and evaluate solutions (either rule-based or machine-learning-based) to
the entity resolution problem, generating a ground truth dataset with entity
pairs or clusters is needed. However, such a data annotation process involves
humans as domain oracles to review the plaintext data for all candidate record
pairs from different parties, which inevitably infringes the privacy of data
owners, especially in privacy-sensitive cases like medical records. To the best
of our knowledge, there is no prior work on privacy-preserving ground truth
dataset generation, especially in the domain of entity resolution. We propose a
novel blind annotation protocol based on homomorphic encryption that allows
domain oracles to collaboratively label ground truths without sharing data in
plaintext with other parties. In addition, we design a domain-specific
easy-to-use language that hides the sophisticated underlying homomorphic
encryption layer. Rigorous proof of the privacy guarantee is provided and our
empirical experiments via an annotation simulator indicate the feasibility of
our privacy-preserving protocol (f-measure on average achieves more than 90\%
compared with the real ground truths).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoRe-MMRAG: <span class="highlight-title">Cross</span>-Source Knowledge Reconciliation for Multimodal RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose Cross-source knowledge \textbf{Re}conciliation for
Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively
reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a
four-stage pipeline: it first generates an internal response from parametric
knowledge, then selects the most relevant multimodal evidence via joint
similarity assessment, generates an external response, and finally integrates
both to produce a reliable answer. Additionally, a specialized training
paradigm enhances knowledge source discrimination, multimodal integration, and
unified answer generation. Experiments on KB-VQA benchmarks show that
CoRe-MMRAG achieves substantial improvements over baseline methods, achieving
5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What <span class="highlight-title">LLM</span>s Miss in <span class="highlight-title">Recommendation</span>s: Bridging the Gap with
  Retrieval-Augmented Collaborative Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahrooz Pouryousef, Ali Montazeralghaem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-item interactions contain rich collaborative signals that form the
backbone of many successful recommender systems. While recent work has explored
the use of large language models (LLMs) for recommendation, it remains unclear
whether LLMs can effectively reason over this type of collaborative
information. In this paper, we conduct a systematic comparison between LLMs and
classical matrix factorization (MF) models to assess LLMs' ability to leverage
user-item interaction data. We further introduce a simple retrieval-augmented
generation (RAG) method that enhances LLMs by grounding their predictions in
structured interaction data. Our experiments reveal that current LLMs often
fall short in capturing collaborative patterns inherent to MF models, but that
our RAG-based approach substantially improves recommendation
quality-highlighting a promising direction for future LLM-based recommenders.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransClean: Finding False Positives in Multi-Source Entity Matching
  under Real-World Conditions via Transitive Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando de Meer Pardo, Branka Hadji Misheva, Martin Braschler, Kurt Stockinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TransClean, a method for detecting false positive predictions of
entity matching algorithms under real-world conditions characterized by
large-scale, noisy, and unlabeled multi-source datasets that undergo
distributional shifts. TransClean is explicitly designed to operate with
multiple data sources in an efficient, robust and fast manner while accounting
for edge cases and requiring limited manual labeling. TransClean leverages the
Transitive Consistency of a matching, a measure of the consistency of a
pairwise matching model f_theta on the matching it produces G_f_theta, based
both on its predictions on directly evaluated record pairs and its predictions
on implied record pairs. TransClean iteratively modifies a matching through
gradually removing false positive matches while removing as few true positive
matches as possible. In each of these steps, the estimation of the Transitive
Consistency is exclusively done through model evaluations and produces
quantities that can be used as proxies of the amounts of true and false
positives in the matching while not requiring any manual labeling, producing an
estimate of the quality of the matching and indicating which record groups are
likely to contain false positives. In our experiments, we compare combining
TransClean with a naively trained pairwise matching model (DistilBERT) and with
a state-of-the-art end-to-end matching method (CLER) and illustrate the
flexibility of TransClean in being able to detect most of the false positives
of either setup across a variety of datasets. Our experiments show that
TransClean induces an average +24.42 F1 score improvement for entity matching
in a multi-source setting when compared to traditional pair-wise matching
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Signals as a First-Class Citizen When Querying Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schwarzinger, Gernot Steindl, Thomas Frühwirth, Thomas Preindl, Konrad Diwold, Katrin Ehrenmüller, Fajar J. Ekaputra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber-Physical Systems (CPSs) tightly integrate computation with physical
entities, often generating vast amounts of time series data from thousands of
sensors. Although knowledge graphs offer a powerful means to contextualize
these data, existing approaches to integrating knowledge graphs with time
series data lack a concept to model the continuous temporal values inherent in
CPSs. This gap can make expressing computations on the sensor data cumbersome.
In this work, we propose the integration of knowledge graphs and signals, a
proven concept for modeling temporal values. By treating signals as first-class
citizens in query languages, we can enable seamless querying over knowledge
graphs and signals. While the knowledge graph captures information on the CPS,
signals represent its run-time data from sensors. We discuss the implications
of such an approach and propose SigSPARQL, an extension to the SPARQL query
language, to demonstrate these concepts. Furthermore, we evaluate the
feasibility of implementing SigSPARQL with a prototype and demonstrate the
applicability of the query language for a monitoring use case within a CPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling <span class="highlight-title">LLM</span> Knowledge Analysis via Extensive Materialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Simon Razniewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have majorly advanced NLP and AI, and next to
their ability to perform a wide range of procedural tasks, a major success
factor is their internalized factual knowledge. Since Petroni et al. (2019),
analyzing this knowledge has gained attention. However, most approaches
investigate one question at a time via modest-sized pre-defined samples,
introducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents
the analysis of knowledge (or beliefs) of LLMs beyond the experimenter's
predisposition.
  To address this challenge, we propose a novel methodology to comprehensively
materialize an LLM's factual knowledge through recursive querying and result
consolidation. Our approach is a milestone for LLM research, for the first time
providing constructive insights into the scope and structure of LLM knowledge
(or beliefs).
  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million
relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB
to exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale,
accuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible
at https://gptkb.org
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 tables, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-03T00:00:00Z">2025-06-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DistRAG: Towards Distance-Based Spatial Reasoning in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real world tasks where Large Language Models (LLMs) can be used require
spatial reasoning, like Point of Interest (POI) recommendation and itinerary
planning. However, on their own LLMs lack reliable spatial reasoning
capabilities, especially about distances. To address this problem, we develop a
novel approach, DistRAG, that enables an LLM to retrieve relevant spatial
information not explicitly learned during training. Our method encodes the
geodesic distances between cities and towns in a graph and retrieves a context
subgraph relevant to the question. Using this technique, our method enables an
LLM to answer distance-based reasoning questions that it otherwise cannot
answer. Given the vast array of possible places an LLM could be asked about,
DistRAG offers a flexible first step towards providing a rudimentary `world
model' to complement the linguistic knowledge held in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Reusability in Recommender Systems: The Case for Dataset- and
  Task-Independent Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Kurniawan Wijaya, Xinyang Shao, Gonzalo Fiz Pontiveros, Edoardo D'Amico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are pivotal in delivering personalized experiences across
industries, yet their adoption and scalability remain hindered by the need for
extensive dataset- and task-specific configurations. Existing systems often
require significant manual intervention, domain expertise, and engineering
effort to adapt to new datasets or tasks, creating barriers to entry and
limiting reusability. In contrast, recent advancements in large language models
(LLMs) have demonstrated the transformative potential of reusable systems,
where a single model can handle diverse tasks without significant
reconfiguration. Inspired by this paradigm, we propose the Dataset- and
Task-Independent Recommender System (DTIRS), a framework aimed at maximizing
the reusability of recommender systems while minimizing barriers to entry.
Unlike LLMs, which achieve task generalization directly, DTIRS focuses on
eliminating the need to rebuild or reconfigure recommendation pipelines for
every new dataset or task, even though models may still need retraining on new
data. By leveraging the novel Dataset Description Language (DsDL), DTIRS
enables standardized dataset descriptions and explicit task definitions,
allowing autonomous feature engineering, model selection, and optimization.
This paper introduces the concept of DTIRS and establishes a roadmap for
transitioning from Level-1 automation (dataset-agnostic but task-specific
systems) to Level-2 automation (fully dataset- and task-independent systems).
Achieving this paradigm would maximize code reusability and lower barriers to
adoption. We discuss key challenges, including the trade-offs between
generalization and specialization, computational overhead, and scalability,
while presenting DsDL as a foundational tool for this vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Rankings and Personalized <span class="highlight-title">Recommendation</span>s in Marketplaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Besbes, Yash Kanoria, Akshit Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individuals often navigate several options with incomplete knowledge of their
own preferences. Information provisioning tools such as public rankings and
personalized recommendations have become central to helping individuals make
choices, yet their value proposition under different marketplace environments
remains unexplored. This paper studies a stylized model to explore the impact
of these tools in two marketplace settings: uncapacitated supply, where items
can be selected by any number of agents, and capacitated supply, where each
item is constrained to be matched to a single agent. We model the agents
utility as a weighted combination of a common term which depends only on the
item, reflecting the item's population level quality, and an idiosyncratic
term, which depends on the agent item pair capturing individual specific
tastes. Public rankings reveal the common term, while personalized
recommendations reveal both terms. In the supply unconstrained settings, both
public rankings and personalized recommendations improve welfare, with their
relative value determined by the degree of preference heterogeneity. Public
rankings are effective when preferences are relatively homogeneous, while
personalized recommendations become critical as heterogeneity increases. In
contrast, in supply constrained settings, revealing just the common term, as
done by public rankings, provides limited benefit since the total common value
available is limited by capacity constraints, whereas personalized
recommendations, by revealing both common and idiosyncratic terms,
significantly enhance welfare by enabling agents to match with items they
idiosyncratically value highly. These results illustrate the interplay between
supply constraints and preference heterogeneity in determining the
effectiveness of information provisioning tools, offering insights for their
design and deployment in diverse settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriPSS: A Tri-Modal Keyframe Extraction Framework Using Perceptual,
  Structural, and Semantic Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Can Cakmak, Nitin Agarwal, Diwash Poudel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient keyframe extraction is critical for effective video summarization
and retrieval, yet capturing the complete richness of video content remains
challenging. In this work, we present TriPSS, a novel tri-modal framework that
effectively integrates perceptual cues from color features in the CIELAB space,
deep structural embeddings derived from ResNet-50, and semantic context from
frame-level captions generated by Llama-3.2-11B-Vision-Instruct. By fusing
these diverse modalities using principal component analysis, TriPSS constructs
robust multi-modal embeddings that enable adaptive segmentation of video
content via HDBSCAN clustering. A subsequent refinement stage incorporating
quality assessment and duplicate filtering ensures that the final keyframe set
is both concise and semantically rich. Comprehensive evaluations on benchmark
datasets TVSum20 and SumMe demonstrate that TriPSS achieves state-of-the-art
performance, substantially outperforming traditional unimodal and previous
multi-modal methods. These results underscore TriPSS's ability to capture
nuanced visual and semantic information, thereby setting a new benchmark for
video content understanding in large-scale retrieval scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Information Retrieval to Enhance Spoken Language
  Understanding Prompts in Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference paper accepted to INTERSPEECH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and
  Prompt-Based Approaches to Depression Symptom Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diogo A. P. Nunes, Eugénio Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token and Span Classification for Entity Recognition in French
  Historical Encyclopedias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Moncla, Hédi Zeghidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepShop: A Benchmark for Deep Research Shopping Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, Xiuying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web agents for online shopping have shown great promise in automating user
interactions across e-commerce platforms. Benchmarks for assessing such agents
do not reflect the complexity of real-world shopping scenarios, as they often
consist of overly simple queries with deterministic paths, such as "Find iPhone
15." Real shopping scenarios are inherently more layered, involving
multi-dimensional product attributes, search filters, and user-specific sorting
preferences. To address this gap, we introduce DeepShop, a benchmark designed
to evaluate web agents in complex and realistic online shopping environments.
DeepShop comprises three key components. (1) Query diversity evolution:
Starting from real user queries, we generate diverse queries across five
popular online shopping domains. (2) Query complexity evolution: We further
evolve these queries to increase complexity, considering product attributes,
search filters, and sorting preferences, and classify them into three levels:
easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and
holistic evaluation: We propose an automated evaluation framework that assesses
agent performance in terms of fine-grained aspects (product attributes, search
filters, and sorting preferences) and reports the overall success rate through
holistic evaluation. We conduct a systematic evaluation of retrieval-augmented
generation (RAG) methods, web agents, and deep research systems. Results show
that RAG struggles with complex queries due to its lack of web interaction,
while other methods face significant challenges with filters and sorting
preferences, leading to low overall success rates. We also perform
cross-category, complexity-based evaluations and error analyses to support the
advancement of deep research shopping agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining social relations and interaction data in Recommender System
  with Graph Convolution Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin T. Tran, Vaclav Snasel, Loc Tan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recommender system is an important subject in the field of data mining,
where the item rating information from users is exploited and processed to make
suitable recommendations with all other users. The recommender system creates
convenience for e-commerce users and stimulates the consumption of items that
are suitable for users. In addition to e-commerce, a recommender system is also
used to provide recommendations on books to read, movies to watch, courses to
take or websites to visit. Similarity between users is an important impact for
recommendation, which could be calculated from the data of past user ratings of
the item by methods of collaborative filtering, matrix factorization or
singular vector decomposition. In the development of graph data mining
techniques, the relationships between users and items can be represented by
matrices from which collaborative filtering could be done with the larger
database, more accurate and faster in calculation. All these data can be
represented graphically and mined by today's highly developed graph neural
network models. On the other hand, users' social friendship data also influence
consumption habits because recommendations from friends will be considered more
carefully than information sources. However, combining a user's friend
influence and the similarity between users whose similar shopping habits is
challenging. Because the information is noisy and it affects each particular
data set in different ways. In this study, we present the input data processing
method to remove outliers which are single reviews or users with little
interaction with the items; the next proposed model will combine the social
relationship data and the similarity in the rating history of users to improve
the accuracy and recall of the recommender system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTCS: Effective Unsupervised Temporal Community Search with Pre-training
  of Temporal Dynamics and Subgraph Knowledge <span class="chip">SIGIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhang, Yankai Chen, Yingli Zhou, Yucan Guo, Xiaolin Han, Chenhao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world applications, the evolving relationships between entities
can be modeled as temporal graphs, where each edge has a timestamp representing
the interaction time.
  As a fundamental problem in graph analysis, {\it community search (CS)} in
temporal graphs has received growing attention but exhibits two major
limitations: (1) Traditional methods typically require predefined subgraph
structures, which are not always known in advance. (2) Learning-based methods
struggle to capture temporal interaction information. To fill this research
gap, in this paper, we propose an effective \textbf{U}nsupervised
\textbf{T}emporal \textbf{C}ommunity \textbf{S}earch with pre-training of
temporal dynamics and subgraph knowledge model (\textbf{\model}).
\model~contains two key stages: offline pre-training and online search. In the
first stage, we introduce multiple learning objectives to facilitate the
pre-training process in the unsupervised learning setting. In the second stage,
we identify a candidate subgraph and compute community scores using the
pre-trained node representations and a novel scoring mechanism to determine the
final community members. Experiments on five real-world datasets demonstrate
the effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR'25 short paper track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yankai Chen, Yue Que, Xinni Zhang, Chen Ma, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning vectorized embeddings is fundamental to many recommender systems for
user-item matching. To enable efficient online inference, representation
binarization, which embeds latent features into compact binary sequences, has
recently shown significant promise in optimizing both memory usage and
computational overhead. However, existing approaches primarily focus on
numerical quantization, neglecting the associated information loss, which often
results in noticeable performance degradation. To address these issues, we
study the problem of graph representation binarization for efficient
collaborative filtering. Our findings indicate that explicitly mitigating
information loss at various stages of embedding binarization has a significant
positive impact on performance. Building on these insights, we propose an
enhanced framework, BiGeaR++, which specifically leverages supervisory signals
from pseudo-positive samples, incorporating both real item data and latent
embedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a
fine-grained inference distillation mechanism and an effective embedding sample
synthesis approach. Empirical evaluations across five real-world datasets
demonstrate that the new designs in BiGeaR++ work seamlessly well with other
modules, delivering substantial improvements of around 1%-10% over BiGeaR and
thus achieving state-of-the-art performance compared to the competing methods.
Our implementation is available at https://github.com/QueYork/BiGeaR-SS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Named Entity Recognition Models for Russian Cultural News
  Texts: From BERT to <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Levchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Information Retrieval with a Monolingual Knowledge Base <span class="chip">SIGIR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Zhuang, Aman Gupta, Anurag Beniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted at GENNEXT@SIGIR25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NextQuill: Causal Preference Modeling for Enhancing <span class="highlight-title">LLM</span> Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Zhao, Juntao You, Yang Zhang, Wenjie Wang, Hong Cheng, Fuli Feng, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalizing large language models (LLMs) for individual users has become
increasingly important as they are progressively integrated into real-world
applications to support users' daily lives. However, existing personalization
approaches often fail to distinguish which components of model predictions and
training data truly reflect user preferences, leading to superficial
personalization alignment. In this paper, we introduce NextQuill, a novel LLM
personalization alignment framework grounded in causal preference modeling. We
approach personalization from a causal perspective, treating both model
predictions and ground-truth data generation as outcomes influenced by user
preferences, along with other factors. We define the true preference effect as
the causal impact of user history (which reflects preferences) on each token
prediction or data generation instance, estimated through causal intervention
techniques. Building on this insight, NextQuill introduces two complementary
alignment strategies: (1) aligning model-internal causal preference effects on
predictions with those reflected in ground-truth data, rather than
indiscriminately fitting predictions, and (2) focusing on fitting
preference-bearing tokens identified via ground-truth data preference effects,
rather than treating all tokens uniformly. By integrating these strategies,
NextQuill shifts the alignment process toward learning from causal preference
effects, facilitating more effective and personalized adaptation. Experiments
across multiple personalization benchmarks demonstrate that NextQuill
significantly improves personalization quality, offering a principled, causal
foundation for LLM personalization. Our codes are available on
https://github.com/juntaoyou/NextQuill.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRAMA: Diverse Augmentation from <span class="highlight-title">Large Language Model</span>s to Smaller Dense
  Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong effectiveness and
robustness while fine-tuned as dense retrievers. However, their large parameter
size brings significant inference time computational challenges, including high
encoding costs for large-scale corpora and increased query latency, limiting
their practical deployment. While smaller retrievers offer better efficiency,
they often fail to generalize effectively with limited supervised fine-tuning
data. In this work, we introduce DRAMA, a training framework that leverages
LLMs to train smaller generalizable dense retrievers. In particular, we adopt
pruned LLMs as the backbone and train on diverse LLM-augmented data in a
single-stage contrastive learning setup. Experiments show that DRAMA offers
better multilingual and long-context capabilities than traditional
encoder-based retrievers, and achieves strong performance across multiple tasks
and languages. These highlight the potential of connecting the training of
smaller retrievers with the growing advancements in LLMs, bridging the gap
between efficiency and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotative Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06256v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06256v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles L. A. Clarke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces annotative indexing, a novel framework that unifies and
generalizes traditional inverted indexes, column stores, object stores, and
graph databases. As a result, annotative indexing can provide the underlying
indexing framework for databases that support retrieval augmented generation,
knowledge graphs, entity retrieval, semi-structured data, and ranked retrieval.
While we primarily focus on human language data in the form of text, annotative
indexing is sufficiently general to support a range of other datatypes, and we
provide examples of SQL-like queries over a JSON store that includes numbers
and dates. Taking advantage of the flexibility of annotative indexing, we also
demonstrate a fully dynamic annotative index incorporating support for ACID
properties of transactions with hundreds of multiple concurrent readers and
writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/claclark/Cottontail</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoisonArena: Uncovering Competing Poisoning Attacks in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12574v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12574v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems, widely used to improve the
factual grounding of large language models (LLMs), are increasingly vulnerable
to poisoning attacks, where adversaries inject manipulated content into the
retriever's corpus. While prior research has predominantly focused on
single-attacker settings, real-world scenarios often involve multiple,
competing attackers with conflicting objectives. In this work, we introduce
PoisonArena, the first benchmark to systematically study and evaluate competing
poisoning attacks in RAG. We formalize the multi-attacker threat model, where
attackers vie to control the answer to the same query using mutually exclusive
misinformation. PoisonArena leverages the Bradley-Terry model to quantify each
method's competitive effectiveness in such adversarial environments. Through
extensive experiments on the Natural Questions and MS MARCO datasets, we
demonstrate that many attack strategies successful in isolation fail under
competitive pressure. Our findings highlight the limitations of conventional
evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore
the need for competitive evaluation to assess real-world attack robustness.
PoisonArena provides a standardized framework to benchmark and develop future
attack and defense strategies under more realistic, multi-adversary conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://poison-arena.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Prompt Engineering: Robust Behavior Control in <span class="highlight-title">LLM</span>s via Steering
  Target Atoms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Podcast P<span class="highlight-title">review</span> Generation: From Expert Models to <span class="highlight-title">LLM</span>-Based
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Winstead Zhu, Ann Clifton, Azin Ghazimatin, Edgar Tanaka, Edward Ronan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering and evaluating long-form talk content such as videos and podcasts
poses a significant challenge for users, as it requires a considerable time
investment. Previews offer a practical solution by providing concise snippets
that showcase key moments of the content, enabling users to make more informed
and confident choices. We propose an LLM-based approach for generating podcast
episode previews and deploy the solution at scale, serving hundreds of
thousands of podcast previews in a real-world application. Comprehensive
offline evaluations and online A/B testing demonstrate that LLM-generated
previews consistently outperform a strong baseline built on top of various ML
expert models, showcasing a significant reduction in the need for meticulous
feature engineering. The offline results indicate notable enhancements in
understandability, contextual clarity, and interest level, and the online A/B
test shows a 4.6% increase in user engagement with preview content, along with
a 5x boost in processing efficiency, offering a more streamlined and performant
solution compared to the strong baseline of feature-engineered expert models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted at ACL 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Lexical and Semantic Vector Search Methods When Classifying
  Medical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Harris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification is a common AI problem, and vector search is a typical
solution. This transforms a given body of text into a numerical representation,
known as an embedding, and modern improvements to vector search focus on
optimising speed and predictive accuracy. This is often achieved through neural
methods that aim to learn language semantics. However, our results suggest that
these are not always the best solution. Our task was to classify
rigidly-structured medical documents according to their content, and we found
that using off-the-shelf semantic vector search produced slightly worse
predictive accuracy than creating a bespoke lexical vector search model, and
that it required significantly more time to execute. These findings suggest
that traditional methods deserve to be contenders in the information retrieval
toolkit, despite the prevalence and success of neural models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This project was funded by a UKRI grant, number: 10048265</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACLR: A Scalable and Efficient Retrieval-based Method for Industrial
  Product Attribute Value Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03835v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03835v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product Attribute Value Identification (PAVI) involves identifying attribute
values from product profiles, a key task for improving product search,
recommendation, and business analytics on e-commerce platforms. However,
existing PAVI methods face critical challenges, such as inferring implicit
values, handling out-of-distribution (OOD) values, and producing normalized
outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive
Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR
formulates PAVI as an information retrieval task by encoding product profiles
and candidate values into embeddings and retrieving values based on their
similarity. It leverages contrastive training with taxonomy-aware hard negative
sampling and employs adaptive inference with dynamic thresholds. TACLR offers
three key advantages: (1) it effectively handles implicit and OOD values while
producing normalized outputs; (2) it scales to thousands of categories, tens of
thousands of attributes, and millions of values; and (3) it supports efficient
inference for high-load industrial deployment. Extensive experiments on
proprietary and public datasets validate the effectiveness and efficiency of
TACLR. Further, it has been successfully deployed on the real-world e-commerce
platform Xianyu, processing millions of product listings daily with frequently
updated, large-scale attribute taxonomies. We release the code to facilitate
reproducibility and future research at https://github.com/SuYindu/TACLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for
  Graph-RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Huang, Shiqi Zhang, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval
in Large Language Model (LLM)-based question answering. It is particularly
useful in domains such as biomedicine, law, and political science, where
retrieval often requires multi-hop reasoning over proprietary documents. Some
existing Graph-RAG systems construct KNN graphs based on text chunk relevance,
but this coarse-grained approach fails to capture entity relationships within
texts, leading to sub-par retrieval and generation quality. To address this,
recent solutions leverage LLMs to extract entities and relationships from text
chunks, constructing triplet-based knowledge graphs. However, this approach
incurs significant indexing costs, especially for large document collections.
  To ensure a good result accuracy while reducing the indexing cost, we propose
KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small
set of key text chunks and leverages an LLM to construct a knowledge graph
skeleton. It then builds a text-keyword bipartite graph from all text chunks,
serving as a lightweight alternative to a full knowledge graph. During
retrieval, KET-RAG searches both structures: it follows the local search
strategy of existing Graph-RAG systems on the skeleton while mimicking this
search on the bipartite graph to improve retrieval quality. We evaluate 13
solutions on three real-world datasets, demonstrating that KET-RAG outperforms
all competitors in indexing cost, retrieval effectiveness, and generation
quality. Notably, it achieves comparable or superior retrieval quality to
Microsoft's Graph-RAG while reducing indexing costs by over an order of
magnitude. Additionally, it improves the generation quality by up to 32.4%
while lowering indexing costs by around 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Dialogue State Tracking through Combinatorial Search for
  In-Context Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haesung Pyun, Yoonah Park, Yohan Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-Driven E-Commerce Marketing Content Optimization: Balancing
  Creati<span class="highlight-title">vit</span>y and Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPOT-Trip: Dual-Preference Driven Out-of-Town Trip <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Liu, Hao Miao, Guojiang Shen, Yan Zhao, Xiangjie Kong, Ivan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-town trip recommendation aims to generate a sequence of Points of
Interest (POIs) for users traveling from their hometowns to previously
unvisited regions based on personalized itineraries, e.g., origin, destination,
and trip duration. Modeling the complex user preferences--which often exhibit a
two-fold nature of static and dynamic interests--is critical for effective
recommendations. However, the sparsity of out-of-town check-in data presents
significant challenges in capturing such user preferences. Meanwhile, existing
methods often conflate the static and dynamic preferences, resulting in
suboptimal performance. In this paper, we for the first time systematically
study the problem of out-of-town trip recommendation. A novel framework
SPOT-Trip is proposed to explicitly learns the dual static-dynamic user
preferences. Specifically, to handle scarce data, we construct a POI attribute
knowledge graph to enrich the semantic modeling of users' hometown and
out-of-town check-ins, enabling the static preference modeling through
attribute relation-aware aggregation. Then, we employ neural ordinary
differential equations (ODEs) to capture the continuous evolution of latent
dynamic user preferences and innovatively combine a temporal point process to
describe the instantaneous probability of each preference behavior. Further, a
static-dynamic fusion module is proposed to merge the learned static and
dynamic user preferences. Extensive experiments on real data offer insight into
the effectiveness of the proposed solutions, showing that SPOT-Trip achieves
performance improvement by up to 17.01%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic
  Iterative Reasoning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding information from visually rich documents remains a significant
challenge for traditional Retrieval-Augmented Generation (RAG) methods.
Existing benchmarks predominantly focus on image-based question answering (QA),
overlooking the fundamental challenges of efficient retrieval, comprehension,
and reasoning within dense visual documents. To bridge this gap, we introduce
ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich
documents requiring complex reasoning. Based on it, we identify key limitations
in current RAG approaches: (i) purely visual retrieval methods struggle to
effectively integrate both textual and visual features, and (ii) previous
approaches often allocate insufficient reasoning tokens, limiting their
effectiveness. To address these challenges, we propose ViDoRAG, a novel
multi-agent RAG framework tailored for complex reasoning across visual
documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy
to effectively handle multi-modal retrieval. To further elicit the model's
reasoning capabilities, we introduce an iterative agent workflow incorporating
exploration, summarization, and reflection, providing a framework for
investigating test-time scaling in RAG domains. Extensive experiments on
ViDoSeek validate the effectiveness and generalization of our approach.
Notably, ViDoRAG outperforms existing methods by over 10% on the competitive
ViDoSeek benchmark. The code is available at
https://github.com/Alibaba-NLP/ViDoRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven effective in integrating
external knowledge into large language models (LLMs) for solving
question-answer (QA) tasks. The state-of-the-art RAG approaches often use the
graph data as the external data since they capture the rich semantic
information and link relationships between entities. However, existing
graph-based RAG approaches cannot accurately identify the relevant information
from the graph and also consume large numbers of tokens in the online retrieval
process. To address these issues, we introduce a novel graph-based RAG
approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by
augmenting the question using attributed communities, and also introducing a
novel LLM-based hierarchical clustering method. To retrieve the most relevant
information from the graph for the question, we build a novel hierarchical
index structure for the attributed communities and develop an effective online
retrieval method. Experimental results demonstrate that ArchRAG outperforms
existing methods in both accuracy and token cost. Moreover, ArchRAG has been
successfully applied to domain knowledge QA in Huawei Cloud Computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving
  Cloud-Device Collaboration <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyi Zhang, Pengyue Jia, Xianneng Li, Derong Xu, Maolin Wang, Yichao Wang, Zhaocheng Du, Huifeng Guo, Yong Liu, Ruiming Tang, <span class="highlight-author">Xiangyu Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)
for handling public user queries and on-device Small Language Models (SLMs) for
processing private user data, collectively forming a powerful and
privacy-preserving solution. However, existing approaches often fail to fully
leverage the scalable problem-solving capabilities of on-cloud LLMs while
underutilizing the advantage of on-device SLMs in accessing and processing
personalized data. This leads to two interconnected issues: 1) Limited
utilization of the problem-solving capabilities of on-cloud LLMs, which fail to
align with personalized user-task needs, and 2) Inadequate integration of user
data into on-device SLM responses, resulting in mismatches in contextual user
information.
  In this paper, we propose a Leader-Subordinate Retrieval framework for
Privacy-preserving cloud-device collaboration (LSRP), a novel solution that
bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM
through a dynamic selection of task-specific leader strategies named as
user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the
data advantages of on-device SLMs through small model feedback Direct
Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the
on-device SLM. Experiments on two datasets demonstrate that LSRP consistently
outperforms state-of-the-art baselines, significantly improving question-answer
relevance and personalization, while preserving user privacy through efficient
on-device retrieval. Our code is available at:
https://github.com/Applied-Machine-Learning-Lab/LSRP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HASH-RAG: Bridging Deep Hashing with Retriever for Efficient, Fine
  Retrieval and Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16133v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16133v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyu Guo, Xunlei Chen, Qiyang Xia, Zhaokun Wang, Jie Ou, Libo Qin, Shunyu Yao, Wenhong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) encounters efficiency challenges when
scaling to massive knowledge bases while preserving contextual relevance. We
propose Hash-RAG, a framework that integrates deep hashing techniques with
systematic optimizations to address these limitations. Our queries directly
learn binary hash codes from knowledgebase code, eliminating intermediate
feature extraction steps, and significantly reducing storage and computational
overhead. Building upon this hash-based efficient retrieval framework, we
establish the foundation for fine-grained chunking. Consequently, we design a
Prompt-Guided Chunk-to-Context (PGCC) module that leverages retrieved
hash-indexed propositions and their original document segments through prompt
engineering to enhance the LLM's contextual awareness. Experimental evaluations
on NQ, TriviaQA, and HotpotQA datasets demonstrate that our approach achieves a
90% reduction in retrieval time compared to conventional methods while
maintaining considerate recall performance. Additionally, The proposed system
outperforms retrieval/non-retrieval baselines by 1.4-4.3% in EM scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPR: Empowering Generation with Graph-Pretrained Retriever 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Wang, Zongyu Wu, Yuan Zhong, Xiang Zhang, Suhang Wang, Fenglong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph retrieval-augmented generation (GRAG) places high demands on
graph-specific retrievers. However, existing retrievers often rely on language
models pretrained on plain text, limiting their effectiveness due to domain
misalignment and structure ignorance. To address these challenges, we propose
GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR
aligns natural language questions with relevant subgraphs through LLM-guided
graph augmentation and employs a structure-aware objective to learn
fine-grained retrieval strategies. Experiments on two datasets, three LLM
backbones, and five baselines show that GPR consistently improves both
retrieval quality and downstream generation, demonstrating its effectiveness as
a robust retrieval solution for GRAG.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Reusability in Recommender Systems: The Case for Dataset- and
  Task-Independent Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Kurniawan Wijaya, Xinyang Shao, Gonzalo Fiz Pontiveros, Edoardo D'Amico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are pivotal in delivering personalized experiences across
industries, yet their adoption and scalability remain hindered by the need for
extensive dataset- and task-specific configurations. Existing systems often
require significant manual intervention, domain expertise, and engineering
effort to adapt to new datasets or tasks, creating barriers to entry and
limiting reusability. In contrast, recent advancements in large language models
(LLMs) have demonstrated the transformative potential of reusable systems,
where a single model can handle diverse tasks without significant
reconfiguration. Inspired by this paradigm, we propose the Dataset- and
Task-Independent Recommender System (DTIRS), a framework aimed at maximizing
the reusability of recommender systems while minimizing barriers to entry.
Unlike LLMs, which achieve task generalization directly, DTIRS focuses on
eliminating the need to rebuild or reconfigure recommendation pipelines for
every new dataset or task, even though models may still need retraining on new
data. By leveraging the novel Dataset Description Language (DsDL), DTIRS
enables standardized dataset descriptions and explicit task definitions,
allowing autonomous feature engineering, model selection, and optimization.
This paper introduces the concept of DTIRS and establishes a roadmap for
transitioning from Level-1 automation (dataset-agnostic but task-specific
systems) to Level-2 automation (fully dataset- and task-independent systems).
Achieving this paradigm would maximize code reusability and lower barriers to
adoption. We discuss key challenges, including the trade-offs between
generalization and specialization, computational overhead, and scalability,
while presenting DsDL as a foundational tool for this vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Östman, Edvin Callisen, Anton Chen, Kristiina Ausmees, Emanuel Gårdh, Jovan Zamac, Jolanta Goldsteine, Hugo Wefer, Simon Whelan, Markus Reimegård
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Money laundering enables organized crime by allowing illicit funds to enter
the legitimate economy. Although trillions of dollars are laundered each year,
only a small fraction is ever uncovered. This stems from a range of factors,
including deliberate evasion by launderers, the rarity of confirmed cases, and
the limited visibility each financial institution has into the global
transaction network. While several synthetic datasets are available, they fail
to model the structural and behavioral complexity of real-world money
laundering. In particular, they often overlook partial observability, sparse
and uncertain labels, strategic behavior, temporal dynamics, class imbalance,
and network-level dependencies. To address these limitations, we present
AMLGentex, an open-source suite for generating realistic, configurable
transaction data and benchmarking detection methods. It enables systematic
evaluation of anti-money laundering (AML) systems in a controlled environment
that captures key real-world challenges. We demonstrate how the framework can
be used to rigorously evaluate methods under conditions that reflect the
complexity of practical AML scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 figures, 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process Mining on Distributed Data Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Weisenseel, Julia Andersen, Samira Akili, Christian Imenkamp, Hendrik Reiter, Christoffer Rubensson, Wilhelm Hasselbring, Olaf Landsiedel, Xixi Lu, Jan Mendling, Florian Tschorsch, Matthias Weidlich, Agnes Koschmider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Major domains such as logistics, healthcare, and smart cities increasingly
rely on sensor technologies and distributed infrastructures to monitor complex
processes in real time. These developments are transforming the data landscape
from discrete, structured records stored in centralized systems to continuous,
fine-grained, and heterogeneous event streams collected across distributed
environments. As a result, traditional process mining techniques, which assume
centralized event logs from enterprise systems, are no longer sufficient. In
this paper, we discuss the conceptual and methodological foundations for this
emerging field. We identify three key shifts: from offline to online analysis,
from centralized to distributed computing, and from event logs to sensor data.
These shifts challenge traditional assumptions about process data and call for
new approaches that integrate infrastructure, data, and user perspectives. To
this end, we define a research agenda that addresses six interconnected fields,
each spanning multiple system dimensions. We advocate a principled methodology
grounded in algorithm engineering, combining formal modeling with empirical
evaluation. This approach enables the development of scalable, privacy-aware,
and user-centric process mining techniques suitable for distributed
environments. Our synthesis provides a roadmap for advancing process mining
beyond its classical setting, toward a more responsive and decentralized
paradigm of process intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learned Cost Model-based <span class="highlight-title">Cross</span>-engine Optimizer for SQL Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        András Strausz, Niels Pardon, Ioana Giurgiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lakehouse systems enable the same data to be queried with multiple execution
engines. However, selecting the engine best suited to run a SQL query still
requires a priori knowledge of the query computational requirements and an
engine capability, a complex and manual task that only becomes more difficult
with the emergence of new engines and workloads. In this paper, we address this
limitation by proposing a cross-engine optimizer that can automate engine
selection for diverse SQL queries through a learned cost model. Optimized with
hints, a query plan is used for query cost prediction and routing. Cost
prediction is formulated as a multi-task learning problem, and multiple
predictor heads, corresponding to different engines and provisionings, are used
in the model architecture. This eliminates the need to train engine-specific
models and allows the flexible addition of new engines at a minimal fine-tuning
cost. Results on various databases and engines show that using a query
optimized logical plan for cost estimation decreases the average Q-error by
even 12.6% over using unoptimized plans as input. Moreover, the proposed
cross-engine optimizer reduces the total workload runtime by up to 25.2% in a
zero-shot setting and 30.4% in a few-shot setting when compared to random
routing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-context Clustering-based Entity Resolution with Large Language
  Models: A Design Space Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Fu, Haitong Tang, Arijit Khan, Sharad Mehrotra, Xiangyu Ke, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Resolution (ER) is a fundamental data quality improvement task that
identifies and links records referring to the same real-world entity.
Traditional ER approaches often rely on pairwise comparisons, which can be
costly in terms of time and monetary resources, especially with large datasets.
Recently, Large Language Models (LLMs) have shown promising results in ER
tasks. However, existing methods typically focus on pairwise matching, missing
the potential of LLMs to perform clustering directly in a more cost-effective
and scalable manner. In this paper, we propose a novel in-context clustering
approach for ER, where LLMs are used to cluster records directly, reducing both
time complexity and monetary costs. We systematically investigate the design
space for in-context clustering, analyzing the impact of factors such as set
size, diversity, variation, and ordering of records on clustering performance.
Based on these insights, we develop LLM-CER (LLM-powered Clustering-based ER),
which achieves high-quality ER results while minimizing LLM API calls. Our
approach addresses key challenges, including efficient cluster merging and LLM
hallucination, providing a scalable and effective solution for ER. Extensive
experiments on nine real-world datasets demonstrate that our method
significantly improves result quality, achieving up to 150% higher accuracy,
10% increase in the F-measure, and reducing API calls by up to 5 times, while
maintaining comparable monetary cost to the most cost-effective baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by SIGMOD26</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Metadata Extraction for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Shkapenyuk, Divesh Srivastava, Theodore Johnson, Parisa Ghane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently become sophisticated enough to
automate many tasks ranging from pattern finding to writing assistance to code
generation. In this paper, we examine text-to-SQL generation. We have observed
from decades of experience that the most difficult part of query development
lies in understanding the database contents. These experiences inform the
direction of our research.
  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata
that is generally not available in practice. Human-generated metadata requires
the use of expensive Subject Matter Experts (SMEs), who are often not fully
aware of many aspects of their databases. In this paper, we explore techniques
for automatic metadata extraction to enable text-to-SQL generation.
  Ee explore the use of two standard and one newer metadata extraction
techniques: profiling, query log analysis, and SQL-to text generation using an
LLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these
techniques. BIRD does not provide query logs on their test database, so we
prepared a submission that uses profiling alone, and does not use any specially
tuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through
Nov 23, 2024 we achieved the highest score both with and without using the
"oracle" information provided with the question set. We regained the number 1
spot on Mar 11, 2025, and are still at #1 at the time of the writing (May,
2025).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-02T00:00:00Z">2025-06-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entity Image and Mixed-Modal Image Retrieval Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian-Ioan Blaga, Paul Suganthan, Sahil Dua, Krishna Srinivasan, Enrique Alfonseca, Peter Dornbach, Tom Duerig, Imed Zitouni, Zhe Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in multimodal learning, challenging benchmarks for
mixed-modal image retrieval that combines visual and textual information are
lacking. This paper introduces a novel benchmark to rigorously evaluate image
retrieval that demands deep cross-modal contextual understanding. We present
two new datasets: the Entity Image Dataset (EI), providing canonical images for
Wikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived
from the WIT dataset. The MMIR benchmark features two challenging query types
requiring models to ground textual descriptions in the context of provided
visual entities: single entity-image queries (one entity image with descriptive
text) and multi-entity-image queries (multiple entity images with relational
text). We empirically validate the benchmark's utility as both a training
corpus and an evaluation set for mixed-modal retrieval. The quality of both
datasets is further affirmed through crowd-sourced human annotations. The
datasets are accessible through the GitHub page:
https://github.com/google-research-datasets/wit-retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransAct V2: Lifelong User Action Sequence Modeling on Pinterest
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Xia, Saurabh Vishwas Joshi, Kousik Rajesh, Kangnan Li, Yangyi Lu, Nikil Pancha, Dhruvil Deven Badani, Jiajing Xu, Pong Eksombatchai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user action sequences has become a popular focus in industrial
recommendation system research, particularly for Click-Through Rate (CTR)
prediction tasks. However, industry-scale CTR models often rely on short user
sequences, limiting their ability to capture long-term behavior. Additionally,
these models typically lack an integrated action-prediction task within a
point-wise ranking framework, reducing their predictive power. They also rarely
address the infrastructure challenges involved in efficiently serving
large-scale sequential models. In this paper, we introduce TransAct V2, a
production model for Pinterest's Homefeed ranking system, featuring three key
innovations: (1) leveraging very long user sequences to improve CTR
predictions, (2) integrating a Next Action Loss function for enhanced user
action forecasting, and (3) employing scalable, low-latency deployment
solutions tailored to handle the computational demands of extended user action
sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-like Preference Profiling in Sequential <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyu Ouyang, Qianlong Wen, Chunhui Zhang, Yanfang Ye, Soroush Vosoughi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems aspire to profile users by interpreting
their interaction histories, echoing how humans make decisions by weighing
experience, relative preference strength, and situational relevance. Yet,
existing large language model (LLM)-based recommenders often fall short of
mimicking the flexible, context-aware decision strategies humans exhibit,
neglecting the structured, dynamic, and context-aware mechanisms fundamental to
human behaviors. To bridge this gap, we propose RecPO, a preference
optimization framework that models structured feedback and contextual delay to
emulate human-like prioritization in sequential recommendation RecPO exploits
adaptive reward margins based on inferred preference hierarchies and temporal
signals, enabling the model to favor immediately relevant items and to
distinguish between varying degrees of preference and aversion. Extensive
experiments across five real-world datasets demonstrate that RecPO not only
yields performance gains over state-of-the-art baselines, but also mirrors key
characteristics of human decision-making: favoring timely satisfaction,
maintaining coherent preferences, and exercising discernment under shifting
contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE)
  Using Embeddings and Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madan Krishnamurthy, Daniel Korn, Melissa A Haendel, Christopher J Mungall, Anne E Thessen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Getting almost all the bits from a quantum random access code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Hsuan Lin, Ronald de Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A quantum random access code (QRAC) is a map $x\mapsto\rho_x$ that encodes
$n$-bit strings $x$ into $m$-qubit quantum states $\rho_x$, in a way that
allows us to recover any one bit of $x$ with success probability $\geq p$. The
measurement on $\rho_x$ that is used to recover, say, $x_1$ may destroy all the
information about the other bits; this is in fact what happens in the
well-known QRAC that encodes $n=2$ bits into $m=1$ qubits. Does this generalize
to large $n$, i.e., could there exist QRACs that are so "obfuscated" that one
cannot get much more than one bit out of them? Here we show that this is not
the case: for every QRAC there exists a measurement that (with high
probability) recovers the full $n$-bit string $x$ up to small Hamming distance,
even for the worst-case $x$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages LaTeX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting
  Out-of-Distribution Corpora Using GradNormIR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayoon Ko, Jinyoung Kim, Sohyeon Kim, Jinhyuk Kim, Jaehoon Lee, Seonghak Song, Minyoung Lee, Gunhee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers encode texts into embeddings to efficiently retrieve
relevant documents from large databases in response to user queries. However,
real-world corpora continually evolve, leading to a shift from the original
training distribution of the retriever. Without timely updates or retraining,
indexing newly emerging documents can degrade retrieval performance for future
queries. Thus, identifying when a dense retriever requires an update is
critical for maintaining robust retrieval systems. In this paper, we propose a
novel task of predicting whether a corpus is out-of-distribution (OOD) relative
to a dense retriever before indexing. Addressing this task allows us to
proactively manage retriever updates, preventing potential retrieval failures.
We introduce GradNormIR, an unsupervised approach that leverages gradient norms
to detect OOD corpora effectively. Experiments on the BEIR benchmark
demonstrate that GradNormIR enables timely updates of dense retrievers in
evolving document collections, significantly enhancing retrieval robustness and
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CiteEval: Principle-Driven Citation Evaluation for Source Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumo Xu, Peng Qi, Jifan Chen, Kunlun Liu, Rujun Han, Lan Liu, Bonan Min, Vittorio Castelli, Arshit Gupta, Zhiguo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation quality is crucial in information-seeking systems, directly
influencing trust and the effectiveness of information access. Current
evaluation frameworks, both human and automatic, mainly rely on Natural
Language Inference (NLI) to assess binary or ternary supportiveness from cited
sources, which we argue is a suboptimal proxy for citation evaluation. In this
work we introduce CiteEval, a citation evaluation framework driven by
principles focusing on fine-grained citation assessment within a broad context,
encompassing not only the cited sources but the full retrieval context, user
query, and generated text. Guided by the proposed framework, we construct
CiteBench, a multi-domain benchmark with high-quality human annotations on
citation quality. To enable efficient evaluation, we further develop
CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation
with human judgments. Experiments across diverse systems demonstrate
CiteEval-Auto's superior ability to capture the multifaceted nature of
citations compared to existing metrics, offering a principled and scalable
approach to evaluate and improve model-generated citations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAM: Generative <span class="highlight-title">Recommendation</span> via Semantic-aware Multi-granular Late
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunkyung Lee, Minjin Choi, Eunseong Choi, Hye-young Kim, Jongwuk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation is an emerging paradigm that leverages the
extensive knowledge of large language models by formulating recommendations
into a text-to-text generation task. However, existing studies face two key
limitations in (i) incorporating implicit item relationships and (ii) utilizing
rich yet lengthy item information. To address these challenges, we propose a
Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM),
introducing two synergistic innovations. First, we design semantic-to-lexical
translation to encode implicit hierarchical and collaborative item
relationships into the vocabulary space of LLMs. Second, we present
multi-granular late fusion to integrate rich semantics efficiently with minimal
information loss. It employs separate encoders for multi-granular prompts,
delaying the fusion until the decoding stage. Experiments on four benchmark
datasets show that GRAM outperforms eight state-of-the-art generative
recommendation models, achieving significant improvements of 11.5-16.0% in
Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at
https://github.com/skleee/GRAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Stickers, Big Meanings: A Multilingual Sticker Semantic
  Understanding Dataset with a Gamified Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stickers, though small, are a highly condensed form of visual expression,
ubiquitous across messaging platforms and embraced by diverse cultures,
genders, and age groups. Despite their popularity, sticker retrieval remains an
underexplored task due to the significant human effort and subjectivity
involved in constructing high-quality sticker query datasets. Although large
language models (LLMs) excel at general NLP tasks, they falter when confronted
with the nuanced, intangible, and highly specific nature of sticker query
generation.
  To address this challenge, we propose a threefold solution. First, we
introduce Sticktionary, a gamified annotation framework designed to gather
diverse, high-quality, and contextually resonant sticker queries. Second, we
present StickerQueries, a multilingual sticker query dataset containing 1,115
English and 615 Chinese queries, annotated by over 60 contributors across 60+
hours. Lastly, Through extensive quantitative and qualitative evaluation, we
demonstrate that our approach significantly enhances query generation quality,
retrieval accuracy, and semantic understanding in the sticker domain. To
support future research, we publicly release our multilingual dataset along
with two fine-tuned query generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engram Memory Encoding and Retrieval: A Neurocomputational Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Szelogowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial research into the biological basis of memory, the precise
mechanisms by which experiences are encoded, stored, and retrieved in the brain
remain incompletely understood. A growing body of evidence supports the engram
theory, which posits that sparse populations of neurons undergo lasting
physical and biochemical changes to support long-term memory. Yet, a
comprehensive computational framework that integrates biological findings with
mechanistic models remains elusive. This work synthesizes insights from
cellular neuroscience and computational modeling to address key challenges in
engram research: how engram neurons are identified and manipulated; how
synaptic plasticity mechanisms contribute to stable memory traces; and how
sparsity promotes efficient, interference-resistant representations. Relevant
computational approaches -- such as sparse regularization, engram gating, and
biologically inspired architectures like Sparse Distributed Memory and spiking
neural networks -- are also examined. Together, these findings suggest that
memory efficiency, capacity, and stability emerge from the interaction of
plasticity and sparsity constraints. By integrating neurobiological and
computational perspectives, this paper provides a comprehensive theoretical
foundation for engram research and proposes a roadmap for future inquiry into
the mechanisms underlying memory, with implications for the diagnosis and
treatment of memory-related disorders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argument-Centric Causal Intervention Method for Mitigating Bias in
  <span class="highlight-title">Cross</span>-Document Event Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Yao, Wenzhong Yang, Yabo Yin, Fuyuan Wei, Hongzhen Lv, Jiaren Peng, Liejun Wang, Xiaoming Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in
natural language processing (NLP) that seeks to determine whether event
mentions across multiple documents refer to the same real-world occurrence.
However, current CD-ECR approaches predominantly rely on trigger features
within input mention pairs, which induce spurious correlations between
surface-level lexical features and coreference relationships, impairing the
overall performance of the models. To address this issue, we propose a novel
cross-document event coreference resolution method based on Argument-Centric
Causal Intervention (ACCI). Specifically, we construct a structural causal
graph to uncover confounding dependencies between lexical triggers and
coreference labels, and introduce backdoor-adjusted interventions to isolate
the true causal effect of argument semantics. To further mitigate spurious
correlations, ACCI integrates a counterfactual reasoning module that quantifies
the causal influence of trigger word perturbations, and an argument-aware
enhancement module to promote greater sensitivity to semantically grounded
information. In contrast to prior methods that depend on costly data
augmentation or heuristic-based filtering, ACCI enables effective debiasing in
a unified end-to-end framework without altering the underlying training
procedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of
88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The
implementation and materials are available at https://github.com/era211/ACCI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Entity Association Mining Framework for Knowledge Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshika Rawal, Abhijeet Kumar, Mridul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting useful signals or pattern to support important business decisions
for example analyzing investment product traction and discovering customer
preference, risk monitoring etc. from unstructured text is a challenging task.
Capturing interaction of entities or concepts and association mining is a
crucial component in text mining, enabling information extraction and reasoning
over and knowledge discovery from text. Furthermore, it can be used to enrich
or filter knowledge graphs to guide exploration processes, descriptive
analytics and uncover hidden stories in the text. In this paper, we introduce a
domain independent pipeline i.e., generalized framework to enable document
filtering, entity extraction using various sources (or techniques) as plug-ins
and association mining to build any text mining business use-case and
quantitatively define a scoring metric for ranking purpose. The proposed
framework has three major components a) Document filtering: filtering
documents/text of interest from massive amount of texts b) Configurable entity
extraction pipeline: include entity extraction techniques i.e., i) DBpedia
Spotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or
dictionary) based c) Association Relationship Mining: To generates
co-occurrence graph to analyse potential relationships among entities,
concepts. Further, co-occurrence count based frequency statistics provide a
holistic window to observe association trends or buzz rate in specific business
context. The paper demonstrates the usage of framework as fundamental building
box in two financial use-cases namely brand product discovery and vendor risk
monitoring. We aim that such framework will remove duplicated effort, minimize
the development effort, and encourage reusability and rapid prototyping in
association mining business applications for institutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Business Analytics and Intelligence Conference, IIM
  Bengaluru</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal
  Discovery <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hasan Ferdous, Emam Hossain, Md Osman Gani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust causal discovery in time series datasets depends on reliable benchmark
datasets with known ground-truth causal relationships. However, such datasets
remain scarce, and existing synthetic alternatives often overlook critical
temporal properties inherent in real-world data, including nonstationarity
driven by trends and seasonality, irregular sampling intervals, and the
presence of unobserved confounders. To address these challenges, we introduce
TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets
that systematically incorporates both linear and nonlinear dependencies while
modeling key temporal characteristics such as trends, seasonal effects, and
heterogeneous noise patterns. Each dataset is accompanied by a fully specified
causal graph featuring varying densities and diverse noise distributions and is
provided in two versions: one including unobserved confounders and one without,
thereby offering extensive coverage of real-world complexity while preserving
methodological neutrality. We further demonstrate the utility of TimeGraph
through systematic evaluations of state-of-the-art causal discovery algorithms
including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and
metrics. Our experiments reveal significant variations in algorithmic
performance under realistic temporal conditions, underscoring the need for
robust synthetic benchmarks in the fair and transparent assessment of causal
discovery methods. The complete TimeGraph suite, including dataset generation
scripts, evaluation metrics, and recommended experimental protocols, is freely
available to facilitate reproducible research and foster community-driven
advancements in time-series causal discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, accepted at KDD 2025 (Datasets and Benchmarks
  Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Platform for Investigating Public Health Content with Efficient
  Concern Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Li, Rickard Stureborg, Bhuwan Dhingra, Jun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent rise in online content expressing concerns with public health
initiatives has contributed to already stalled uptake of preemptive measures
globally. Future public health efforts must attempt to understand such content,
what concerns it may raise among readers, and how to effectively respond to it.
To this end, we present ConcernScope, a platform that uses a teacher-student
framework for knowledge transfer between large language models and light-weight
classifiers to quickly and effectively identify the health concerns raised in a
text corpus. The platform allows uploading massive files directly,
automatically scraping specific URLs, and direct text editing. ConcernScope is
built on top of a taxonomy of public health concerns. Intended for public
health officials, we demonstrate several applications of this platform: guided
data exploration to find useful examples of common concerns found in online
community datasets, identification of trends in concerns through an example
time series analysis of 186,000 samples, and finding trends in topic frequency
before and after significant events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala, Christos Faloutsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a semi-structured knowledge base (SKB), where text documents are
interconnected by relations, how can we effectively retrieve relevant
information to answer user questions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language models (LLMs) in question
answering; while Graph RAG (GRAG) uses structured knowledge bases as its
knowledge source. However, many questions require both textual and relational
information from SKB - referred to as "hybrid" questions - which complicates
the retrieval process and underscores the need for a hybrid retrieval method
that leverages both information. In this paper, through our empirical analysis,
we identify key insights that show why existing methods may struggle with
hybrid question answering (HQA) over SKB. Based on these insights, we propose
HybGRAG for HQA consisting of a retriever bank and a critic module, with the
following advantages: (1) Agentic, it automatically refines the output by
incorporating feedback from the critic module, (2) Adaptive, it solves hybrid
questions requiring both textual and relational information with the retriever
bank, (3) Interpretable, it justifies decision making with intuitive refinement
path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In
experiments on the STaRK benchmark, HybGRAG achieves significant performance
gains, with an average relative improvement in Hit@1 of 51%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collapse of Dense Retrievers: Short, Early, and Literal Biases
  Outranking Factual Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Fayyaz, Ali Modarressi, Hinrich Schuetze, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrieval models are commonly used in Information Retrieval (IR)
applications, such as Retrieval-Augmented Generation (RAG). Since they often
serve as the first step in these systems, their robustness is critical to avoid
downstream failures. In this work, we repurpose a relation extraction dataset
(e.g., Re-DocRED) to design controlled experiments that quantify the impact of
heuristic biases, such as a preference for shorter documents, on retrievers
like Dragon+ and Contriever. We uncover major vulnerabilities, showing
retrievers favor shorter documents, early positions, repeated entities, and
literal matches, all while ignoring the answer's presence! Notably, when
multiple biases combine, models exhibit catastrophic performance degradation,
selecting the answer-containing document in less than 10% of cases over a
synthetic biased document without the answer. Furthermore, we show that these
biases have direct consequences for downstream applications like RAG, where
retrieval-preferred documents can mislead LLMs, resulting in a 34% performance
drop than providing no documents at all.
https://huggingface.co/datasets/mohsenfayyaz/ColDeR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask in Any Modality: A Comprehensive <span class="highlight-title">Survey</span> on Multimodal
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) suffer from hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information for improved factual grounding. With advances in multimodal
learning, Multimodal RAG extends this approach by incorporating multiple
modalities such as text, images, audio, and video to enhance the generated
outputs. However, cross-modal alignment and reasoning introduce unique
challenges beyond those in unimodal RAG. This survey offers a structured and
comprehensive analysis of Multimodal RAG systems, covering datasets,
benchmarks, metrics, evaluation, methodologies, and innovations in retrieval,
fusion, augmentation, and generation. We review training strategies, robustness
enhancements, loss functions, and agent-based approaches, while also exploring
the diverse Multimodal RAG scenarios. In addition, we outline open challenges
and future directions to guide research in this evolving field. This survey
lays the foundation for developing more capable and reliable AI systems that
effectively leverage multimodal dynamic external knowledge bases. All resources
are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository:
  https://github.com/llm-lab-org/Multimodal-RAG-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Needle: A Generative AI-Powered <span class="highlight-title">Multi-modal</span> Database for Answering
  Complex Natural Language Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Erfanian, Mohsen Dehghankar, Abolfazl Asudeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal datasets, like those involving images, often miss the detailed
descriptions that properly capture the rich information encoded in each item.
This makes answering complex natural language queries a major challenge in this
domain. In particular, unlike the traditional nearest neighbor search, where
the tuples and the query are represented as points in a single metric space,
these settings involve queries and tuples embedded in fundamentally different
spaces, making the traditional query answering methods inapplicable. Existing
literature addresses this challenge for image datasets through vector
representations jointly trained on natural language and images. This technique,
however, underperforms for complex queries due to various reasons.
  This paper takes a step towards addressing this challenge by introducing a
Generative-based Monte Carlo method that utilizes foundation models to generate
synthetic samples that capture the complexity of the natural language query and
represent it in the same metric space as the multi-modal data.
  Following this method, we propose Needle, a database for image data
retrieval. Instead of relying on contrastive learning or metadata-searching
approaches, our system is based on synthetic data generation to capture the
complexities of natural language queries. Our system is open-source and ready
for deployment, designed to be easily adopted by researchers and developers.
The comprehensive experiments on various benchmark datasets verify that this
system significantly outperforms state-of-the-art text-to-image retrieval
methods in the literature. Any foundation model and embedder can be easily
integrated into Needle to improve the performance, piggybacking on the
advancements in these technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGA-V1: Unifying Online Advertising with End-to-End Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Qiu, Ze Wang, Fan Zhang, Zuowu Zheng, Jile Zhu, Jiangke Fan, Teng Zhang, Haitao Wang, Yongkang Wang, Xingxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern industrial advertising systems commonly employ Multi-stage Cascading
Architectures (MCA) to balance computational efficiency with ranking accuracy.
However, this approach presents two fundamental challenges: (1) performance
inconsistencies arising from divergent optimization targets and capability
differences between stages, and (2) failure to account for advertisement
externalities - the complex interactions between candidate ads during ranking.
These limitations ultimately compromise system effectiveness and reduce
platform profitability. In this paper, we present EGA-V1, an end-to-end
generative architecture that unifies online advertising ranking as one model.
EGA-V1 replaces cascaded stages with a single model to directly generate
optimal ad sequences from the full candidate ad corpus in location-based
services (LBS). The primary challenges associated with this approach stem from
high costs of feature processing and computational bottlenecks in modeling
externalities of large-scale candidate pools. To address these challenges,
EGA-V1 introduces an algorithm and engine co-designed hybrid feature service to
decouple user and ad feature processing, reducing latency while preserving
expressiveness. To efficiently extract intra- and cross-sequence mutual
information, we propose RecFormer with an innovative cluster-attention
mechanism as its core architectural component. Furthermore, we propose a
bi-stage training strategy that integrates pre-training with reinforcement
learning-based post-training to meet sophisticated platform and advertising
objectives. Extensive offline evaluations on public benchmarks and large-scale
online A/B testing on industrial advertising platform have demonstrated the
superior performance of EGA-V1 over state-of-the-art MCAs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerSRV: Personalized Sticker Retrieval with Vision-Language Model <span class="chip">WWW '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instant Messaging is a popular means for daily communication, allowing users
to send text and stickers. As the saying goes, "a picture is worth a thousand
words", so developing an effective sticker retrieval technique is crucial for
enhancing user experience. However, existing sticker retrieval methods rely on
labeled data to interpret stickers, and general-purpose Vision-Language Models
(VLMs) often struggle to capture the unique semantics of stickers.
Additionally, relevant-based sticker retrieval methods lack personalization,
creating a gap between diverse user expectations and retrieval results. To
address these, we propose the Personalized Sticker Retrieval with
Vision-Language Model framework, namely PerSRV, structured into offline
calculations and online processing modules. The online retrieval part follows
the paradigm of relevant recall and personalized ranking, supported by the
offline pre-calculation parts, which are sticker semantic understanding,
utility evaluation and personalization modules. Firstly, for sticker-level
semantic understanding, we supervised fine-tuned LLaVA-1.5-7B to generate
human-like sticker semantics, complemented by textual content extracted from
figures and historical interaction queries. Secondly, we investigate three
crowd-sourcing metrics for sticker utility evaluation. Thirdly, we cluster
style centroids based on users' historical interactions to achieve personal
preference modeling. Finally, we evaluate our proposed PerSRV method on a
public sticker retrieval dataset from WeChat, containing 543,098 candidates and
12,568 interactions. Experimental results show that PerSRV significantly
outperforms existing methods in multi-modal sticker retrieval. Additionally,
our fine-tuned VLM delivers notable improvements in sticker semantic
understandings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRACode: LoRA Adapters for Code Embeddings <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saumya Chaturvedi, Aman Chadha, Laurent Bindschaedler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code embeddings are essential for semantic code search; however, current
approaches often struggle to capture the precise syntactic and contextual
nuances inherent in code. Open-source models such as CodeBERT and UniXcoder
exhibit limitations in scalability and efficiency, while high-performing
proprietary systems impose substantial computational costs. We introduce a
parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to
construct task-specific adapters for code retrieval. Our approach reduces the
number of trainable parameters to less than two percent of the base model,
enabling rapid fine-tuning on extensive code corpora (2 million samples in 25
minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in
Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code
search tasks across multiple programming languages. Distinction in task-wise
and language-wise adaptation helps explore the sensitivity of code retrieval
for syntactical and linguistic variations. To foster research in this area, we
make our code and pre-trained models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Deep Learning for Code (DL4C) Workshop at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12499v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12499v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Likes and Dislikes in Personalized Generative Explainable
  <span class="highlight-title">Recommendation</span> <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on explainable recommendation generally frames the task as a
standard text generation problem, and evaluates models simply based on the
textual similarity between the predicted and ground-truth explanations.
However, this approach fails to consider one crucial aspect of the systems:
whether their outputs accurately reflect the users' (post-purchase) sentiments,
i.e., whether and why they would like and/or dislike the recommended items. To
shed light on this issue, we introduce new datasets and evaluation methods that
focus on the users' sentiments. Specifically, we construct the datasets by
explicitly extracting users' positive and negative opinions from their
post-purchase reviews using an LLM, and propose to evaluate systems based on
whether the generated explanations 1) align well with the users' sentiments,
and 2) accurately identify both positive and negative opinions of users on the
target items. We benchmark several recent models on our datasets and
demonstrate that achieving strong performance on existing metrics does not
ensure that the generated explanations align well with the users' sentiments.
Lastly, we find that existing models can provide more sentiment-aware
explanations when the users' (predicted) ratings for the target items are
directly fed into the models as input. The datasets and benchmark
implementation are available at: https://github.com/jchanxtarov/sent_xrec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted for presentation at The Web
  Conference (WWW) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reassessing <span class="highlight-title">Large Language Model</span> Boolean Query Generation for Systematic
  <span class="highlight-title">Review</span>s <span class="chip">SIGIR-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic reviews are comprehensive literature reviews that address highly
focused research questions and represent the highest form of evidence in
medicine. A critical step in this process is the development of complex Boolean
queries to retrieve relevant literature. Given the difficulty of manually
constructing these queries, recent efforts have explored Large Language Models
(LLMs) to assist in their formulation. One of the first studies,Wang et al.,
investigated ChatGPT for this task, followed by Staudinger et al., which
evaluated multiple LLMs in a reproducibility study. However, the latter
overlooked several key aspects of the original work, including (i) validation
of generated queries, (ii) output formatting constraints, and (iii) selection
of examples for chain-of-thought (Guided) prompting. As a result, its findings
diverged significantly from the original study. In this work, we systematically
reproduce both studies while addressing these overlooked factors. Our results
show that query effectiveness varies significantly across models and prompt
designs, with guided query formulation benefiting from well-chosen seed
studies. Overall, prompt design and model selection are key drivers of
successful query formulation. Our findings provide a clearer understanding of
LLMs' potential in Boolean query generation and highlight the importance of
model- and prompt-specific optimisations. The complex nature of systematic
reviews adds to challenges in both developing and reproducing methods but also
highlights the importance of reproducibility studies in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in SIGIR-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Retrieval with Evidence Curation for Open-<span class="highlight-title">Domain</span> Financial
  Question Answering on Standardized Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Choe, Jihoon Kim, Woohwan Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) based large language models (LLMs) are
widely used in finance for their excellent performance on knowledge-intensive
tasks. However, standardized documents (e.g., SEC filing) share similar formats
such as repetitive boilerplate texts, and similar table structures. This
similarity forces traditional RAG methods to misidentify near-duplicate text,
leading to duplicate retrieval that undermines accuracy and completeness. To
address these issues, we propose the Hierarchical Retrieval with Evidence
Curation (HiREC) framework. Our approach first performs hierarchical retrieval
to reduce confusion among similar texts. It first retrieve related documents
and then selects the most relevant passages from the documents. The evidence
curation process removes irrelevant passages. When necessary, it automatically
generates complementary queries to collect missing information. To evaluate our
approach, we construct and release a Large-scale Open-domain Financial (LOFin)
question answering benchmark that includes 145,897 SEC documents and 1,595
question-answer pairs. Our code and data are available at
https://github.com/deep-over/LOFin-bench-HiREC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for
  Auto-Generating Chemical Process and Instrumentation Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ scDataset: Scalable Data Loading for Deep Learning on Large-Scale
  Single-Cell Omics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide D'Ascenzo, Sebastiano Cultrera di Montesano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern single-cell datasets now comprise hundreds of millions of cells,
presenting significant challenges for training deep learning models that
require shuffled, memory-efficient data loading. While the AnnData format is
the community standard for storing single-cell datasets, existing data loading
solutions for AnnData are often inadequate: some require loading all data into
memory, others convert to dense formats that increase storage demands, and many
are hampered by slow random disk access. We present scDataset, a PyTorch
IterableDataset that operates directly on one or more AnnData files without the
need for format conversion. The core innovation is a combination of block
sampling and batched fetching, which together balance randomness and I/O
efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$
speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and
an 18$\times$ speed-up over BioNeMo in single-core settings. These advances
democratize large-scale single-cell model training for the broader research
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All You Need Is Binary Search! A Practical View on Lightweight Database
  Indexing on GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justus Henneberg, Felix Schuhknecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing binary search on a sorted dense array is a widely used baseline
when benchmarking sophisticated index structures: It is simple, fast to build,
and indexes the dataset with minimal memory footprint. However, the popular
opinion is that it cannot compete with sophisticated indexes in terms of lookup
performance, and hence, should not actually be considered in practice.
  Interestingly, in our recent works on (even more sophisticated) GPU-resident
index structures, we observed the surprisingly good performance of binary
search in a variety of situations. As a consequence, in this work, we analyze
the reasons for this and perform three types of optimizations to the standard
implementation to push binary search to its limits on GPUs. We show that our
highly-optimized version of binary search outperforms the naive variant by up
to a factor of 2x which makes it a practical alternative to full-fledged
indexes, such as the state-of-the-art GPU B+-Tree, while consuming considerably
less space and having a shorter build time. Apart from the optimizations, we
discuss a generalization of binary search in form of K-ary search, which is
able to consistently outperform the B+-Tree by a factor of 1.5x to 2.7x while
having a negligible space overhead over binary search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Generation of Ontologies from Relational Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Nayyeri, Athish A Yogi, Nadeen Fathallah, Ratan Bahadur Thapa, Hans-Michael Tautenhahn, Anton Schnurpel, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming relational databases into knowledge graphs with enriched
ontologies enhances semantic interoperability and unlocks advanced graph-based
learning and reasoning over data. However, previous approaches either demand
significant manual effort to derive an ontology from a database schema or
produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative
Generation of RDB Ontologies, an LLM-driven approach that turns relational
schemas into rich OWL ontologies with minimal human effort. RIGOR combines
three sources via RAG, the database schema and its documentation, a repository
of domain ontologies, and a growing core ontology, to prompt a generative LLM
for producing successive, provenance-tagged delta ontology fragments. Each
fragment is refined by a judge-LLM before being merged into the core ontology,
and the process iterates table-by-table following foreign key constraints until
coverage is complete. Applied to real-world databases, our approach outputs
ontologies that score highly on standard quality dimensions such as accuracy,
completeness, conciseness, adaptability, clarity, and consistency, while
substantially reducing manual effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thrift<span class="highlight-title">LLM</span>: On Cost-Effective Selection of <span class="highlight-title">Large Language Model</span>s for
  Classification Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Huang, Yimin Shi, Dujian Ding, Yifei Li, Yang Fei, Laks Lakshmanan, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have demonstrated remarkable
capabilities in comprehending and generating natural language content,
attracting widespread attention in both industry and academia. An increasing
number of services offer LLMs for various tasks via APIs. Different LLMs
demonstrate expertise in different domains of queries (e.g., text
classification queries). Meanwhile, LLMs of different scales, complexities, and
performance are priced diversely. Driven by this, several researchers are
investigating strategies for selecting an ensemble of LLMs, aiming to decrease
overall usage costs while enhancing performance. However, to the best of our
knowledge, none of the existing works addresses the problem, how to find an LLM
ensemble subject to a cost budget, which maximizes the ensemble performance
with guarantees.
  In this paper, we formalize the performance of an ensemble of models (LLMs)
using the notion of correctness probability, which we formally define. We
develop an approach for aggregating responses from multiple LLMs to enhance
ensemble performance. Building on this, we formulate the Optimal Ensemble
Selection problem of selecting a set of LLMs subject to a cost budget that
maximizes the overall correctness probability. We show that the correctness
probability function is non-decreasing and non-submodular and provide evidence
that the Optimal Ensemble Selection problem is likely to be NP-hard. By
leveraging a submodular function that upper bounds correctness probability, we
develop an algorithm called ThriftLLM and prove that it achieves an
instance-dependent approximation guarantee with high probability. Our framework
functions as a data processing system that selects appropriate LLM operators to
deliver high-quality results under budget constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perception-aware Sampling for Scatterplot Visualizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zafeiria Moumoulidou, Hamza Elhamdadi, Ke Yang, Subrata Mitra, Cindy Xiong Bearfield, Alexandra Meliou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualizing data is often a crucial first step in data analytics workflows,
but growing data sizes pose challenges due to computational and visual
perception limitations. As a result, data analysts commonly down-sample their
data and work with subsets. Deriving representative samples, however, remains a
challenge. This paper focuses on scatterplots, a widely-used visualization
type, and introduces a novel sampling objective -- perception-awareness --
aiming to improve sample efficacy by targeting humans' perception of a
visualization.
  We make the following contributions: (1) We propose perception-augmented
databases and design PAwS: a novel perception-aware sampling method for
scatterplots that leverages saliency maps -- a computer vision tool for
predicting areas of attention focus in visualizations -- and models
perception-awareness via saliency, density, and coverage objectives. (2) We
design ApproPAwS: a fast, perception-aware method for approximate
visualizations, which exploits the fact that small visual perturbations are
often imperceptible to humans. (3) We introduce the concept of perceptual
similarity as a metric for sample quality, and present a novel method that
compares saliency maps to measure it. (4) Our extensive experimental evaluation
shows that our methods consistently outperform prior art in producing samples
with high perceptual similarity, while ApproPAwS achieves up to 100x speed-ups
with minimal loss in visual fidelity. Our user study shows that PAwS is often
preferred by humans, validating our quantitative findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Needle: A Generative AI-Powered <span class="highlight-title">Multi-modal</span> Database for Answering
  Complex Natural Language Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Erfanian, Mohsen Dehghankar, Abolfazl Asudeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal datasets, like those involving images, often miss the detailed
descriptions that properly capture the rich information encoded in each item.
This makes answering complex natural language queries a major challenge in this
domain. In particular, unlike the traditional nearest neighbor search, where
the tuples and the query are represented as points in a single metric space,
these settings involve queries and tuples embedded in fundamentally different
spaces, making the traditional query answering methods inapplicable. Existing
literature addresses this challenge for image datasets through vector
representations jointly trained on natural language and images. This technique,
however, underperforms for complex queries due to various reasons.
  This paper takes a step towards addressing this challenge by introducing a
Generative-based Monte Carlo method that utilizes foundation models to generate
synthetic samples that capture the complexity of the natural language query and
represent it in the same metric space as the multi-modal data.
  Following this method, we propose Needle, a database for image data
retrieval. Instead of relying on contrastive learning or metadata-searching
approaches, our system is based on synthetic data generation to capture the
complexities of natural language queries. Our system is open-source and ready
for deployment, designed to be easily adopted by researchers and developers.
The comprehensive experiments on various benchmark datasets verify that this
system significantly outperforms state-of-the-art text-to-image retrieval
methods in the literature. Any foundation model and embedder can be easily
integrated into Needle to improve the performance, piggybacking on the
advancements in these technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Counterfactual Evidences for Node Classification <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhuo Qiu, Jinwen Chen, Arijit Khan, Yan Zhao, Francesco Bonchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual learning is emerging as an important paradigm, rooted in
causality, which promises to alleviate common issues of graph neural networks
(GNNs), such as fairness and interpretability. However, as in many real-world
application domains where conducting randomized controlled trials is
impractical, one has to rely on available observational (factual) data to
detect counterfactuals. In this paper, we introduce and tackle the problem of
searching for counterfactual evidences for the GNN-based node classification
task. A counterfactual evidence is a pair of nodes such that, regardless they
exhibit great similarity both in the features and in their neighborhood
subgraph structures, they are classified differently by the GNN. We develop
effective and efficient search algorithms and a novel indexing solution that
leverages both node features and structural information to identify
counterfactual evidences, and generalizes beyond any specific GNN. Through
various downstream applications, we demonstrate the potential of counterfactual
evidences to enhance fairness and accuracy of GNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning in Business Analytics: A Clash of Expectations and Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have, so far, interfered with widespread industry
adoption. This paper explains why DL, despite its popularity, has difficulties
speeding up its adoption within business analytics. It is shown that the
adoption of deep learning is not only affected by computational complexity,
lacking big data architecture, lack of transparency (black-box), skill
shortage, and leadership commitment, but also by the fact that DL does not
outperform traditional ML models in the case of structured datasets with
fixed-length feature vectors. Deep learning should be regarded as a powerful
addition to the existing body of ML models instead of a one size fits all
solution. The results strongly suggest that gradient boosting can be seen as
the go-to model for predictions on structured datasets within business
analytics. In addition to the empirical study based on three industry use
cases, the paper offers a comprehensive discussion of those results, practical
implications, and a roadmap for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor
  Difference Semantics (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wen, Yutong Ye, Xiang Lian, Mingsong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the past decades, the \textit{subgraph similarity search} over a
large-scale data graph has become increasingly important and crucial in many
real-world applications, such as social network analysis, bioinformatics
network analytics, knowledge graph discovery, and many others. While previous
works on subgraph similarity search used various graph similarity metrics such
as the graph isomorphism, graph edit distance, and so on, in this paper, we
propose a novel problem, namely \textit{subgraph similarity search under
aggregated neighbor difference semantics} (S$^3$AND), which identifies
subgraphs $g$ in a data graph $G$ that are similar to a given query graph $q$
by considering both keywords and graph structures (under new keyword/structural
matching semantics). To efficiently tackle the S$^3$AND problem, we design two
effective pruning methods, \textit{keyword set} and \textit{aggregated neighbor
difference lower bound pruning}, which rule out false alarms of candidate
vertices/subgraphs to reduce the S$^3$AND search space. Furthermore, we
construct an effective indexing mechanism to facilitate our proposed efficient
S$^3$AND query answering algorithm. Through extensive experiments, we
demonstrate the effectiveness and efficiency of our S$^3$AND approach over both
real and synthetic graphs under various parameter settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented
  Data Processing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander W. Lee, Justin Chan, Michael Fu, Nicolas Kim, Akshay Mehta, Deepti Raghavan, Ugur Cetintemel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-augmented data processing systems (DPSs) integrate large language models
(LLMs) into query pipelines, allowing powerful semantic operations on
structured and unstructured data. However, the reliability (a.k.a. trust) of
these systems is fundamentally challenged by the potential for LLMs to produce
errors, limiting their adoption in critical domains. To help address this
reliability bottleneck, we introduce semantic integrity constraints (SICs) -- a
declarative abstraction for specifying and enforcing correctness conditions
over LLM outputs in semantic queries. SICs generalize traditional database
integrity constraints to semantic settings, supporting common types of
constraints, such as grounding, soundness, and exclusion, with both proactive
and reactive enforcement strategies.
  We argue that SICs provide a foundation for building reliable and auditable
AI-augmented data systems. Specifically, we present a system design for
integrating SICs into query planning and runtime execution and discuss its
realization in AI-augmented DPSs. To guide and evaluate the vision, we outline
several design goals -- covering criteria around expressiveness, runtime
semantics, integration, performance, and enterprise-scale applicability -- and
discuss how our framework addresses each, along with open research challenges.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-01T00:00:00Z">2025-06-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI4Contracts: <span class="highlight-title">LLM</span> & RAG-Powered Encoding of Financial Derivative
  Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maruf Ahmed Mridul, Ian Sloyan, Aparna Gupta, Oshani Seneviratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) are
reshaping how AI systems extract and organize information from unstructured
text. A key challenge is designing AI methods that can incrementally extract,
structure, and validate information while preserving hierarchical and
contextual relationships. We introduce CDMizer, a template-driven, LLM, and
RAG-based framework for structured text transformation. By leveraging
depth-based retrieval and hierarchical generation, CDMizer ensures a
controlled, modular process that aligns generated outputs with predefined
schema. Its template-driven approach guarantees syntactic correctness, schema
adherence, and improved scalability, addressing key limitations of direct
generation methods. Additionally, we propose an LLM-powered evaluation
framework to assess the completeness and accuracy of structured
representations. Demonstrated in the transformation of Over-the-Counter (OTC)
financial derivative contracts into the Common Domain Model (CDM), CDMizer
establishes a scalable foundation for AI-driven document understanding,
structured synthesis, and automated validation in broader contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Unseen Capabilities: How Many Theorems Do <span class="highlight-title">LLM</span>s Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap: From Ad-hoc to Proactive Search in Conversations <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Meng, Francesco Tonolini, Fengran Mo, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proactive search in conversations (PSC) aims to reduce user effort in
formulating explicit queries by proactively retrieving useful relevant
information given conversational context. Previous work in PSC either directly
uses this context as input to off-the-shelf ad-hoc retrievers or further
fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on
short and concise queries, while the PSC input is longer and noisier. This
input mismatch between ad-hoc search and PSC limits retrieval quality. While
fine-tuning on PSC data helps, its benefits remain constrained by this input
gap. In this work, we propose Conv2Query, a novel conversation-to-query
framework that adapts ad-hoc retrievers to PSC by bridging the input gap
between ad-hoc search and PSC. Conv2Query maps conversational context into
ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc
retrievers or for further fine-tuning on PSC data. Extensive experiments on two
PSC datasets show that Conv2Query significantly improves ad-hoc retrievers'
performance, both when used directly and after fine-tuning on PSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AliBoost: Ecological Boosting Framework in Alibaba Platform <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijie Shen, Yuanchen Bei, Zihong Huang, Jialin Zhu, Keqin Xu, Boya Du, Jiawei Tang, Yuning Jiang, Feiran Huang, Xiao Huang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maintaining a healthy ecosystem in billion-scale online platforms is
challenging, as users naturally gravitate toward popular items, leaving cold
and less-explored items behind. This ''rich-get-richer'' phenomenon hinders the
growth of potentially valuable cold items and harms the platform's ecosystem.
Existing cold-start models primarily focus on improving initial recommendation
performance for cold items but fail to address users' natural preference for
popular content. In this paper, we introduce AliBoost, Alibaba's ecological
boosting framework, designed to complement user-oriented natural
recommendations and foster a healthier ecosystem. AliBoost incorporates a
tiered boosting structure and boosting principles to ensure high-potential
items quickly gain exposure while minimizing disruption to low-potential items.
To achieve this, we propose the Stacking Fine-Tuning Cold Predictor to enhance
the foundation CTR model's performance on cold items for accurate CTR and
potential prediction. AliBoost then employs an Item-oriented Bidding Boosting
mechanism to deliver cold items to the most suitable users while balancing
boosting speed with user-personalized preferences. Over the past six months,
AliBoost has been deployed across Alibaba's mainstream platforms, successfully
cold-starting over a billion new items and increasing both clicks and GMV of
cold items by over 60% within 180 days. Extensive online analysis and A/B
testing demonstrate the effectiveness of AliBoost in addressing ecological
challenges, offering new insights into the design of billion-scale recommender
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, accepted by KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NR4DER: Neural Re-ranking for Diversified Exercise <span class="highlight-title">Recommendation</span> <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghe Cheng, Xufang Zhou, Liangda Fang, Chaobo He, Yuyu Zhou, Weiqi Luo, Zhiguo Gong, Quanlong Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption of online education platforms, an increasing
number of students are gaining new knowledge through Massive Open Online
Courses (MOOCs). Exercise recommendation have made strides toward improving
student learning outcomes. However, existing methods not only struggle with
high dropout rates but also fail to match the diverse learning pace of
students. They frequently face difficulties in adjusting to inactive students'
learning patterns and in accommodating individualized learning paces, resulting
in limited accuracy and diversity in recommendations. To tackle these
challenges, we propose Neural Re-ranking for Diversified Exercise
Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to
improve the effectiveness of the exercise filter module. It then employs a
sequence enhancement method to enhance the representation of inactive students,
accurately matches students with exercises of appropriate difficulty. Finally,
it utilizes neural re-ranking to generate diverse recommendation lists based on
individual students' learning histories. Extensive experimental results
indicate that NR4DER significantly outperforms existing methods across multiple
real-world datasets and effectively caters to the diverse learning pace of
students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for presentation at the SIGIR 2025 Full Papers track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured Semantics from Unstructured Notes: Language Model Approaches
  to EHR-Based Decision Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wu Hao Ran, Xi Xi, Furong Li, Jingyi Lu, Jian Jiang, Hui Huang, Yuzhuan Zhang, Shi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has opened new avenues for
analyzing complex, unstructured data, particularly within the medical domain.
Electronic Health Records (EHRs) contain a wealth of information in various
formats, including free text clinical notes, structured lab results, and
diagnostic codes. This paper explores the application of advanced language
models to leverage these diverse data sources for improved clinical decision
support. We will discuss how text-based features, often overlooked in
traditional high dimensional EHR analysis, can provide semantically rich
representations and aid in harmonizing data across different institutions.
Furthermore, we delve into the challenges and opportunities of incorporating
medical codes and ensuring the generalizability and fairness of AI models in
healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaker: Removing Shortcut Cues with User Clustering for Single-slot
  <span class="highlight-title">Recommendation</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Yue Zheng, Yujing Zhang, Yan Feng, Zhe Wang, Xiaowei Shi, An You, Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a single-slot recommendation system, users are only exposed to one item at
a time, and the system cannot collect user feedback on multiple items
simultaneously. Therefore, only pointwise modeling solutions can be adopted,
focusing solely on modeling the likelihood of clicks or conversions for items
by users to learn user-item preferences, without the ability to capture the
ranking information among different items directly. However, since user-side
information is often much more abundant than item-side information, the model
can quickly learn the differences in user intrinsic tendencies, which are
independent of the items they are exposed to. This can cause these intrinsic
tendencies to become a shortcut bias for the model, leading to insufficient
mining of the most concerned user-item preferences. To solve this challenge, we
introduce the Breaker model. Breaker integrates an auxiliary task of user
representation clustering with a multi-tower structure for cluster-specific
preference modeling. By clustering user representations, we ensure that users
within each cluster exhibit similar characteristics, which increases the
complexity of the pointwise recommendation task on the user side. This forces
the multi-tower structure with cluster-driven parameter learning to better
model user-item preferences, ultimately eliminating shortcut biases related to
user intrinsic tendencies. In terms of training, we propose a delayed parameter
update mechanism to enhance training stability and convergence, enabling
end-to-end joint training of the auxiliary clustering and classification tasks.
Both offline and online experiments demonstrate that our method surpasses the
baselines. It has already been deployed and is actively serving tens of
millions of users daily on Meituan, one of the most popular e-commerce
platforms for services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core
  Components 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jumana Alsubhi, Mohammad D. Alahmadi, Ahmed Alhusayni, Ibrahim Aldailami, Israa Hamdine, Ahmad Shabana, Yazeed Iskandar, Suhayb Khayyat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Inequality of <span class="highlight-title">LLM</span> Fact-Checking over Geographic Regions
  with Agent and Retrieval models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Coelho, Shujaat Mirza, Yuyuan Cui, Christina Pöpper, Damon McCoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is a potentially useful application of Large Language Models
(LLMs) to combat the growing dissemination of disinformation. However, the
performance of LLMs varies across geographic regions. In this paper, we
evaluate the factual accuracy of open and private models across a diverse set
of regions and scenarios.
  Using a dataset containing 600 fact-checked statements balanced across six
global regions we examine three experimental setups of fact-checking a
statement: (1) when just the statement is available, (2) when an LLM-based
agent with Wikipedia access is utilized, and (3) as a best case scenario when a
Retrieval-Augmented Generation (RAG) system provided with the official fact
check is employed. Our findings reveal that regardless of the scenario and LLM
used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global
North perform substantially better than those from the Global South.
Furthermore, this gap is broadened for the more realistic case of a Wikipedia
agent-based system, highlighting that overly general knowledge bases have a
limited ability to address region-specific nuances. These results underscore
the urgent need for better dataset balancing and robust retrieval strategies to
enhance LLM fact-checking capabilities, particularly in geographically diverse
contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Pairwise Learning-To-Rank At Airbnb 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09795v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09795v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malay Haldar, Daochen Zha, Huiji Gao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are three fundamental asks from a ranking algorithm: it should scale to
handle a large number of items, sort items accurately by their utility, and
impose a total order on the items for logical consistency. But here's the
catch-no algorithm can achieve all three at the same time. We call this
limitation the SAT theorem for ranking algorithms. Given the dilemma, how can
we design a practical system that meets user needs? Our current work at Airbnb
provides an answer, with a working solution deployed at scale. We start with
pairwise learning-to-rank (LTR) models-the bedrock of search ranking tech
stacks today. They scale linearly with the number of items ranked and perform
strongly on metrics like NDCG by learning from pairwise comparisons. They are
at a sweet spot of performance vs. cost, making them an ideal choice for
several industrial applications. However, they have a drawback-by ignoring
interactions between items, they compromise on accuracy. To improve accuracy,
we create a "true" pairwise LTR model-one that captures interactions between
items during pairwise comparisons. But accuracy comes at the expense of
scalability and total order, and we discuss strategies to counter these
challenges. For greater accuracy, we take each item in the search result, and
compare it against the rest of the items along two dimensions: (1) Superiority:
How strongly do searchers prefer the given item over the remaining ones? (2)
Similarity: How similar is the given item to all the other items? This forms
the basis of our "all-pairwise" LTR framework, which factors in interactions
across all items at once. Looking at items on the search result page all
together-superiority and similarity combined-gives us a deeper understanding of
what searchers truly want. We quantify the resulting improvements in searcher
experience through offline and online experiments at Airbnb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Yambda-5B -- A Large-Scale <span class="highlight-title">Multi-modal</span> Dataset for Ranking And Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Ploshkin, V. Tytskiy, A. Pismenny, V. Baikalov, E. Taychinov, A. Permiakov, D. Burlakov, E. Krofto, N. Savushkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Yambda-5B, a large-scale open dataset sourced from the Yandex
Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of <span class="highlight-title">LLM</span> $\times$ DATA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please refer to the paper list at:
  https://github.com/weAIDB/awesome-data-llm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Resource-Efficient Streaming of Large-Scale Medical Image
  Datasets for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Eliot Siegel, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale medical imaging datasets have accelerated deep learning (DL) for
medical image analysis. However, the large scale of these datasets poses a
challenge for researchers, resulting in increased storage and bandwidth
requirements for hosting and accessing them. Since different researchers have
different use cases and require different resolutions or formats for DL, it is
neither feasible to anticipate every researcher's needs nor practical to store
data in multiple resolutions and formats. To that end, we propose the Medical
Image Streaming Toolkit (MIST), a format-agnostic database that enables
streaming of medical images at different resolutions and formats from a single
high-resolution copy. We evaluated MIST across eight popular, large-scale
medical imaging datasets spanning different body parts, modalities, and
formats. Our results showed that our framework reduced the storage and
bandwidth requirements for hosting and downloading datasets without impacting
image quality. We demonstrate that MIST addresses the challenges posed by
large-scale medical imaging datasets by building a data-efficient and
format-agnostic database to meet the diverse needs of researchers and reduce
barriers to DL research in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures, 10 tables, accepted to MIDL'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGA-V2: An End-to-end Generative Framework for Industrial Advertising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17549v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17549v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuowu Zheng, Ze Wang, Fan Yang, Jiangke Fan, Teng Zhang, Yongkang Wang, Xingxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional online industrial advertising systems suffer from the limitations
of multi-stage cascaded architectures, which often discard high-potential
candidates prematurely and distribute decision logic across disconnected
modules. While recent generative recommendation approaches provide end-to-end
solutions, they fail to address critical advertising requirements of key
components for real-world deployment, such as explicit bidding, creative
selection, ad allocation, and payment computation. To bridge this gap, we
introduce End-to-End Generative Advertising (EGA-V2), the first unified
framework that holistically models user interests, point-of-interest (POI) and
creative generation, ad allocation, and payment optimization within a single
generative model. Our approach employs hierarchical tokenization and
multi-token prediction to jointly generate POI recommendations and ad
creatives, while a permutation-aware reward model and token-level bidding
strategy ensure alignment with both user experiences and advertiser objectives.
Additionally, we decouple allocation from payment using a differentiable
ex-post regret minimization mechanism, guaranteeing approximate incentive
compatibility at the POI level. Through extensive offline evaluations we
demonstrate that EGA-V2 significantly outperforms traditional cascaded systems
in both performance and practicality. Our results highlight its potential as a
pioneering fully generative advertising solution, paving the way for
next-generation industrial ad systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Descriptor: C++ Self-Admitted Technical Debt Dataset (CppSATD) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc Pham, Murali Sridharan, Matteo Esposito, Valentina Lenarduzzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In software development, technical debt (TD) refers to suboptimal
implementation choices made by the developers to meet urgent deadlines and
limited resources, posing challenges for future maintenance. Self-Admitted
Technical Debt (SATD) is a sub-type of TD, representing specific TD instances
``openly admitted'' by the developers and often expressed through source code
comments. Previous research on SATD has focused predominantly on the Java
programming language, revealing a significant gap in cross-language SATD. Such
a narrow focus limits the generalizability of existing findings as well as SATD
detection techniques across multiple programming languages. Our work addresses
such limitation by introducing CppSATD, a dedicated C++ SATD dataset,
comprising over 531,000 annotated comments and their source code contexts. Our
dataset can serve as a foundation for future studies that aim to develop SATD
detection methods in C++, generalize the existing findings to other languages,
or contribute novel insights to cross-language SATD research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecLM: <span class="highlight-title">Recommendation</span> Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed
$\underline{Rec}$ommendation $\underline{L}$anguage $\underline{M}$odel (RecLM)
enhances the capture of user preference diversity through a carefully designed
reinforcement learning reward function that facilitates self-augmentation of
language models. Comprehensive evaluations demonstrate significant advantages
of our approach across various settings, and its plug-and-play compatibility
with state-of-the-art recommender systems results in notable performance
enhancements. The implementation of our RecLM framework is publicly available
at: https://github.com/HKUDS/RecLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,
  Facts, and Logic Error Correction in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05806v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05806v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese, as a linguistic system rich in depth and complexity, is
characterized by distinctive elements such as ancient poetry, proverbs, idioms,
and other cultural constructs. However, current Large Language Models (LLMs)
face limitations in these specialized domains, highlighting the need for the
development of comprehensive datasets that can assess, continuously update, and
progressively improve these culturally-grounded linguistic competencies through
targeted training optimizations. To address this gap, we introduce CKnowEdit,
the first-ever Chinese knowledge editing dataset designed to correct
linguistic, factual, and logical errors in LLMs. We collect seven types of
knowledge from a wide range of sources, including classical texts, idioms, and
content from Baidu Tieba Ruozhiba, taking into account the unique polyphony,
antithesis, and logical structures inherent in the Chinese language. By
analyzing this dataset, we highlight the challenges current LLMs face in
mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques reveals opportunities to advance the correction of Chinese
knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025; project website is available at
  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Structural-and-Textual Retrieval over Text-rich Graph
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjia Lei, Haoyu Han, Ryan A. Rossi, Franck Dernoncourt, Nedim Lipka, Mahantesh M Halappanavar, Jiliang Tang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for
answering queries by providing textual and structural knowledge. However,
current retrieval methods often retrieve these two types of knowledge in
isolation without considering their mutual reinforcement and some hybrid
methods even bypass structural retrieval entirely after neighboring
aggregation. To fill in this gap, we propose a Mixture of
Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge
via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR
generates textual planning graphs delineating the logic for answering queries.
Following planning graphs, in the Reasoning stage, MoR interweaves structural
traversal and textual matching to obtain candidates from TG-KBs. In the
Organizing stage, MoR further reranks fetched candidates based on their
structural trajectory. Extensive experiments demonstrate the superiority of MoR
in harmonizing structural and textual retrieval with insights, including uneven
retrieving performance across different query logics and the benefits of
integrating structural trajectories for candidate reranking. Our code is
available at https://github.com/Yoega/MoR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyang Liu, Junlin Li, Yinjun Wu, Zhen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Multi-Vector Retrieval (MVR) has achieved the state of the art on
many information retrieval (IR) tasks, its performance highly depends on how to
decompose queries into smaller pieces, say phrases or tokens. However,
optimizing query decomposition for MVR performance is not end-to-end
differentiable. Even worse, jointly solving this problem and training the
downstream retrieval-based systems, say RAG systems could be highly
inefficient. To overcome these challenges, we propose Performance-Oriented
Query Decomposer (POQD), a novel query decomposition framework for MVR. POQD
leverages one LLM for query decomposition and searches the optimal prompt with
an LLM-based optimizer. We further propose an end-to-end training algorithm to
alternatively optimize the prompt for query decomposition and the downstream
models. This algorithm can achieve superior MVR performance at a reasonable
training cost as our theoretical analysis suggests. POQD can be integrated
seamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented
Generation (RAG) systems. Extensive empirical studies on representative
RAG-based QA tasks show that POQD outperforms existing query decomposition
strategies in both retrieval performance and end-to-end QA accuracy. POQD is
available at https://github.com/PKU-SDS-lab/POQD-ICML25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VecFlow: A High-Performance Vector Data Management System for
  Filtered-Search on GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Xi, Chenghao Mo, Benjamin Karsin, Artem Chirkin, Mingqin Li, Minjia Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector search and database systems have become a keystone component in many
AI applications. While many prior research has investigated how to accelerate
the performance of generic vector search, emerging AI applications require
running more sophisticated vector queries efficiently, such as vector search
with attribute filters. Unfortunately, recent filtered-ANNS solutions are
primarily designed for CPUs, with few exploration and limited performance of
filtered-ANNS that take advantage of the massive parallelism offered by GPUs.
In this paper, we present VecFlow, a novel high-performance vector filtered
search system that achieves unprecedented high throughput and recall while
obtaining low latency for filtered-ANNS on GPUs. We propose a novel
label-centric indexing and search algorithm that significantly improves the
selectivity of ANNS with filters. In addition to algorithmic level
optimization, we provide architectural-aware optimization for VecFlow's
functional modules, effectively supporting both small batch and large batch
queries, and single-label and multi-label query processing. Experimental
results on NVIDIA A100 GPU over several public available datasets validate that
VecFlow achieves 5 million QPS for recall 90%, outperforming state-of-the-art
CPU-based solutions such as Filtered-DiskANN by up to 135 times. Alternatively,
VecFlow can easily extend its support to high recall 99% regime, whereas strong
GPU-based baselines plateau at around 80% recall. The source code is available
at https://github.com/Supercomputing-System-AI-Lab/VecFlow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of <span class="highlight-title">LLM</span> $\times$ DATA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please refer to the paper list at:
  https://github.com/weAIDB/awesome-data-llm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bag Semantics Query Containment: The CQ vs. UCQ Case and Other Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerzy Marcinkowski, Piotr Ostropolski-Nalewaja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query Containment Problem (QCP) is a fundamental decision problem in query
processing and optimization. While QCP has for a long time been completely
understood for the case of set semantics, decidability of QCP for conjunctive
queries under multi-set semantics ($QCP_{\text{CQ}}^{\text{bag}}$) remains one
of the most intriguing open problems in database theory. Certain effort has
been put, in last 30 years, to solve this problem and some decidable special
cases of $QCP_{\text{CQ}}^{\text{bag}}$ were identified, as well as some
undecidable extensions, including $QCP_{\text{UCQ}}^{\text{bag}}$. In this
paper we introduce a new technique which produces, for a given UCQ $\Phi$, a CQ
$\phi$ such that the application of $\phi$ to a database $D$ is, in some sense,
an approximation of the application of $\Phi$ to $D$. Using this technique we
could analyze the status of $QCP^{\text{bag}}$ when one of the queries in
question is a CQ and the other is a UCQ, and we reached conclusions which
surprised us a little bit. We also tried to use this technique to translate the
known undecidability proof for $QCP_{\text{UCQ}}^{\text{bag}}$ into a proof of
undecidability of $QCP_{\text{CQ}}^{\text{bag}}$. And, as you are going to see,
we got stopped just one infinitely small $\varepsilon$ before reaching this
ultimate goal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expanded explanations to provide better intuitions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyang Liu, Junlin Li, Yinjun Wu, Zhen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Multi-Vector Retrieval (MVR) has achieved the state of the art on
many information retrieval (IR) tasks, its performance highly depends on how to
decompose queries into smaller pieces, say phrases or tokens. However,
optimizing query decomposition for MVR performance is not end-to-end
differentiable. Even worse, jointly solving this problem and training the
downstream retrieval-based systems, say RAG systems could be highly
inefficient. To overcome these challenges, we propose Performance-Oriented
Query Decomposer (POQD), a novel query decomposition framework for MVR. POQD
leverages one LLM for query decomposition and searches the optimal prompt with
an LLM-based optimizer. We further propose an end-to-end training algorithm to
alternatively optimize the prompt for query decomposition and the downstream
models. This algorithm can achieve superior MVR performance at a reasonable
training cost as our theoretical analysis suggests. POQD can be integrated
seamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented
Generation (RAG) systems. Extensive empirical studies on representative
RAG-based QA tasks show that POQD outperforms existing query decomposition
strategies in both retrieval performance and end-to-end QA accuracy. POQD is
available at https://github.com/PKU-SDS-lab/POQD-ICML25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-31T00:00:00Z">2025-05-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pitfalls in Evaluating Language Model Forecasters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Paleka, Shashwat Goel, Jonas Geiping, Florian Tramèr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently been applied to forecasting tasks,
with some works claiming these systems match or exceed human performance. In
this paper, we argue that, as a community, we should be careful about such
conclusions as evaluating LLM forecasters presents unique challenges. We
identify two broad categories of issues: (1) difficulty in trusting evaluation
results due to many forms of temporal leakage, and (2) difficulty in
extrapolating from evaluation performance to real-world forecasting. Through
systematic analysis and concrete examples from prior work, we demonstrate how
evaluation flaws can raise concerns about current and future performance
claims. We argue that more rigorous evaluation methodologies are needed to
confidently assess the forecasting abilities of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on E-Commerce Long-Tail Product <span class="highlight-title">Recommendation</span> Mechanism Based
  on Large-Scale Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyi Lu, Haotian Lyu, Jiayun Zheng, Yang Wang, Li Zhang, Chengrui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As e-commerce platforms expand their product catalogs, accurately
recommending long-tail items becomes increasingly important for enhancing both
user experience and platform revenue. A key challenge is the long-tail problem,
where extreme data sparsity and cold-start issues limit the performance of
traditional recommendation methods. To address this, we propose a novel
long-tail product recommendation mechanism that integrates product text
descriptions and user behavior sequences using a large-scale language model
(LLM). First, we introduce a semantic visor, which leverages a pre-trained LLM
to convert multimodal textual content such as product titles, descriptions, and
user reviews into meaningful embeddings. These embeddings help represent
item-level semantics effectively. We then employ an attention-based user intent
encoder that captures users' latent interests, especially toward long-tail
items, by modeling collaborative behavior patterns. These components feed into
a hybrid ranking model that fuses semantic similarity scores, collaborative
filtering outputs, and LLM-generated recommendation candidates. Extensive
experiments on a real-world e-commerce dataset show that our method outperforms
baseline models in recall (+12%), hit rate (+9%), and user coverage (+15%).
These improvements lead to better exposure and purchase rates for long-tail
products. Our work highlights the potential of LLMs in interpreting product
content and user intent, offering a promising direction for future e-commerce
recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in
  Finance-Specific Deployment of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Xu, Fufang Wen, Beilin Chu, Zhibing Fu, Qinhong Lin, Jiaqi Liu, Binjie Fei, Zhongliang Yang, Linna Zhou, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-based learning for news headline <span class="highlight-title">recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Bouras, Audrey Durand, Richard Khoury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Question Semantic Space for Dynamic Retrieval-Augmented
  Multi-hop Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Ye, Lang Yu, Zhikai Lei, Qin Chen, Jie Zhou, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is usually integrated into large
language models (LLMs) to mitigate hallucinations and knowledge obsolescence.
Whereas,conventional one-step retrieve-and-read methods are insufficient for
multi-hop question answering, facing challenges of retrieval semantic
mismatching and the high cost in handling interdependent subquestions. In this
paper, we propose Optimizing Question Semantic Space for Dynamic
Retrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of
three key modules: (1) the Question Decomposition Module (QDM), which
decomposes multi-hop questions into fine-grained subquestions; (2) the
Subquestion Dependency Optimizer Module (SDOM), which models the interdependent
relations of subquestions for better understanding; and (3) the Dynamic Passage
Retrieval Module (DPRM), which aligns subquestions with relevant passages by
optimizing the semantic embeddings. Experimental results across various
benchmarks demonstrate that Q-DREAM significantly outperforms existing RAG
methods, achieving state-of-the-art performance in both in-domain and
out-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency
while maintaining high accuracy compared with recent baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DV365: Extremely Long User History Modeling at Instagram <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Lyu, Devashish Tyagi, Yihang Yang, Ziwei Li, Ajay Somani, Karthikeyan Shanmugasundaram, Nikola Andrejevic, Ferdi Adeputra, Curtis Zeng, Arun K. Singh, Maxime Ransan, Sagar Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long user history is highly valuable signal for recommendation systems, but
effectively incorporating it often comes with high cost in terms of data center
power consumption and GPU. In this work, we chose offline embedding over
end-to-end sequence length optimization methods to enable extremely long user
sequence modeling as a cost-effective solution, and propose a new user
embedding learning strategy, multi-slicing and summarization, that generates
highly generalizable user representation of user's long-term stable interest.
History length we encoded in this embedding is up to 70,000 and on average
40,000. This embedding, named as DV365, is proven highly incremental on top of
advanced attentive user sequence models deployed in Instagram. Produced by a
single upstream foundational model, it is launched in 15 different models
across Instagram and Threads with significant impact, and has been production
battle-proven for >1 year since our first launch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGKDD 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K-order Ranking Preference Optimization for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Cai, Chongming Gao, Yang Zhang, Wentao Shi, Jizhi Zhang, Keqin Bao, Qifan Wang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To adapt large language models (LLMs) to ranking tasks, existing list-wise
methods, represented by list-wise Direct Preference Optimization (DPO), focus
on optimizing partial-order or full-order list ranking consistency for LLMs to
enhance their ranking abilities. However, we argue that optimizing top-K
ranking consistency could be more appropriate for real-world applications.
There are two main reasons: (1) users are typically concerned with only the
top-K results, making top-K ranking more important, and (2) tail items often
lack precise feedback, making top-K ranking more reliable. Based on this, we
propose K-order Ranking Preference Optimization (KPO) by extending the DPO's
Plackett-Luce model to accommodate top-K rankings. Additionally, recognizing
that the number of important items can vary across queries, we extend KPO to
dynamically determine appropriate K for different samples and introduce a
curriculum learning strategy to boost training efficiency. Extensive
experiments demonstrate the effectiveness of KPO, highlighting its high sample
efficiency and robustness to noise. The code is available at
https://github.com/Lanyu0303/KPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Significant Are the Real Performance Gains? An Unbiased Evaluation
  Framework for GraphRAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Zeng, Xiao Yan, Hao Luo, Yuhao Lin, Yuxiang Wang, Fangcheng Fu, Bo Du, Quanqing Xu, Jiawei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting General-Purpose Embedding Models to Private Datasets Using
  Keyword-based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubai Wei, Jiale Han, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models play a cornerstone role in AI applications, such as
retrieval-augmented generation (RAG). While general-purpose text embedding
models demonstrate strong performance on generic retrieval benchmarks, their
effectiveness diminishes when applied to private datasets (e.g.,
company-specific proprietary data), which often contain specialized terminology
and lingo. In this work, we introduce BMEmbed, a novel method for adapting
general-purpose text embedding models to private datasets. By leveraging the
well-established keyword-based retrieval technique (BM25), we construct
supervisory signals from the ranking of keyword-based retrieval results to
facilitate model adaptation. We evaluate BMEmbed across a range of domains,
datasets, and models, showing consistent improvements in retrieval performance.
Moreover, we provide empirical insights into how BM25-based signals contribute
to improving embeddings by fostering alignment and uniformity, highlighting the
value of this approach in adapting models to domain-specific data. We release
the source code available at https://github.com/BaileyWei/BMEmbed for the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://github.com/BaileyWei/BMEmbed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic
  Languages with Example Selection from Related Example Banks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated impressive few-shot
learning capabilities through in-context learning (ICL). However, ICL
performance is highly dependent on the choice of few-shot demonstrations,
making the selection of the most optimal examples a persistent research
challenge. This issue is further amplified in low-resource Indic languages,
where the scarcity of ground-truth data complicates the selection process. In
this work, we propose PromptRefine, a novel Alternating Minimization approach
for example selection that improves ICL performance on low-resource Indic
languages. PromptRefine leverages auxiliary example banks from related
high-resource Indic languages and employs multi-task learning techniques to
align language-specific retrievers, enabling effective cross-language
retrieval. Additionally, we incorporate diversity in the selected examples to
enhance generalization and reduce bias. Through comprehensive evaluations on
four text generation tasks -- Cross-Lingual Question Answering, Multilingual
Question Answering, Machine Translation, and Cross-Lingual Summarization using
state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and
Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms
existing frameworks for retrieving examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ It's High Time: A <span class="highlight-title">Survey</span> of Temporal Information Retrieval and Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Avishek Anand, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time plays a critical role in how information is generated, retrieved, and
interpreted. In this survey, we provide a comprehensive overview of Temporal
Information Retrieval and Temporal Question Answering, two research areas aimed
at handling and understanding time-sensitive information. As the amount of
time-stamped content from sources like news articles, web archives, and
knowledge bases increases, systems must address challenges such as detecting
temporal intent, normalizing time expressions, ordering events, and reasoning
over evolving or ambiguous facts. These challenges are critical across many
dynamic and time-sensitive domains, from news and encyclopedias to science,
history, and social media. We review both traditional approaches and modern
neural methods, including those that use transformer models and Large Language
Models (LLMs). We also review recent advances in temporal language modeling,
multi-hop reasoning, and retrieval-augmented generation (RAG), alongside
benchmark datasets and evaluation strategies that test temporal robustness,
recency awareness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Multi-Hop Document Retrieval Through Intermediate
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaen Lin, Jingyu Liu, Yingbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) encounters challenges when addressing
complex queries, particularly multi-hop questions. While several methods tackle
multi-hop queries by iteratively generating internal queries and retrieving
external documents, these approaches are computationally expensive. In this
paper, we identify a three-stage information processing pattern in LLMs during
layer-by-layer reasoning, consisting of extraction, processing, and subsequent
extraction steps. This observation suggests that the representations in
intermediate layers contain richer information compared to those in other
layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike
prior methods that focus on generating new internal queries, L-RAG leverages
intermediate representations from the middle layers, which capture next-hop
information, to retrieve external knowledge. L-RAG achieves performance
comparable to multi-step approaches while maintaining inference overhead
similar to that of standard RAG. Experimental results show that L-RAG
outperforms existing RAG methods on open-domain multi-hop question-answering
datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is
available in https://anonymous.4open.science/r/L-RAG-ADD5/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TestNUC: Enhancing Test-Time Computing Approaches and Scaling through
  Neighboring Unlabeled Data Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time computing approaches, which leverage additional computational
resources during inference, have been proven effective in enhancing large
language model performance. This work introduces a novel, linearly scaling
approach, TestNUC, that improves test-time predictions by leveraging the local
consistency of neighboring unlabeled data-it classifies an input instance by
considering not only the model's prediction on that instance but also on
neighboring unlabeled instances. We evaluate TestNUC across eight diverse
datasets, spanning intent classification, topic mining, domain discovery, and
emotion detection, demonstrating its consistent superiority over baseline
methods such as standard prompting and self-consistency. Furthermore, TestNUC
can be seamlessly integrated with existing test-time computing approaches,
substantially boosting their performance. Our analysis reveals that TestNUC
scales effectively with increasing amounts of unlabeled data and performs
robustly across different embedding models, making it practical for real-world
applications. Our code is available at https://github.com/HenryPengZou/TestNUC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Generation In Large Model Era: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Jinghao Zhang, Alireza Salemi, Xinting Hu, Wenjie Wang, Fuli Feng, Hamed Zamani, Xiangnan He, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large models, content generation is gradually shifting to
Personalized Generation (PGen), tailoring content to individual preferences and
needs. This paper presents the first comprehensive survey on PGen,
investigating existing research in this rapidly growing field. We conceptualize
PGen from a unified perspective, systematically formalizing its key components,
core objectives, and abstract workflows. Based on this unified perspective, we
propose a multi-level taxonomy, offering an in-depth review of technical
advancements, commonly used datasets, and evaluation metrics across multiple
modalities, personalized contexts, and tasks. Moreover, we envision the
potential applications of PGen and highlight open challenges and promising
directions for future exploration. By bridging PGen research across multiple
modalities, this survey serves as a valuable resource for fostering knowledge
sharing and interdisciplinary collaboration, ultimately contributing to a more
personalized digital landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExPerT: Effective and Explainable Evaluation of Personalized Long-Form
  Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Julian Killingback, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating personalized text generated by large language models (LLMs) is
challenging, as only the LLM user, i.e., prompt author, can reliably assess the
output, but re-engaging the same individuals across studies is infeasible. This
paper addresses the challenge of evaluating personalized text generation by
introducing ExPerT, an explainable reference-based evaluation framework. ExPerT
leverages an LLM to extract atomic aspects and their evidence from the
generated and reference texts, match the aspects, and evaluate their alignment
based on content and writing style -- two key attributes in personalized text
generation. Additionally, ExPerT generates detailed, fine-grained explanations
for every step of the evaluation process, enhancing transparency and
interpretability. Our experiments demonstrate that ExPerT achieves a 7.2%
relative improvement in alignment with human judgments compared to the
state-of-the-art text generation evaluation methods. Furthermore, human
evaluators rated the usability of ExPerT's explanations at 4.7 out of 5,
highlighting its effectiveness in making evaluation decisions more
interpretable.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Databases
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Connor, Alan Dearle, Ben Claydon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern search domains comprise high-dimensional vectors of floating
point numbers derived from neural networks, in the form of embeddings. Typical
embeddings range in size from hundreds to thousands of dimensions, making the
size of the embeddings, and the speed of comparison, a significant issue.
  Quantisation is a class of mechanism which replaces the floating point values
with a smaller representation, for example a short integer. This gives an
approximation of the embedding space in return for a smaller data
representation and a faster comparison function.
  Here we take this idea almost to its extreme: we show how vectors of
arbitrary-precision floating point values can be replaced by vectors whose
elements are drawn from the set {-1,0,1}. This yields very significant savings
in space and metric evaluation cost, while maintaining a strong correlation for
similarity measurements.
  This is achieved by way of a class of convex polytopes which exist in the
high-dimensional space. In this article we give an outline description of these
objects, and show how they can be used for the basis of such radical
quantisation while maintaining a surprising degree of accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to SISAP25 International Conference on Similarity Search
  and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinkit Patel, Kee Siong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many large enterprises that operate highly governed and complex ICT
environments have no efficient and effective way to support their Data and AI
teams in rapidly spinning up and tearing down self-service data and compute
infrastructure, to experiment with new data analytic tools, and deploy data
products into operational use. This paper proposes a key piece of the solution
to the overall problem, in the form of an on-demand self-service data-platform
infrastructure to empower de-centralised data teams to build data products on
top of centralised templates, policies and governance. The core innovation is
an efficient method to leverage immutable container operating systems and
infrastructure-as-code methodologies for creating, from scratch, vendor-neutral
and short-lived Kubernetes clusters on-premises and in any cloud environment.
Our proposed approach can serve as a repeatable, portable and cost-efficient
alternative or complement to commercial Platform-as-a-Service (PaaS) offerings,
and this is particularly important in supporting interoperability in complex
data mesh environments with a mix of modern and legacy compute infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> OmniRouter: Budget and Performance Controllable Multi-<span class="highlight-title">LLM</span> Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20576v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20576v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Mei, Wujiang Xu, Shuhang Lin, <span class="highlight-author">Yongfeng Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) deliver superior performance but require
substantial computational resources and operate with relatively low efficiency,
while smaller models can efficiently handle simpler tasks with fewer resources.
LLM routing is a crucial paradigm that dynamically selects the most suitable
large language models from a pool of candidates to process diverse inputs,
ensuring optimal resource utilization while maintaining response quality.
Existing routing frameworks typically model this as a locally optimal
decision-making problem, selecting the presumed best-fit LLM for each query
individually, which overlook global budget constraints, resulting in
ineffective resource allocation. To tackle this problem, we introduce
OmniRouter, a fundamentally controllable routing framework for multi-LLM
serving. Instead of making per-query greedy choices, OmniRouter models the
routing task as a constrained optimization problem, assigning models that
minimize total cost while ensuring the required performance level.
Specifically, a hybrid retrieval-augmented predictor is designed to predict the
capabilities and costs of LLMs and a constrained optimizer is employed to
control globally optimal query-model allocation. Experiments show that
OmniRouter achieves up to 6.30% improvement in response accuracy while
simultaneously reducing computational costs by at least 10.15% compared to
competitive router baselines. The code and the dataset are available at
https://github.com/agiresearch/OmniRouter.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-06-29T05:33:36.055939321Z">
            2025-06-29 05:33:36 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
